---
layout: default
title: "Cross Attentive Pooling"
parent: "Paper review"
nav_order: 7
permalink: /docs/review/7
use_math: true
comments: true
---

# **Cross attentive pooling for speaker verification**

#### Seong Min Kye, Yoohwan Kwon, Joon Son Chung


### âœ Abstract ğŸ”

- **ëª©í‘œ : 'in the wild' videoì™€ ê´€ë ¨ì—†ëŠ” signalì„ í¬í•¨í•˜ëŠ” utteranceë¥¼ ì‚¬ìš©í•˜ëŠ” TI-SV(Text-Independent Speaker Verification)**
- SVëŠ” pair-wise ë¬¸ì œ(ë“±ë¡ê³¼ í…ŒìŠ¤íŠ¸ ìŒì„ ë¹„êµ), ê¸°ì¡´ì˜ embedding ì¶”ì¶œì€ instance-wise ë¬¸ì œ(ê° utteranceì— ëŒ€í•œ embeddingì„ ì¶”ì¶œí•˜ì—¬ ì„œë¡œ ë¹„êµ)
- <span style="background-color:#ffed54">ë³¸ ë…¼ë¬¸ì—ì„œëŠ” reference-query pair ì „ì²´ì˜ context ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ **pair-wise ë¬¸ì œì— ê°€ì¥ discriminativeí•œ utterance-levelì˜ embedding ì¶”ì¶œì„ ìƒì„±**í•˜ëŠ” **CAP(Cross Attention Pooling)**ì„ ì œì•ˆ</span>
- VoxCeleb datasetì„ ì‚¬ìš©í•˜ê³ , ë‹¤ë¥¸ pooling ë°©ë²•ê³¼ ë¹„êµí•˜ì—¬ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŒ

<br/>

---

<br/>


### â… . Introduction ğŸ”

- Automatic Speaker Recognition; ìŒì„±ì€ ê°€ì¥ ì‰½ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆëŠ” ìƒì²´ ì •ë³´ ì¤‘ í•˜ë‚˜ì´ê¸° ë•Œë¬¸ì— ëˆ„êµ°ê°€ì˜ ì‹ ì›ì„ í™•ì¸í•˜ëŠ”ë° ë§¤ë ¥ì ì¸ ë°©ë²•
- speaker recognitionì€ identificationê³¼ verificationì„ ëª¨ë‘ í¬í•¨í•˜ì§€ë§Œ, í›„ìì˜ ê²½ìš° ë” ì‹¤ìš©ì ì¸ ì‘ìš© ë¶„ì•¼ë¥¼ ê°€ì§(ex. ì½œì„¼í„°, AI ìŠ¤í”¼ì»¤ ë“±)
- closed-set identificationê³¼ ë‹¬ë¦¬ open-set verificationì€ í›ˆë ¨ì—ì„œ ë³´ì§€ ëª»í–ˆë˜ í™”ìì˜ identityë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•˜ê¸° ë•Œë¬¸ì—, speaker verificationì€ ìŒì„±ì´ discriminativeí•œ embedding ì°¨ì›ì˜ í‘œí˜„ìœ¼ë¡œ mappingë˜ì–´ì•¼í•˜ëŠ” metric learning ë¬¸ì œ
- ë‹¤ë¥¸ ë…¼ë¬¸ë“¤ì—ì„œ ì£¼ë¡œ classification lossë¥¼ ì‚¬ìš©í•˜ì—¬ embeddingì„ í•™ìŠµí•˜ì˜€ìœ¼ë‚˜ embedding similarityë¥¼ ìµœì í™”í•˜ë„ë¡ ì„¤ê³„ë˜ì§€ ì•ŠìŒ
- ìµœê·¼ ì—°êµ¬ë“¤ì—ì„œ class ê°„ ë¶„ë¦¬ë¥¼ ê°•í™”í•˜ê¸° ìœ„í•´ verificationì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì§„ margin variantë¥¼ ì¶”ê°€í•œ softmaxë¥¼ ì ‘ëª©ì‹œí‚´ (AM-softmax)

<br/>

- **open-set verification**ì€ networkê°€ ì œí•œëœ exampleì„ ê°–ìœ¼ë©´ì„œ unseen classì— ëŒ€í•´ ì¸ì‹í•´ì•¼í•˜ë¯€ë¡œ **few-shot learning** ë¬¸ì œë¼ê³  ë³¼ ìˆ˜ ìˆìŒ
- few-shot learning ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ëª¨ë°©í•˜ëŠ” **prototypical network**ê°€ ì œì•ˆë˜ì—ˆìœ¼ë©°, **ìµœê·¼ speaker verificationì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë‹¬ì„±**í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¨

<br/>

- similarity metricì„ ìµœì í™”í•˜ë„ë¡ networkë¥¼ í›ˆë ¨ì‹œí‚¤ê¸° ìœ„í•´ì„œëŠ” frame-levelì˜ representation(feature)ë¥¼ utterance-levelë¡œ ëª¨ì•„ì•¼ í•¨
- ê°€ì¥ ë‹¨ìˆœí•œ ë°©ë²•ì€ frame-levelì„ í‰ê· í•˜ëŠ” ê²ƒ(TAP, Temporal Average Pooling), ì´ë•Œ frameë“¤ì€ ëª¨ë‘ ê°™ì€ weightë¥¼ ê°–ê²Œ ë¨
- verificationì— ë” discriminativeí•œ frameì— attentioní•˜ë„ë¡ SAP(Self-Attentive Pooling)ë°©ë²•ì´ ì œì•ˆ
- ê·¸ëŸ¬ë‚˜ instance-level self-attentionì€ support set(training set)ì˜ íŠ¹ì • sampleì´ ì•„ë‹Œ, ì¼ë°˜ì ìœ¼ë¡œ(training setì˜ ì „ì²´ dataë¥¼ ì•„ìš°ë¦„) discriminativeí•œ featureë¥¼ ì°¾ìŒ; training datasetì˜ ì „ì²´ì ì¸ íŠ¹ì„±ì´ ë°˜ì˜ë˜ì–´ íŠ¹ì • sampleì— ëŒ€í•´ì„œëŠ” íš¨ê³¼ì ì´ì§€ ì•Šì„ ìˆ˜ ìˆìŒ

<br/>

- CAN(Cross Attention Network): few-shot learningì—ì„œ ìµœê·¼ support setì˜ exampleë“¤ê³¼ ê´€ë ¨ìˆê³ , discriminativeí•œ input imageì˜ ë¶€ë¶„ì— attentioní•¨ìœ¼ë¡œì¨ unseen target classë¥¼ ê¸°ë°˜ì˜ attentionì„ ì„ íƒí•  ìˆ˜ ìˆë„ë¡ ì œì•ˆëœ ë°©ë²•
- support setì˜ í•œ class(speaker)ì™€ utteranceë¥¼ ë¹„êµí•˜ê¸° ìœ„í•œ discriminativeí•œ íŠ¹ì„±ì´ ë‹¤ë¥¸ classì™€ ë¹„êµí•˜ê¸° ìœ„í•´ ìƒì„±ë˜ëŠ” íŠ¹ì§•ê³¼ ë‹¤ë¥¼ ê²ƒ, ë”°ë¼ì„œ ì´ ì•„ì´ë””ì–´ë¥¼ speaker verificationì— ì ìš©í•  ìˆ˜ ìˆìŒ
- ë³¸ ë…¼ë¬¸ì—ì„œëŠ” frame-levelì˜ ì •ë³´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ utterance-levelì˜ embeddingìœ¼ë¡œ ëª¨ìœ¼ê¸° ìœ„í•´ support setì˜ exampleì„ ì°¸ì¡°í•˜ì—¬ attentionì„ ê³„ì‚°í•˜ëŠ” CAP(Cross Attentive Pooling)ë¥¼ ì œì•ˆ
- **ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ networkëŠ” support setì˜ íŠ¹ì • classì— ëŒ€í•œ íŠ¹ì • íŠ¹ì§•ì„ ì œê³µí•˜ëŠ” utteranceì„ ì‹ë³„í•˜ê³  ì§‘ì¤‘ì‹œí‚¬ ìˆ˜ ìˆìŒ**
- ì´ëŠ” ì‚¬ëŒì´ unseen classì˜ instanceë¥¼ ì¸ì‹í•  ë•Œ, sample ìŒë“¤ì˜ ê³µí†µì ì¸ íŠ¹ì„±ì„ ê°–ëŠ” íŠ¹ì§•ì„ ì°¾ëŠ” ê²ƒê³¼ ìœ ì‚¬í•¨
- instance-levelì˜ poolingê³¼ ë‹¬ë¦¬, ì œì•ˆëœ attention moduleì€ class(prototype) featureì™€ query featureì˜ ê´€ë ¨ì„±ì„ ëª¨ë¸ë§í•˜ì—¬ verification taskì—ì„œ pair-wise íŠ¹ì„±ì„ ìµœëŒ€í•œ í™œìš©

<br/>

---

<br/>

### â…¡. Methods ğŸ”

#### <span style="background-color:#aee4ff">**2.1 Few-shot learning framwork**</span>

- Speaker recognitionì„ ìœ„í•œ embeddingì„ í›ˆë ¨í•˜ê¸° ìœ„í•´ few-shot learning frameworkì¸ prototypical network ì‚¬ìš©

<br/>

**Batch formation**

- ê° mini-batchì—ëŠ” support(training) set $S$ì™€ query(test) set $Q$ê°€ í¬í•¨
- ì„œë¡œ ë‹¤ë¥¸ í™”ì Nëª…ë§ˆë‹¤ Mê°œì˜ ë°œí™” í¬í•¨

<center>

$S = {(x_i, y_i)}^{N \times 1}_{i=1}$

$Q = {(\tilde{x_i}, \tilde{y_i})}^{N \times (M-1)}_{i=1}$
</center>
> support setì€ ê° í™”ìë§ˆë‹¤ 1ê°œì˜ ë°œí™”ë¥¼ ì‚¬ìš©í•˜ê³ , query setì€ ë‚˜ë¨¸ì§€ ë°œí™”($2 \leq i \leq M$)ë¥¼ ì‚¬ìš©
>
> $y, \tilde{y} \in {1, ..., N}$; class label

<br/>

**Training object**

- support setì€ ë‹¨ì¼ ë°œí™” $x$ë¡œ êµ¬ì„±ë˜ì–´, prototype(centroid)ëŠ” ê° í™”ì %y%ì˜ support utteranceì™€ ê°™ìŒ
<center><img src="https://user-images.githubusercontent.com/46676700/95678027-a5714b00-0c04-11eb-816d-01da565f1eaa.png" alt="img" style="zoom: 80%;" /></center>

<br/>

- log-softmax functionì„ ì‚¬ìš©í•˜ëŠ” cross-entropy lossëŠ” ê°™ì€ speakerì˜ segment ê°„ ê±°ë¦¬ëŠ” ìµœì†Œí™”í•˜ë©´ì„œ ë‹¤ë¥¸ speaker ê°„ì˜ ê±°ë¦¬ëŠ” ìµœëŒ€í™”
<center><img src="https://user-images.githubusercontent.com/46676700/95678054-cb96eb00-0c04-11eb-9eb8-6e1a2c6ccb3a.png" alt="img" style="zoom: 80%;" /></center>

<br/>

- query embeddingì˜ í¬ê¸°ì™€ prototypeê³¼ queryì˜ cosine similarityë¥¼ distance metricìœ¼ë¡œ ì‚¬ìš© (**Normalized prototypical, NP**)
<center><img src="https://user-images.githubusercontent.com/46676700/95678059-d2bdf900-0c04-11eb-808e-efe28e67875f.png" alt="img" style="zoom: 80%;" /></center>

- kye et al.[16]ì€ speaker embeddingì„ ë³´ë‹¤ discriminativeí•˜ê²Œ ë§Œë“¤ê¸° ìœ„í•´ global classification lossì™€ í•¨ê»˜ <span style="background-color:#d2d8d8">episodic training*</span>ì„ ì‚¬ìš©
(few-shot taskì™€ ìœ ì‚¬í•œ í˜•íƒœì˜ í›ˆë ¨ taskë¥¼ í†µí•´ ëª¨ë¸ ìŠ¤ìŠ¤ë¡œ í•™ìŠµ ê·œì¹™ì„ ë„ì¶œí•  ìˆ˜ ìˆê²Œ í•¨ìœ¼ë¡œì¨ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë†’ì¼ ìˆ˜ ìˆìŒ [ì°¸ì¡°-kakaobrainBlog](https://www.kakaobrain.com/blog/106))
- global classificationì€ supportì™€ query set ëª¨ë‘ì— ì ìš©
- softmax classification lossë¥¼ í†µí•©í•˜ì—¬ mini-batchì— ìˆëŠ” classë¿ë§Œ ì•„ë‹ˆë¼ ëª¨ë“  classì— ëŒ€í•´ discriminativeí•˜ë„ë¡ embeddingì„ í›ˆë ¨ ê°€ëŠ¥
- **ìµœì¢…ì ì¸ objective function**ì€ ë™ì¼í•œ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ **NPì™€ softmax cross-entropy lossì˜ í•©**(ë‹¨ìˆœ sum)

<br/>

#### <span style="background-color:#aee4ff">**2.2 Instance-wise aggregation**</span>

- ì´ìƒì ì¸ utterance-level embeddingì€ frequencyê°€ ì•„ë‹Œ temporal ìœ„ì¹˜ì— ë”°ë¼ ë‹¬ë¼ì ¸ì•¼í•¨
- 2D convolutional neural networkëŠ” 2D activation mapì„ ìƒì„±í•˜ê¸° ë•Œë¬¸ì— frequency ì¶•ë§Œ ëª¨ë‘ ì—°ê²°ë˜ëŠ” aggregation layerë¥¼ [1]ì—ì„œ ì œì•ˆ
- ë”°ë¼ì„œ pooling layerì— ë“¤ì–´ê°€ê¸° ì „ 1xT feature map ìƒì„±

<br/>

**Temporal Average Pooling(TAP)**

- ë‹¨ìˆœí•˜ê²Œ temporal domainì— ëŒ€í•´ featureì˜ í‰ê· ì„ ì·¨í•¨
<center><img src="https://user-images.githubusercontent.com/46676700/95678556-70ff8e00-0c08-11eb-8d56-7d26175f42c7.png" alt="img" style="zoom: 80%;" /></center>

<br/>

**Self-Attentive Pooling(SAP)**

- ê° ì‹œê°„ì— ëŒ€í•œ frame ëª¨ë‘ ê°™ì€ weightë¥¼ ê°–ëŠ” TAPì™€ ë‹¬ë¦¬, utterance-levelì— ë” ë§ì€ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” frame-levelì— attentioní•¨
<center><img src="https://user-images.githubusercontent.com/46676700/95678562-7ceb5000-0c08-11eb-90d6-3498d822c878.png" alt="img" style="zoom: 80%;" /></center>

> frame-level íŠ¹ì§• $x_t$ê°€ ìš°ì„  parameter Wì™€ bë¥¼ ê°–ëŠ” MLPì˜ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ non-linearí•˜ê²Œ projection(hidden representationìœ¼ë¡œ mapping)

<br/>

- hidden vector $h_t$ì™€ í›ˆë ¨ë˜ëŠ” context vector $\mu$ ì‚¬ì´ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ score(hidden featureì˜ ìƒëŒ€ì ì¸ ì¤‘ìš”ë„)ë¡œ ì‚¬ìš©
- softmax functionì„ í†µí•´ ë‚˜ì˜¨ ê²°ê³¼ë¥¼ ê° frameì˜ ì¤‘ìš”ë„(attention weight)ë¡œ ì‚¬ìš©
- context vectorëŠ” speaker recognitionì— ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” high-level representationìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŒ
<center><img src="https://user-images.githubusercontent.com/46676700/95678567-807ed700-0c08-11eb-8766-296995b8de48.png" alt="img" style="zoom: 80%;" /></center>

<br/>

- utterance-level embedding $e$ëŠ” frame-level íŠ¹ì§•ê³¼ frame-levelì˜ attention weightì™€ ê°€ì¤‘í•©í•˜ì—¬ ì–»ì„ ìˆ˜ ìˆìŒ
<center><img src="https://user-images.githubusercontent.com/46676700/95678569-84aaf480-0c08-11eb-8291-24f23db5b892.png" alt="img" style="zoom: 80%;" /></center>

<br/>

#### <span style="background-color:#aee4ff">**2.3 Pair-wise aggregation**</span>

- ê¸°ì¡´ì˜ instance-wise aggregationê³¼ ë‹¬ë¦¬ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” <span style="background-color:#ffed54">**ë‹¤ë¥¸ utteranceì˜ frame featureë¥¼ ì‚¬ìš©í•˜ì—¬ frame-level featureë¥¼ ëª¨ìœ¼ëŠ” ë°©ë²•**</span>ì„ ì œì•ˆ
- trainingê³¼ testingì˜ ëª©í‘œë¥¼ ë§ì¶”ê¸° ìœ„í—¤ metricê¸°ë°˜ì˜ meta-learning frameworkì¸ prototypical network ì‚¬ìš©

- ì´ frameworkì—ì„œ supportì™€ query set pairë¥¼ ì‚¬ìš©í•˜ì—¬ CAPë¥¼ í›ˆë ¨
- test ì‹œ, support setê³¼ query setì€ enrollmentì™€ test utteranceì— í•´ë‹¹

<br/>

 - queryì™€ support setì˜ ëª¨ë“  utterance pairì— ëŒ€í•´ frame-level representation $s={s_1, s_2,\dots, s_{T_s}}, q={q_1, q_2,\dots, q_{T_q}}$ ì¶”ì¶œ
- meta-projection layer $g_{\Phi}(Â·)$ë¥¼ ì‚¬ìš©í•˜ì—¬ frame-levelì—ì„œ hidden feature ì¶”ì¶œ
- non-linear projectionì„ í†µí•´ ì„ì˜ì˜ frameì— ë¹ ë¥´ê²Œ ì ì‘í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ frame pairì˜ ìœ ì‚¬ë„ë¥¼ ì˜ ì¸¡ì •í•  ìˆ˜ ìˆìŒ
- ì´ layerëŠ” MLPì™€ ReLU activation functionìœ¼ë¡œ êµ¬ì„±
<center><img src="https://user-images.githubusercontent.com/46676700/95679512-50d2cd80-0c0e-11eb-846c-fa3f1bfe0bde.png" alt="img" style="zoom: 80%;" /></center>

- meta-projection layer ì´í›„, ëª¨ë“  frameì— ëŒ€í•œ hidden representationì¸ $S, Q$ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ

> $ S = {S_i}^{T_s}_{i=1}$
>
> $ Q = {Q_i}^{T_q}_{i=1}$
>
> $S_i, Q_i$ ëŠ” ê°ê° $g_{\Phi}(s_i), g_{\Phi}(q_i)$

<br/>

**Correlation matrix**

- Correlation matrix(ìƒê´€í–‰ë ¬) Rì€ ê°€ëŠ¥í•œ ëª¨ë“  frame pairì— ëŒ€í•œ similarityë¥¼ ìš”ì•½

<center><img src="https://user-images.githubusercontent.com/46676700/95679513-55978180-0c0e-11eb-8991-dc7ca123ddc4.png" alt="img" style="zoom: 80%;" /></center>

> $R^Q = (R^S)^T$; ìˆœì„œë§Œ ë°”ë€Œê¸° ë•Œë¬¸ì— $R^S$ì˜ transposeê°€ $R^Q$
>
> $R^S_{1, 1}$; support setì˜ 1ë²ˆì§¸ frame hidden representationê³¼ query setì˜ 1ë²ˆì§¸ frame hidden representationì˜ similarity
>
> ë”°ë¼ì„œ $R^S \in \mathbb{R}^{T_s \times T_q}$; [support set frame ìˆ˜ x query set frame ìˆ˜]

<br/>

**Pair-adaptive attention**

- pair-adaptive context vectorë¥¼ ì–»ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì´ timeì¶•ì— ëŒ€í•´ correlation matrixë¥¼ í‰ê· 
<center><img src="https://user-images.githubusercontent.com/46676700/95679520-5a5c3580-0c0e-11eb-9c92-c802e2e3bcd0.png" alt="img" style="zoom: 80%;" /></center>

> **$\mu_s \in \mathbb{R}^{T_q}$** ì´ê³ , $\mathbb{R}^S_{i,*}$ì€ $i$ë²ˆì§¸ row vector

- ë…¼ë¬¸ì—ì„œ $T_s$ë¡œ ë˜ì–´ìˆëŠ”ë°, $T_s$ê°€ ì•„ë‹Œ $T_q$ì´ ë˜ì–´ì•¼ ìˆ˜ì‹ì ìœ¼ë¡œ ë§ìŒ (ê·¸ë¦¼ì—ì„œëŠ” context vectorì˜ sizeë¥¼ $T_q$ë¡œ í‘œê¸°)
- ê° row vectorëŠ” ë‹¤ë¥¸ utteranceì˜ ëª¨ë“  frameê³¼ì˜ ìœ ì‚¬ë„ ì •ë³´ê°€ ìˆìŒ
- ë”°ë¼ì„œ ë‹¤ë¥¸ utteranceì˜ ê° frameì— ëŒ€í•œ í‰ê·  ìƒê´€ê´€ê³„ë¥¼ $\mu$ë¡œ í‘œì‹œí•  ìˆ˜ ìˆê³ , ì´ëŠ” ë‹¤ë¥¸ utteranceì™€ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€ ê³„ì‚°í•˜ê¸° ìœ„í•´ context vectorë¡œ ì‚¬ìš©

<br/>

- attention weightëŠ” ëª¨ë“  utteranceì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°
<center><img src="https://user-images.githubusercontent.com/46676700/95679528-60521680-0c0e-11eb-9a06-8745d6fa010b.png" alt="img" style="zoom: 80%;" /></center>

> $\tau$ : temperature scaling (attention distributionì˜ ì„ ëª…ë„ ì¡°ì ˆ) - $\tau \rightarrow \infty$ì´ë©´ ë™ì¼í•œ attention weightë¥¼ ê°–ìŒ

<center><img src="https://user-images.githubusercontent.com/46676700/95679531-647e3400-0c0e-11eb-9e6c-b9b0e4c58a0e.png" alt="img" style="zoom: 80%;" /></center>

- Hou et al [22], utterance-levelì˜ íŠ¹ì§•ì„ ì–»ê¸° ìœ„í•´ residual attention mechanismì„ ì‚¬ìš©
- ë‹¤ë¥¸ utteranceì— ëŒ€í•´ì„œë„ ë™ì¼í•œ ë°©ë²•ìœ¼ë¡œ utterance-level feature $q$ë¡œ $e_q$ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ

<br/>

**ì œì•ˆí•˜ëŠ” ë°©ë²•ì˜ procedure**

<center><img src="https://user-images.githubusercontent.com/46676700/95680543-7a432780-0c15-11eb-80a4-709be1187867.png" alt="img" style="zoom: 80%;" /></center>
<center><img src="https://user-images.githubusercontent.com/46676700/95680550-829b6280-0c15-11eb-93fc-0ac5babd0115.png" alt="img" style="zoom: 80%;" /></center>

<br/>

---

<br/>

### â…¢. Experiments ğŸ”

**Model architecture**

<img src="https://user-images.githubusercontent.com/46676700/95680589-cb531b80-0c15-11eb-9d17-c3ead5a27fd8.png" alt="img"/>

<br/>

**Results**

<img src="https://user-images.githubusercontent.com/46676700/95680595-d0b06600-0c15-11eb-8d5a-b8b7166ea620.png" alt="img"/>

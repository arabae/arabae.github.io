<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
    <meta name="renderer" content="webkit">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <meta http-equiv="Cache-Control" content="no-transform"/>
    <meta http-equiv="Cache-Control" content="no-siteapp"/>
    <title>Conformer: Convolution-augmented Transformer for Speech Recognition</title>
	<meta name="description" content="">
	<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="apple-touch-icon" href="/style/image/favicon.png"/>
    <link href="/style/css/highlight.min.css" rel="stylesheet">
    <link href="/style/css/style.min.css" rel="stylesheet">
	<link rel="stylesheet" href="/style/css/iconfont/iconfont.css">
	
	  <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$'] ],
    processEscapes: true,
  }
});
MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
	  alert("Math Processing Error: "+message[1]);
	});
MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
	  alert("Math Processing Error: "+message[1]);
	});
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
	
</head>

<body class="bg-grey" gtools_scp_screen_capture_injected="true">

    <header id="header" class="header bg-white">
    <div class="navbar-container">
        <a href="/" class="navbar-logo">
            <img src="/style/image/logo.png" alt="ARa's DevBlog" />
            <span>ARa's DevBlog</span>
        </a>
        <div class="navbar-menu">
            <a href="/archives">Archives</a>
            <a href="/about">About</a>
        </div>
        <div class="navbar-mobile-menu" onclick="">
            <span class="icon-menu cross"><span class="middle"></span></span>
            <ul>
                <li><a href="/archives">Archives</a></li>
                <li><a href="/about">About</a></li>
            </ul>
        </div>
    </div>
</header>

    <article class="main-content page-page" itemscope itemtype="http://schema.org/Article">
    <div class="post-header">
        <h1 class="post-title" itemprop="name headline">
            Conformer: Convolution-augmented Transformer for Speech Recognition
        </h1>
        <div class="post-data">
            <time itemprop="datePublished">Aug 03, 2021</time>
        </div>
    </div>
</article>
<div class="main-container">
    <div class="post-container">
        <div class="navigation" id="navigation">
            <h1>Contents</h1>
            <div class="nav sidenav">
	    </div>
        </div>
        <article class="post-content">
            <ul id="markdown-toc">
  <li><a href="#abstract" id="markdown-toc-abstract"><strong><em>Abstract</em></strong></a></li>
  <li><a href="#1-introduction" id="markdown-toc-1-introduction"><strong><em>1. Introduction</em></strong></a></li>
  <li><a href="#2-conformerencoder" id="markdown-toc-2-conformerencoder"><em>2. ConformerEncoder</em></a>    <ul>
      <li><a href="#21-multi-headed-self-attention-module" id="markdown-toc-21-multi-headed-self-attention-module">2.1. Multi-Headed Self-Attention Module</a></li>
      <li><a href="#22-convolution-module" id="markdown-toc-22-convolution-module">2.2. Convolution Module</a></li>
      <li><a href="#23-feedforward-module" id="markdown-toc-23-feedforward-module">2.3. FeedForward Module</a></li>
      <li><a href="#24-conformer-block" id="markdown-toc-24-conformer-block">2.4. Conformer Block</a></li>
    </ul>
  </li>
  <li><a href="#3-experiments" id="markdown-toc-3-experiments"><em>3. Experiments</em></a>    <ul>
      <li><a href="#31-data" id="markdown-toc-31-data">3.1 Data</a></li>
      <li><a href="#32-conformer-tranducer" id="markdown-toc-32-conformer-tranducer">3.2 Conformer Tranducer</a></li>
      <li><a href="#33-results-on-librispeech" id="markdown-toc-33-results-on-librispeech">3.3 Results on LibriSpeech</a></li>
      <li><a href="#34-ablation-studies" id="markdown-toc-34-ablation-studies">3.4 Ablation Studies</a></li>
    </ul>
  </li>
  <li><a href="#4-conclusion" id="markdown-toc-4-conclusion"><em>4. Conclusion</em></a>    <ul>
      <li><a href="#further-reading" id="markdown-toc-further-reading"><strong>Further reading</strong></a></li>
    </ul>
  </li>
</ul>

<p><span style="font-size:13pt">Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, Ruoming Pang</span></p>

<h1 id="abstract"><strong><em>Abstract</em></strong></h1>

<p>ìµœê·¼ Transformer ë° Convolution neural network(CNN) ê¸°ë°˜ ëª¨ë¸ì€ Automatic Speech Recognition(ASR)ì—ì„œ Recurrent neural networks (RNNs)ë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ì•„ ê¸°ëŒ€ë˜ëŠ” ê²°ê³¼ë¥¼ ë³´ì„</p>

<p>Transformer ëª¨ë¸ì€ content-based global interactionì„ ì˜ í¬ì°©í•˜ëŠ” ë°˜ë©´ CNNì€ local featureë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•¨</p>
<ul>
  <li>parameter-efficient ë°©ì‹ìœ¼ë¡œ audio sequenceì˜ local ë° global dependencyë¥¼ ëª¨ë‘ ëª¨ë¸ë§í•˜ê¸° ìœ„í•´ CNNê³¼ Transformerë¥¼ ê²°í•©í•˜ëŠ” ë°©ë²•ì„ ì—°êµ¬í•˜ì—¬ ë‘ ì„¸ê³„ì˜ ì¥ì ì„ ëª¨ë‘ ë‹¬ì„±</li>
</ul>

<p><strong>â‡’ Conformerë¼ëŠ” ìŒì„± ì¸ì‹ì„ ìœ„í•œ Convolution-Augmented Transformerë¥¼ ì œì•ˆ</strong></p>

<p>ConformerëŠ” SOTA ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ëŠ” ì´ì „ Transformer ë° CNN ê¸°ë°˜ ëª¨ë¸ë³´ë‹¤ í›¨ì”¬ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ê°€ì ¸ì˜´</p>

<p>LibriSpeech ë²¤ì¹˜ë§ˆí¬ ì‚¬ìš©</p>
<ul>
  <li>WER 2.1% / 4.3% (language model X) - test/testother</li>
  <li>WER 1.9% / 3.9% (language model O)</li>
  <li>WER 2.7% / 6.3% (small model, only 10M parameter)</li>
</ul>

<h1 id="1-introduction"><strong><em>1. Introduction</em></strong></h1>

<p>NNê¸°ë°˜ì˜ End-to-End ASR systemì€ ìµœê·¼ ëª‡ ë…„ ë™ì•ˆ í¬ê²Œ ê°œì„ ë¨</p>

<p>RNNì€ audio sequenceì˜ temproal dependencyë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ëª¨ë¸ë§í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ASRì— ëŒ€í•´ ì‚¬ì‹¤ìƒ ì¼ë°˜ì ì¸ ì„ íƒ</p>

<p>ìµœê·¼ self-attentionì— ê¸°ë°˜ì˜ <strong>transformer êµ¬ì¡°</strong>ëŠ” <strong>long distance interactionì„ capture</strong>í•˜ëŠ” ëŠ¥ë ¥ê³¼ <strong>high training efficiency</strong>ë¡œ sequence ëª¨ë¸ë§ì— ì£¼ë¡œ ì‚¬ìš©ë¨</p>

<p>ë”ë¶ˆì–´, CNNë„ <strong>local receptive field layer</strong>ë¥¼ í†µí•´ <strong>ì ì§„ì ìœ¼ë¡œ local contextë¥¼ capture</strong>í•˜ì—¬ ASRì—ì„œë„ ì„±ê³µì </p>

<p>ê·¸ëŸ¬ë‚˜ self-attention ë˜ëŠ” CNN ëª¨ë¸ì€ ê°ê° í•œê³„ì ì´ ì¡´ì¬</p>

<blockquote>
  <p><strong><em>Transformers</em></strong></p>
</blockquote>

<ul>
  <li>long-range global context patternì— íš¨ê³¼ì </li>
  <li>ì„¸ë¶„í™”ëœ local feature patternì„ ì¶”ì¶œí•˜ëŠ” ëŠ¥ë ¥ì€ ë–¨ì–´ì§</li>
</ul>

<blockquote>
  <p><strong><em>CNN</em></strong></p>
</blockquote>

<ul>
  <li>local ì •ë³´ë¥¼ í™œìš©í•˜ê³ , visionì—ì„œ ì‚¬ì‹¤ìƒ computational blockìœ¼ë¡œ ì‚¬ìš©ë¨</li>
  <li><a href="#further-reading">translation equivariance</a>ë¥¼ ìœ ì§€í•˜ê³  edgeì™€ shapeê³¼ ê°™ì€ featureë¥¼ captureí•  ìˆ˜ ìˆëŠ” local windowë¥¼ í†µí•´ shared position-based kernelì„ í•™ìŠµ</li>
  <li>local connectivityë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ global informationì„ captureí•˜ê¸° ìœ„í•´ì„  ë” ë§ì€ layerì™€ parameterê°€ í•„ìš”í•˜ë‹¤ëŠ” ì œí•œì´ ì¡´ì¬</li>
</ul>

<p>ì´ëŸ¬í•œ ë¬¸ì œì ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ë™ì‹œì— ì—°êµ¬ëœ <strong>contextnet</strong>ì€ ë” ê¸´ contextë¥¼ capture í•˜ê¸° ìœ„í•´ <strong>ê° residual blockì— squeeze-and-excitation moduleì„ ë‘š</strong></p>
<ul>
  <li>ê·¸ëŸ¬ë‚˜ ì „ì²´ sequenceì— ëŒ€í•´ <strong>global averageë§Œ ì ìš©</strong>í•˜ê¸° ë•Œë¬¸ì— <strong>dynamicí•œ global context</strong>ë¥¼ captureí•˜ê¸°ì—” ì—¬ì „íˆ <strong>ì œí•œì </strong>ì„</li>
</ul>

<p>ìµœê·¼ ì—°êµ¬ì— ë”°ë¥´ë©´ CNNê³¼ self-attentionì„ ê²°í•©í•˜ë©´ ê°œë³„ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤ í–¥ìƒë˜ì—ˆìŒ</p>

<ul>
  <li>position-wise local featureë¥¼ ëª¨ë‘ í•™ìŠµí•˜ê³  content-based global interactionì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ</li>
  <li>ë™ì‹œì— [15, 16]ê³¼ ê°™ì€ ë…¼ë¬¸ì€ equivarianceì„ ìœ ì§€í•˜ëŠ” ìƒëŒ€ì  ìœ„ì¹˜ ê¸°ë°˜ ì •ë³´ë¡œ self-attentionì„ ê°•í™”í•¨</li>
  <li>Wu et al. [17]ì€ ì…ë ¥ì„ self-attentionê³¼ convolutionì˜ ë‘ ê°€ì§€ branchë¡œ ë¶„í• í•˜ê³  ì¶œë ¥ì„ ì—°ê²°í•˜ëŠ” multi-branch architectureë¥¼ ì œì•ˆ
    <ul>
      <li>ì´ taskëŠ” mobile applicationì„ ëŒ€ìƒìœ¼ë¡œ í–ˆìœ¼ë©°, machine translation taskì˜ ê°œì„ ì„ ë³´ì—¬ì¤Œ</li>
    </ul>
  </li>
</ul>

<center><img src="https://user-images.githubusercontent.com/46676700/128826541-f87104f7-5b5e-41c9-9081-29db15b294bf.png" alt="img" style="zoom:40%;" /></center>

<p>ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ASRì—ì„œ CNNê³¼ self-attentionì„ ìœ ê¸°ì (organically)ìœ¼ë¡œ ê²°í•©í•˜ëŠ” ë°©ë²•ì„ ì—°êµ¬
globalê³¼ local interactionì´ parameter íš¨ìœ¨ì„±ì„ ìœ„í•´ ì¤‘ìš”í•˜ë‹¤ê³  ê°€ì •
â†’ ì´ë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ self-attentionê³¼ convolutionì˜ ìƒˆë¡œìš´ ì¡°í•©ì´ ë‘ê°œì˜ ì¥ì ì„ ëª¨ë‘ ë‹¬ì„±í•  ê²ƒì´ë¼ê³  ì œì•ˆ</p>

<p>self-attentionì€ global interationì„ í•™ìŠµí•˜ëŠ” ë°˜ë©´ convolutionì€ relative-offset-based local correlationë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ captureí•¨</p>
<ul>
  <li>Wu et al. [17, 18],ì—ì„œ ì˜ê°ì„ ë°›ì•˜ê³ , ê·¸ë¦¼ 1ê³¼ ê°™ì´ í•œ ìŒì˜ feedforward module ì‚¬ì´ì— ë¼ì›Œì§„ self-attentionê³¼ convolutionì˜ ìƒˆë¡œìš´ ì¡°í•©ì„ ì†Œê°œ!</li>
</ul>

<blockquote>
  <p><strong><em>Conformer</em></strong></p>
</blockquote>

<p>ì´ì „ SOTA Transformer Transducer[7]ì™€ ë¹„êµ</p>
<ul>
  <li>LibriSpeech dataset ì‚¬ìš© (ì™¸ë¶€ language modelì´ ìˆëŠ” testother ë°ì´í„° ì…‹ì—ì„œ ìƒëŒ€ì ìœ¼ë¡œ 15% í–¥ìƒ)</li>
</ul>

<p>10M, 30M, 118M parameter í¬ê¸°ë¥¼ ê°–ëŠ” ëª¨ë¸ ë¹„êµ</p>
<ul>
  <li>10M: test/testotherì—ì„œ 2.7%/6.3%ë¡œ ìœ ì‚¬í•œ í¬ê¸°ì˜ ë‹¤ë¥¸ ëª¨ë¸[10]ê³¼ ë¹„êµí–ˆì„ ë•Œ ê°œì„ ë¨</li>
  <li>30M: 139M parameterë¥¼ ì‚¬ìš©í•˜ëŠ” transformer transducer[7]ë³´ë‹¤ ê°œì„ ë¨</li>
  <li>118M: ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  2.1%/4.3%, ì‚¬ìš©í•˜ë©´ 1.9%/3.9% ì„±ëŠ¥ì„ ë³´ì„</li>
</ul>

<p>â• attention head ìˆ˜, convolution kernel size, activation fuction, feedforward layer ë°°ì¹˜, convolution moduleì„ transformerê¸°ë°˜ networkì— ì¶”ê°€í•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²•ì˜ íš¨ê³¼ì— ëŒ€í•´ ê¹Šì´ ì—°êµ¬í•˜ê³ , ê°ê°ì´ ì–´ë–»ê²Œ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚¤ëŠ”ì§€ ì´ˆì ì„ ë‘š</p>

<p><img src="https://user-images.githubusercontent.com/46676700/128826558-cf9ff480-0a20-4313-804d-569ac4c39e3e.png" alt="img" style="zoom:60%;" /></p>

<h1 id="2-conformerencoder"><em>2. ConformerEncoder</em></h1>

<p>audio encoderëŠ” ë¨¼ì € convolution subsampling layerì„ ì‚¬ìš©í•´ ì…ë ¥ì„ ì²˜ë¦¬í•˜ê³ , ë‹¤ìŒì— fig1ê³¼ ê°™ì´ ì—¬ëŸ¬ conformer blockì„ ê±°ì¹¨</p>

<p>ë³¸ ë…¼ë¬¸ modelì˜ êµ¬ë³„ë˜ëŠ” íŠ¹ì§•ì€ [7, 19]ì—ì„œ transformer block ë¶€ë¶„ì´ conformer blockìœ¼ë¡œ ì‚¬ìš©ë¨</p>

<p>conformer blockì€ 4ê°œì˜ module(feed-forward module, self-attention module, convolution module, second feed-forward module)ì´ í•¨ê»˜ ìŒ“ì—¬ êµ¬ì„±ë¨</p>

<p>section 2.1, 2 and 2.3ì—ì„œëŠ” ê°ê° self-attention, convolution, feed-forward moduleì„ ì†Œê°œí•˜ê³ , ë§ˆì§€ë§‰ìœ¼ë¡œ 2.4ì—ì„œëŠ” ì´ëŸ¬í•œ í•˜ìœ„ blockì´ ì–´ë–»ê²Œ ê²°í•©ë˜ëŠ”ì§€ ì„¤ëª…</p>

<h3 id="21-multi-headed-self-attention-module">2.1. Multi-Headed Self-Attention Module</h3>

<p>relative sinusoidal(sin ê³¡ì„ ) positional encoding ë°©ì‹ì¸ Transformer-XLì˜ ì¤‘ìš”í•œ ê¸°ìˆ ì„ í†µí•©í•˜ë©´ì„œ multi-head self-attention (MHSA)ë¥¼ ì‚¬ìš©</p>

<p><strong>ğŸ’¡ relative positional encoding</strong></p>
<ul>
  <li>self-attention moduleì´ ë‹¤ë¥¸ ì…ë ¥ ê¸¸ì´ì— ëŒ€í•´ ë”ìš± ì˜ ì¼ë°˜í™”í•  ìˆ˜ ìˆë„ë¡ í•¨</li>
  <li>resulting encoderëŠ” ë°œí™” ê¸¸ì´ì˜ ë³€í™”ì— ëŒ€í•´ ë” ê°•ì¸í•¨</li>
</ul>

<p>ë” ê¹Šì€ ëª¨ë¸ì„ í›ˆë ¨í•˜ê³ , ì •ê·œí™”í•˜ëŠ”ë° ë„ì›€ì´ ë˜ëŠ” dropoutê³¼ í•¨ê»˜ pre-norm residual unitì„ ì‚¬ìš©í•¨</p>

<p>ì•„ë˜ì˜ ê·¸ë¦¼ 3ì€ multi-head self-attention module blockì„ ë‚˜íƒ€ëƒ„</p>

<center><img src="https://user-images.githubusercontent.com/46676700/128826564-520cebdc-c97e-45b1-8349-2842c44f6ca0.png" alt="img" style="zoom:40%;" /></center>

<h3 id="22-convolution-module">2.2. Convolution Module</h3>

<p>[17]ì—ì„œ ì˜ê°ì„ ë°›ì•„ convolution moduleì€ pointwise convolutionê³¼ gated linear unit(glu)ì¸ gating mechanismìœ¼ë¡œ ì‹œì‘</p>

<p>ê·¸ ë‹¤ìŒ 1D depthwise convolution layerê°€ ì´ì–´ì§€ê³ , Batchnormì€ deep ëª¨ë¸ í›ˆë ¨ì„ ë•ê¸° ìœ„í•´ convolution ì§í›„ì— ìœ„ì¹˜í•¨</p>

<p>ê·¸ë¦¼ 2ëŠ” convolution blockì„ ë‚˜íƒ€ëƒ„</p>

<p><img src="https://user-images.githubusercontent.com/46676700/128827668-4697e2e9-3d33-49e7-9968-28a8af2a70e8.png" alt="img" style="zoom:60%;" /></p>

<h3 id="23-feedforward-module">2.3. FeedForward Module</h3>

<p>[6]ì—ì„œ ì œì•ˆëœ Transformer êµ¬ì¡°ëŠ” MHSA layer ì´í›„ feed-forward moduleì´ ì´ì–´ì§€ê³ , two linear transformation ì‚¬ì´ì— nonlinear activationì´ ì¡´ì¬í•¨</p>

<p>residual connectiondms feed-forward layer ìœ„ì— ì¶”ê°€ë˜ê³  layer normalizationì´ ì´ì–´ì§</p>

<p>ì´ êµ¬ì¡°ëŠ” Transformer ASR model [7, 24]ì—ë„ ì ìš©ë¨</p>

<p><img src="https://user-images.githubusercontent.com/46676700/128826571-ee6a4944-a20c-4625-98ba-99df6b0fc53c.png" alt="img" style="zoom:60%;" /></p>

<p>pre-norm residual unit[21, 22]ì„ ë”°ë¥´ê³ , residual unitì•ˆì— ì²« ë²ˆì§¸ linear layer ì´ì „ ì…ë ¥ì—ì„œ layer normalizationì„ ì ìš©í•¨</p>

<p>ë˜í•œ, Swish activation ë° dropoutì„ ì ìš©í•˜ì—¬ networkë¥¼ ì •ê·œí™”í•˜ëŠ”ë° ë„ì›€ì„ ì¤Œ</p>

<p>ê·¸ë¦¼ 4ëŠ” Feed-Forward Network(FFN) moduleì„ ë‚˜íƒ€ëƒ„</p>

<h3 id="24-conformer-block">2.4. Conformer Block</h3>

<p>ì œì•ˆí•œ conformer blockì—ëŠ” ê·¸ë¦¼ 1ê³¼ ê°™ì´ <strong>multi-head self-attention moduleê³¼ convolution module ì‚¬ì´ì— 2ê°œì˜ feed-forward module</strong>ì´ í¬í•¨ë¨</p>
<ul>
  <li>ì´ ìƒŒë“œìœ„ì¹˜ êµ¬ì¡°ëŠ” transformer blockì˜ ì›ë˜ feed-forward layerë¥¼ 2ê°œì˜ half-step feed-forwar layer(attention layer ì „ í›„ë¡œ ë°°ì¹˜)ë¡œ ëŒ€ì²´í•œ Macaron-Net[18]ì—ì„œ ì˜ê°ì„ ì–»ì—ˆìŒ</li>
  <li>Macron-Netì—ì„œì™€ ê°™ì´ ë³¸ ë…¼ë¬¸ì˜ feed-forward layerì—ì„œ half-step residual weightë¥¼ ì‚¬ìš©í•¨</li>
</ul>

<p>ë‘ë²ˆì§¸ feed-forward module ë‹¤ìŒì— ìµœì¢… layernorm layerê°€ ì˜´</p>

<p>ìˆ˜í•™ì ìœ¼ë¡œ conformer block iì— ëŒ€í•œ ì…ë ¥ $x_i$ì— ëŒ€í•´ blockì˜ ì¶œë ¥ $y_i$ê°€ ë‹¤ìŒê³¼ ê°™ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•¨</p>

<p>$\tilde{x_i} = x_i + \frac{1}{2}FFN(x_i)$
$xâ€™_i = \tilde{x_i} + MHSA(\tilde{x_i})$</p>

<p>$xâ€™â€˜_i = xâ€™_i + Conv(xâ€™_i)$</p>

<p>$y_i = Layernorm(xâ€™â€˜_i + \frac{1}{2}FFN(xâ€™â€˜_i))$</p>

<p>section 3.4.3ì—ì„œ ì´ì „ ì‘ì—…ì—ì„œ ì‚¬ìš©ëœ <strong>vanilla FFNê³¼ Macron-styleì˜ half-step FFNì„ ë¹„êµ</strong>í•¨</p>

<ul>
  <li>2ê°œì˜ macaron-net style feed-forward layer ì‚¬ì´ì— attention moduleê³¼ convolution moduleì„ ë¼ì›Œë„£ëŠ” half-step residual connectionì´ ìˆëŠ”ê²Œ conformer architectureì—ì„œ ë‹¨ì¼ feed-forward moduleì„ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤ <strong>ìƒë‹¹íˆ ê°œì„ </strong>ëœë‹¤ëŠ” ê²ƒì„ ë°œê²¬í•¨</li>
</ul>

<p>convolutionê³¼ self-attentionì˜ ì¡°í•©ì€ ì´ì „ì— ì—°êµ¬ë˜ì—ˆìœ¼ë©° ì´ë¥¼ ë‹¬ì„±í•˜ëŠ” ë§ì€ ë°©ë²•ì„ ìƒìƒí•  ìˆ˜ ìˆì—ˆìŒ
self-attentionìœ¼ë¡œ convolutionì„ ì¦ê°€ì‹œí‚¤ëŠ” ë‹¤ì–‘í•œ ì˜µì…˜ì€ section 3.4.2ì— ì‘ì„±</p>

<p>â‡’ <strong>self-attention module ë’¤ì— ìŒ“ì¸ convolution module</strong>ì´ ìŒì„± ì¸ì‹ì— ê°€ì¥ ì˜ ì‘ë™í•˜ëŠ” ê²ƒì„ ë°œê²¬</p>

<h1 id="3-experiments"><em>3. Experiments</em></h1>

<h3 id="31-data">3.1 Data</h3>

<p>970ì‹œê°„ labeled speechì™€ language model êµ¬ì¶•ì„ ìœ„í•œ ì¶”ê°€ 800M word token textì „ìš© corpusë¡œ êµ¬ì„±ëœ LibriSpeech datasetì—ì„œ ì œì•ˆëœ ëª¨ë¸ì„ í‰ê°€</p>
<ul>
  <li>25ms window, 10ms stride</li>
  <li>80-channel filterbank feature</li>
</ul>

<p>SpecAugment [27, 28] with mask parameter (F=27)ì™€ ìµœëŒ€ time-mask ratio(ps=0.05)ë¥¼ ê°€ì§„ 10ê°œ time mask ì‚¬ìš©</p>
<ul>
  <li>time msakì˜ ìµœëŒ€ sizeëŠ” ë°œí™” ê¸¸ì´ * psë¡œ ì„¤ì •</li>
</ul>

<h3 id="32-conformer-tranducer">3.2 Conformer Tranducer</h3>

<p>network ê¹Šì´, model dimension, attention head ìˆ˜ì˜ ë‹¤ì–‘í•œ ì¡°í•©ì„ ìŠ¤ìœ„í•‘í•˜ê³ , model parameter size ì œì•½ ë‚´ì—ì„œ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ì„ ì„ íƒí•´ 10M, 30M, 118M  parameterë¥¼ ì‚¬ìš©í•˜ì—¬ ì†Œ, ì¤‘, ëŒ€ ì„¸ê°€ì§€ ëª¨ë¸ì„ ì‹ë³„</p>
<ul>
  <li>ëª¨ë“  ëª¨ë¸ì—ì„œ single-LSTM layer decoderë¥¼ ì‚¬ìš©</li>
</ul>

<p>í‘œ 1ì€ architecture hyperparameterë¥¼ ë³´ì—¬ì¤Œ</p>

<p><img src="https://user-images.githubusercontent.com/46676700/128827078-593e8915-0585-42e0-b603-f3974ec64f4d.png" alt="img" style="zoom:60%;" /></p>

<ul>
  <li><strong>dropout</strong>: module ì…ë ¥ì— ì¶”ê°€ë˜ê¸° ì „ì— conformerì˜ ê° residual unit, ì¦‰ ê° moduleì˜ ì¶œë ¥ì— ì ìš© (ë¹„ìœ¨ $P_{drop}$ = 0.1)</li>
  <li><strong>Variational noise</strong>[5, 30]</li>
  <li><strong>L2 regularization</strong>: 1e-6 weight (ëª¨ë“  í•™ìŠµ ê°€ëŠ¥í•œ wightì— ì¶”ê°€)</li>
  <li><strong>Adam</strong> optimizer(Î²1 = 0.9, Î²2 = 0.98, Îµ = 10âˆ’9)</li>
  <li><strong>transformer</strong> <strong>learning rate schedule</strong> (10k warm-up step, ìµœëŒ€ learning rate $\frac{0.05}{\sqrt{d}}$ (d: model dimension)</li>
  <li><strong>3-layer LSTM LM</strong> (width 4096)
    <ul>
      <li>LibriSpeech 960hì—ì„œ êµ¬ì¶•ëœ 1k Words Per Minute(WPM)ìœ¼ë¡œ tokenized LibriSpeech960h transcriptê°€ ì¶”ê°€ëœ LibriSpeech language model corpusì—ì„œ í›ˆë ¨</li>
      <li>LMì€ dev-set transcriptì˜ word-level perplexity(í˜¼ë€ë„)ê°€ 63.9</li>
      <li>shallow fusionì— ëŒ€í•œ LM weigth Î»ëŠ” grid searchë¥¼ í†µí•´ dev-setì—ì„œ ì¡°ì •</li>
    </ul>
  </li>
</ul>

<p>â‡’ ëª¨ë“  ëª¨ë¸ì€ <strong>Lingvo toolkit</strong>ìœ¼ë¡œ êµ¬í˜„</p>

<h3 id="33-results-on-librispeech">3.3 Results on LibriSpeech</h3>

<p><img src="https://user-images.githubusercontent.com/46676700/128827091-238c5479-203d-4918-b555-655df0c6614a.png" alt="img" style="zoom:60%;" /></p>

<p>í‘œ 2ëŠ” LibriSpeech test-clean/test-otherì— ëŒ€í•œ ëª¨ë¸ì˜ WER ê²°ê³¼ë¥¼ ContextNet, Transformer transducer ë° QuartzNetì„ í¬í•¨í•œ ëª‡ ê°€ì§€ ìµœì‹  ëª¨ë¸ê³¼ ë¹„êµ</p>
<ul>
  <li>ëª¨ë“  í‰ê°€ ê²°ê³¼ëŠ” ì†Œìˆ˜ì  ì´í•˜ 1ìë¦¬ë¡œ ë°˜ì˜¬ë¦¼</li>
</ul>

<p><strong>ì–¸ì–´ ëª¨ë¸ X</strong></p>
<ul>
  <li>ì¤‘ê°„ ëª¨ë¸ì˜ ì„±ëŠ¥ì€ test/testotherì—ì„œ ì´ë¯¸ ê°€ì¥ ì˜ ì•Œë ¤ì§„ Transformer, LSTM ê¸°ë°˜ ëª¨ë¸ ë˜ëŠ” ìœ ì‚¬í•œ í¬ê¸°ì˜ convolution ëª¨ë¸ì„ ëŠ¥ê°€í•˜ëŠ” 2.3/5.0ë¡œ ê²½ìŸë ¥ ìˆëŠ” ê²°ê³¼ë¥¼ ë‹¬ì„±</li>
</ul>

<p><strong>ì–¸ì–´ ëª¨ë¸ O</strong></p>
<ul>
  <li>ëª¨ë“  ê¸°ì¡´ ëª¨ë¸ ì¤‘ ê°€ì¥ ë‚®ì€ WER</li>
  <li>single NNì—ì„œ Transformerì™€ convolutionì„ ê²°í•©í•˜ëŠ” ê²ƒì˜ íš¨ìœ¨ì„±ì„ ë¶„ëª…íˆ ë³´ì—¬ì¤Œ</li>
</ul>

<h3 id="34-ablation-studies">3.4 Ablation Studies</h3>

<blockquote>
  <p><strong><em>3.4.1. Conformer Block vs Transformer Block</em></strong></p>
</blockquote>

<p>Conformer blockì€ ì—¬ëŸ¬ ë°©ë©´ì—ì„œ Transformer blockê³¼ ë‹¤ë¦„</p>

<p>íŠ¹íˆ, macaron-styleì˜ convolution blockê³¼ ì´ë¥¼ ë‘˜ëŸ¬ì‹¼ FFN pairê°€ ì¡´ì¬
â‡’ ì´ parameter ìˆ˜ë¥¼ ë³€ê²½í•˜ì§€ ì•Šê³ , conformer blockì„ transformer blockìœ¼ë¡œ ë³€ê²½í•˜ì—¬ ì°¨ì´ë¥¼ í™•ì¸</p>

<p>í‘œ 3ëŠ” conformer blockì— ëŒ€í•œ ê° ë³€í˜•ì˜ ì˜í–¥ì„ ë‚˜íƒ€ëƒ„</p>

<p><img src="https://user-images.githubusercontent.com/46676700/128827098-dde8d71e-599e-405c-b83e-ad70b5fe9e0e.png" alt="img" style="zoom:60%;" /></p>

<p>ëª¨ë“  ì°¨ì´ì  ì¤‘ì—ì„œ <strong>convolution sub-block</strong>ì´ ê°€ì¥ ì¤‘ìš”í•œ featureì´ì§€ë§Œ macaron-styleì˜ FFN pairë¥¼ ê°–ëŠ” ê²ƒì´ ë™ì¼í•œ ìˆ˜ì˜ parameterë¥¼ ê°–ëŠ” single FFNë³´ë‹¤ ë” íš¨ê³¼ì </p>

<p>swish activationì„ ì‚¬ìš©í•˜ë©´ Conformer ëª¨ë¸ì—ì„œ ë” ë¹ ë¥¸ ìˆ˜ë ´ì´ ì´ë£¨ì–´ì§</p>

<blockquote>
  <p><strong><em>3.4.2 Combinations of Convolution and Transformer Modules</em></strong></p>
</blockquote>

<p>MHSA moduleê³¼ convolution moduleì„ ê²°í•©í•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²•ì˜ íš¨ê³¼ë¥¼ ì—°êµ¬</p>

<ol>
  <li>convolution moduleì˜ depthwise convolutionì„ lightweight convolution[35]ìœ¼ë¡œ êµì²´ ì‹œë„
    <ul>
      <li>íŠ¹íˆ, dev-other datasetì—ì„œ ì„±ëŠ¥ì´ í¬ê²Œ ë–¨ì–´ì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŒ</li>
    </ul>
  </li>
  <li>Conformer ëª¨ë¸ì—ì„œ MHSA module ì•ì— convolution moduleì„ ë°°ì¹˜
    <ul>
      <li>dev-otherì—ì„œ 0.1ë§Œí¼ ê²°ê³¼ê°€ ì €í•˜ì‹œí‚¤ëŠ” ê²ƒì„ ë°œê²¬</li>
    </ul>
  </li>
  <li>[17]ì—ì„œ ì œì•ˆí•œ ëŒ€ë¡œ outputì´ ì—°ê²°ëœ multi-head self-attention moduleê³¼ convolution moduleì˜ parallel branchë¡œ inputì„ ë¶„í• 
    <ul>
      <li>ì œì•ˆí•œ architectureì™€ ë¹„êµí•  ë•Œ ì„±ëŠ¥ì„ ì•…í™”ì‹œí‚¨ë‹¤ëŠ” ê²ƒì„ ë°œê²¬</li>
    </ul>
  </li>
</ol>

<p>â‡’ í‘œ 4ëŠ” Conformer blockì—ì„œ self-attention module ë’¤ì— convolution moduleì„ ë°°ì¹˜í•˜ëŠ” ì´ì ì„ ì‹œì‚¬í•¨</p>

<p><img src="https://user-images.githubusercontent.com/46676700/128826577-dfee6a64-4c88-426e-8652-9ea83a8a39de.png" alt="img" style="zoom:60%;" /></p>

<blockquote>
  <p><strong><em>3.4.3. Macaron Feed Forward Modules</em></strong></p>
</blockquote>

<p>Transformer ëª¨ë¸ì—ì„œì™€ ê°™ì´ attention block ì´í›„ single FFN ëŒ€ì‹  Conformer blockì—ëŠ” self-attention ë° convolution module ì‚¬ì´ì— macaronê³¼ ê°™ì€ í•œ ìŒì˜ feed-forward moduleì´ ìˆìŒ</p>

<p>ë˜í•œ, Conformer feed-forward moduleì€ half-step residuleê³¼ í•¨ê»˜ ì‚¬ìš©ë¨</p>

<p>í‘œ 5ëŠ” single FFN ë˜ëŠ” ì „ì²´ full-step residualì„ ì‚¬ìš©í•´ Conformer blockì„ ë³€ê²½í•  ë•Œ ê²°ê³¼ë¥¼ ë‚˜íƒ€ëƒ„</p>
<ul>
  <li>ì°¨ì´ê°€ ë§ì´ ì—†ì§€ë§Œ, macaron style feed-forward moduleì´ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/46676700/128827103-fde4055b-51d1-48d7-a372-6e8e0624c306.png" alt="img" style="zoom:60%;" /></p>

<blockquote>
  <p><strong><em>3.4.4. Number of Attention Heads</em></strong></p>
</blockquote>

<p>self-attentionì—ì„œ ê° attention headëŠ” ì…ë ¥ì˜ ë‹¤ë¥¸ ë¶€ë¶„ì— ì´ˆì ì„ ë§ì¶”ì–´ í•™ìŠµí•˜ì—¬ ë‹¨ìˆœí•œ weighted average ì´ìƒìœ¼ë¡œ predictë¥¼ ê°œì„ í•  ìˆ˜ ìˆìŒ</p>

<p>large ëª¨ë¸ì—ì„œ ëª¨ë“  layerì—ì„œ 4~32ê¹Œì§€ ë™ì¼í•œ ìˆ˜ì˜ attention headë¥¼ ë³€ê²½í•˜ë©´ì„œ ì‚¬ìš©í•´ íš¨ê³¼ë¥¼ ì—°êµ¬í•˜ê¸° ìœ„í•´ ì‹¤í—˜ì„ ìˆ˜í–‰</p>

<p>í‘œ 6ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ íŠ¹íˆ dev-other datasetì— ëŒ€í•´ attention headë¥¼ ìµœëŒ€ 16ê¹Œì§€ ì¦ê°€ì‹œí‚¤ë©´ ì •í™•ë„ê°€ í–¥ìƒëœë‹¤ëŠ” ê²ƒì„ ë°œê²¬</p>

<p><img src="https://user-images.githubusercontent.com/46676700/128826587-7f85fcb1-4ec0-4660-8df9-5d54144b5562.png" alt="img" style="zoom:60%;" /></p>

<blockquote>
  <p><strong><em>3.4.5. Ablation study on depthwise convolution kernel sizes</em></strong></p>
</blockquote>

<p>depthwise convolutionì—ì„œ kernel sizeì˜ ì˜í–¥ì„ ì—°êµ¬í•˜ê¸° ìœ„í•´ ëª¨ë“  layerì— ëŒ€í•´ ë™ì¼í•œ kernel sizeë¥¼ ì‚¬ìš©í•´ large ëª¨ë¸ì—ì„œ kernel sizeë¥¼ {3, 7, 17, 32, 65}ë¡œ ìŠ¤ìœ•í•˜ì—¬ ì‹¤í—˜</p>

<p>kernel size 17ê³¼ 32ê¹Œì§€ sizeê°€ í´ìˆ˜ë¡ ì„±ëŠ¥ì´ í–¥ìƒë˜ì§€ë§Œ, í‘œ 7ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ size 65ì˜ ê²½ìš°ì—ëŠ” ì„±ëŠ¥ì´ ì•…í™”ëœë‹¤ëŠ” ê²ƒì„ ë°œê²¬</p>

<p>dev WERì—ì„œ ì†Œìˆ˜ ë‘˜ì§¸ìë¦¬ë¥¼ ë¹„êµí•˜ë©´ ë¹„êµí•˜ë©´ ë‚˜ë¨¸ì§€ë³´ë‹¤ size 32ê°€ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì„</p>

<p><img src="https://user-images.githubusercontent.com/46676700/128826596-fede2ba9-ac3d-4a18-b5ac-0c2a6196592e.png" alt="img" style="zoom:60%;" /></p>

<h1 id="4-conclusion"><em>4. Conclusion</em></h1>

<p>ë³¸ ëª¬ë¬¸ì—ì„œëŠ” End-to-End speech recognitionì„ ìœ„í•´ <strong>CNN ë° Transformerì˜ êµ¬ì„± ìš”ì†Œë¥¼ í†µí•©</strong>í•˜ëŠ” architectureì¸ <strong>Conformerë¥¼ ë„ì…</strong></p>

<p>ê° êµ¬ì„± ìš”ì†Œì˜ ì¤‘ìš”ì„±ì„ ì—°êµ¬í•´ Convolution moduleì„ í¬í•¨í•˜ëŠ” ê²ƒì´ Conformer ì„±ëŠ¥ì— ì¤‘ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤Œ</p>

<p>LibriSpeech datasetì— ëŒ€í•œ ì´ì „ modelë³´ë‹¤ ë” ì ì€ parameterë¡œ í–¥ìƒëœ ì •í™•ë„ë¥¼ ë³´ì„</p>
<ul>
  <li><strong>test/test-otherì— ëŒ€í•´ 1.9%/3.9%ë¡œ SOTA ë‹¬ì„±</strong></li>
</ul>

<hr />

<h3 id="further-reading"><strong>Further reading</strong></h3>
<p><strong>ğŸ’¡ translation equivariance</strong><br />
<a href="https://chriswolfvision.medium.com/what-is-translation-equivariance-and-why-do-we-use-convolutions-to-get-it-6f18139d4c59">What is translation equivariance, and why do we use convolutions to get it?</a></p>

<p><strong>ğŸ’¡ Transformerì™€ êµ¬ì¡°ì ìœ¼ë¡œ ë¹„êµ</strong><br />
<a href="https://github.com/kakaobrain/nlp-paper-reading/blob/master/notes/conformer.md">kakaobrain/nl-paper-reading</a></p>

        </article>
        <div class="post-content">
         <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<div id="gitalk-container"></div>

<script data-no-instant type="text/javascript">
const gitalk = new Gitalk({
  clientID: '5304ae17c5ba0ce3b2aa',
  clientSecret: '22801b52c86101ff074072faa1631475954d2936',
  repo: 'arabae.github.io',
  owner: 'arabae',
  admin: ['arabae'],
  id: location.pathname,      // Ensure uniqueness and length less than 50
  distractionFreeMode: true  // Facebook-like distraction free mode
})

gitalk.render('gitalk-container')
</script>
        </div>
    </div>
</div>

    
    <footer id="footer" class="footer bg-white">
    <div class="footer-social">
        <div class="footer-container clearfix">
            <div class="social-list">
		<a href="/"><span class='iconfont icon-home'></span>&nbsp;&nbsp;HOME</a>
                <a rel="nofollow" target="_blank" href="https://github.com/arabae"><span class='iconfont icon-github'></span>&nbsp;&nbsp;Github</a>
                <a target="_blank" href="/feed.xml"><span class='iconfont icon-rss'></span>&nbsp;&nbsp;RSS</a>
            </div>
        </div>
    </div>
    <div class="footer-meta">
        <div class="footer-container">
            <div class="meta-item meta-copyright">
                <div class="meta-copyright-info">
                    <a href="https://github.com/arabae" class="info-logo">
                        <img src="/style/image/logo.png" alt="wonder">
                    </a>
                    <div class="info-text">
                        <p>Copyright &copy; 2021 - 2021 <a href="https://github.com/arabae"><code>ARa Bae</code></a></p>
                        <p>Powered by <a href="http://jekyllrb.com" target="_blank" rel="nofollow"><code>jekyll</code></a>ï¼Œtheme is <a href="https://github.com/lightfish-zhang/pinghsu-jekyll" target="_blank" rel="nofollow"><code>pinghsu</code></a></p>
                    </div>
                </div>
            </div>

            <div class="meta-item meta-posts">
                <h3 class="meta-title">RECENT POSTS</h3>
                
                    <li>
                        <a href="/2021-08-03-Conformer">Conformer: Convolution-augmented Transformer for Speech Recognition</a>
                    </li>
                
                    <li>
                        <a href="/2021-01-25-Very-Deep-Convolutional-Networks-for-Large-Scale-Image-Recognition">Very Deep Convolutional Networks for Large-Scale Image Recognition</a>
                    </li>
                
                    <li>
                        <a href="/2020-10-13-Cross-Attentive-Pooling-for-SV">Cross attentive pooling for speaker verification</a>
                    </li>
                
                    <li>
                        <a href="/2020-10-06-Metric-Laerning-for-Keyword-Spotting">Metric Learning for Keyword Spotting</a>
                    </li>
                
                    <li>
                        <a href="/2019-07-30-Attention-based-models-for-TDSV">Attention-based Models For Text-dependent Speaker Verification</a>
                    </li>
                
                    <li>
                        <a href="/2019-07-24-TISV-with-Adversarial-Learning-on-Short-Utterances">Text-Independent Speaker Verification with Adversarial Learning on Short Utterances</a>
                    </li>
                
                    <li>
                        <a href="/2019-07-10-GE2E-loss-for-SV">Generalized End to End Loss For Speaker Verification</a>
                    </li>
                
            </div>

        </div>
    </div>
</footer>

<!-- #end -->
<script src="//cdn.bootcss.com/jquery/1.10.1/jquery.min.js"></script>
<script>
	!window.jQuery && document.write(unescape('%3Cscript src="/style/js/jquery.min.js"%3E%3C/script%3E'))
</script>
<script src="/style/js/headroom.min.js"></script>
<script src="/style/js/nav.min.js"></script>
<script type="text/javascript">
    var header = new Headroom(document.getElementById("header"), {
        tolerance: 10,
        offset : 80,
        classes: {
            initial: "animated",
            pinned: "slideDown",
            unpinned: "slideUp"
        }
    });
    header.init();
</script>

<script>window.SmoothScrollOptions = { stepSize: 36 }</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/smoothscroll/1.4.8/SmoothScroll.min.js"></script>
<script>
	!window.SmoothScroll && document.write(unescape('%3Cscript src="/style/js/SmoothScroll.min.js"%3E%3C/script%3E'))
</script>




  </body>
</html>

<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
    <meta name="renderer" content="webkit">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <meta http-equiv="Cache-Control" content="no-transform"/>
    <meta http-equiv="Cache-Control" content="no-siteapp"/>
    <title>Attention-based Models For Text-dependent Speaker Verification</title>
	<meta name="description" content="">
	<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="apple-touch-icon" href="/style/image/favicon.png"/>
    <link href="/style/css/highlight.min.css" rel="stylesheet">
    <link href="/style/css/style.min.css" rel="stylesheet">
	<link rel="stylesheet" href="/style/css/iconfont/iconfont.css">
</head>

<body class="bg-grey" gtools_scp_screen_capture_injected="true">

    <header id="header" class="header bg-white">
    <div class="navbar-container">
        <a href="/" class="navbar-logo">
            <img src="/style/image/logo.png" alt="ARa's DevBlog" />
            <span>ARa's DevBlog</span>
        </a>
        <div class="navbar-menu">
            <a href="/archives">Archives</a>
            <a href="/about">About</a>
        </div>
        <div class="navbar-mobile-menu" onclick="">
            <span class="icon-menu cross"><span class="middle"></span></span>
            <ul>
                <li><a href="/archives">Archives</a></li>
                <li><a href="/about">About</a></li>
            </ul>
        </div>
    </div>
</header>

    <article class="main-content page-page" itemscope itemtype="http://schema.org/Article">
    <div class="post-header">
        <h1 class="post-title" itemprop="name headline">
            Attention-based Models For Text-dependent Speaker Verification
        </h1>
        <div class="post-data">
            <time itemprop="datePublished">Jul 30, 2019</time>
        </div>
    </div>
</article>
<div class="main-container">
    <div class="post-container">
        <div class="navigation" id="navigation">
            <h1>Contents</h1>
            <div class="nav sidenav">
	    </div>
        </div>
        <article class="post-content">
            <ul id="markdown-toc">
  <li><a href="#-abstract" id="markdown-toc--abstract">ğŸ“Œ <strong>Abstract</strong></a></li>
  <li><a href="#â…°-introduction" id="markdown-toc-â…°-introduction"><strong>â… . Introduction</strong></a></li>
  <li><a href="#â…±-baseline-architecture" id="markdown-toc-â…±-baseline-architecture"><strong>â…¡. Baseline Architecture</strong></a>    <ul>
      <li><a href="#te2e-model" id="markdown-toc-te2e-model"><strong>TE2E model</strong></a></li>
    </ul>
  </li>
  <li><a href="#â…²-attention-based-model" id="markdown-toc-â…²-attention-based-model"><strong>â…¢. Attention-based Model</strong></a>    <ul>
      <li><a href="#31-basic-attention-layer" id="markdown-toc-31-basic-attention-layer"><strong>3.1 Basic attention layer</strong></a></li>
      <li><a href="#32-scoring-functions" id="markdown-toc-32-scoring-functions"><strong>3.2 Scoring functions</strong></a></li>
      <li><a href="#33-attention-layer-variants" id="markdown-toc-33-attention-layer-variants"><strong>3.3 Attention layer variants</strong></a></li>
      <li><a href="#34-weights-pooling" id="markdown-toc-34-weights-pooling"><strong>3.4 Weights pooling</strong></a></li>
    </ul>
  </li>
  <li><a href="#â…³-experiments" id="markdown-toc-â…³-experiments"><strong>â…£. Experiments</strong></a>    <ul>
      <li><a href="#41-datasets-and-basic-setup" id="markdown-toc-41-datasets-and-basic-setup"><strong>4.1 Datasets and basic setup</strong></a></li>
      <li><a href="#42-basic-attention-layer" id="markdown-toc-42-basic-attention-layer"><strong>4.2 Basic attention layer</strong></a></li>
      <li><a href="#43-variants" id="markdown-toc-43-variants"><strong>4.3 Variants</strong></a></li>
      <li><a href="#44-weights-pooling" id="markdown-toc-44-weights-pooling"><strong>4.4 Weights pooling</strong></a></li>
    </ul>
  </li>
  <li><a href="#â…´--conclusion" id="markdown-toc-â…´--conclusion"><strong>â…¤.  Conclusion</strong></a></li>
</ul>

<p><span style="font-size:13pt">F A Rezaur Rahman Chowdhury, Quan Wang, Ignacio Lopez Moreno, Li Wan</span></p>

<h1 id="-abstract">ğŸ“Œ <strong>Abstract</strong></h1>

<ul>
  <li>Attention ê¸°ë°˜ ëª¨ë¸ : ì…ë ¥ sequenceì˜ ì „ì²´ ê¸¸ì´ë¥¼ ìš”ì•½í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥</li>
  <li>ìŒì„± ì¸ì‹, ê¸°ê³„ ë²ˆì—­, ì´ë¯¸ì§€ ìº¡ì…˜ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ê³³ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì„</li>
  <li>End-to-End Text-dependent í™”ì ì¸ì‹ ì‹œìŠ¤í…œì—ì„œ attention mechanism ì‚¬ìš©ì„ ë¶„ì„</li>
  <li>ë‹¤ì–‘í•œ attention layerì˜ ë³€í˜•ì„ ì—°êµ¬í•˜ê³  attention weightì— ëŒ€í•œ ë‹¤ì–‘í•œ poolingë°©ë²•ì„ ë¹„êµ</li>
  <li>Attention mechanismì„ ì‚¬ìš©í•˜ì§€ ì•Šì€ LSTMê³¼ ì„±ëŠ¥ ë¹„êµ</li>
</ul>

<p><br /></p>

<hr />

<p><br /></p>

<h1 id="â…°-introduction"><strong>â… . Introduction</strong></h1>

<p><strong>âœ” Global Password Text-dependent Speaker Verification(SV) ì‹œìŠ¤í…œ</strong></p>

<ul>
  <li>ë“±ë¡ ë° í…ŒìŠ¤íŠ¸ ë°œí™”ê°€ íŠ¹ì • ë‹¨ì–´ë¡œ ì œí•œ (Text-dependent)</li>
  <li>â€œOk-Googleâ€ê³¼ â€œHey Googleâ€ ì‚¬ìš© ( Global password)</li>
</ul>

<p><br /></p>

<p><strong>âœ” í˜„ì¬ ê°€ì¥ ë§ì´ ì ‘ê·¼í•˜ê³  ìˆëŠ” í›ˆë ¨ ë°©ë²•</strong></p>

<ul>
  <li>ë“±ë¡ ë° í…ŒìŠ¤íŠ¸í•˜ëŠ” ë‹¨ê³„ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” End-to-End êµ¬ì¡°</li>
  <li>[6]ë…¼ë¬¸ â€œi-vector+PLDA ì‹œìŠ¤í…œì„ ê·¸ëŒ€ë¡œ ëª¨ë°©í•œ êµ¬ì¡°â€ì˜ ê²½ìš°, ë” ë‚˜ì€ ì„±ëŠ¥ì„ ìœ„í•´ ëª¨ë¸ì„ ê·œì œí•˜ì˜€ìœ¼ë‚˜ ì´ˆê¸°í™”ë¥¼ ìœ„í•´ ê¸°ì¡´ì˜ i-vectorì™€ PLDA ëª¨ë¸ì´ í•„ìš”</li>
  <li>[7] ë…¼ë¬¸, TD-SV taskì—ì„œ LSTM ë„¤íŠ¸ì›Œí¬ê°€ ê¸°ì¡´ End-to-End DNNë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œ</li>
</ul>

<p><br /></p>

<p><strong>âœ”ì´ì „ ë…¼ë¬¸ì—ì„œì˜ ë¬¸ì œì </strong></p>

<ul>
  <li>ë¬µìŒê³¼ ë°°ê²½ ì¡ìŒì´ ë§ì´ ì—†ìŒ</li>
  <li>ë³¸ ë…¼ë¬¸ì—ì„œëŠ” keyword ê²€ì¶œì— ì˜í•´ ë¶„í• ëœ 800msì˜ ì§§ì€ frameì´ì§€ë§Œ, ë¬µìŒê³¼ ì¡ìŒì´ ìˆìŒ</li>
</ul>

<p><br /></p>

<p><strong>âœ”ì´ìƒì ì¸ Embedding ìƒì„±</strong></p>

<ul>
  <li>ìŒì†Œì— í•´ë‹¹í•˜ëŠ” frameì„ ì‚¬ìš©í•˜ì—¬ ì œì‘</li>
  <li>ì…ë ¥ sequence ì¤‘ ê´€ë ¨ì„±ì´ ë†’ì€ ìš”ì†Œë¥¼ ê°•ì¡°í•˜ê¸° ìœ„í•´ attention layer ì‚¬ìš©</li>
</ul>

<p><br /></p>

<hr />

<p><br /></p>

<h1 id="â…±-baseline-architecture"><strong>â…¡. Baseline Architecture</strong></h1>

<h3 id="te2e-model"><strong>TE2E model</strong></h3>

<p><strong>âœ”  baseline end-to-end training architecture</strong></p>

<center><img src="https://user-images.githubusercontent.com/46676700/94424981-1573e000-01c6-11eb-8bf5-4890542a60db.png" alt="img" style="zoom: 80%;" /></center>

<ul>
  <li>í›ˆë ¨ ë‹¨ê³„ì—ì„œ, í•˜ë‚˜ì˜ í‰ê°€ìš© ë°œí™” ğ’™ğ‘—~ì™€ Nê°œì˜ ë“±ë¡ ë°œí™” ğ’™ğ‘˜ğ‘› (ğ‘“ğ‘œğ‘Ÿ ğ‘›=1, â€¦, ğ‘) tupleì´ LSTM networkì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©</li>
</ul>

<blockquote>
  <p>${x_{j\tilde{}}, (x_{k_1}, â€¦, x_{k_N})}$ ; input<br />
$x$ : ê³ ì • ê¸¸ì´ì˜ log-mel fiterbank feature<br />
$j, k$ : ë°œí™”í•œ í™”ì ($j$ì™€ $k$ëŠ” ê°™ì„ ìˆ˜ ìˆìŒ)<br />
ë§Œì•½ $x_{j\tilde{}}$ì™€ $M$ ê°œì˜ ë“±ë¡ ë°œí™”ê°€ ê°™ì€ í™”ìë¼ë©´ tuple positive $(j=k)$, ë‹¤ë¥´ë©´ negative</p>
</blockquote>

<ul>
  <li>â„ğ‘¡ : të²ˆì§¸ frameì—ì„œ LSTMì˜ ë§ˆì§€ë§‰ layerì˜ ì¶œë ¥ ( ê³ ì • ì°¨ì›ì˜ vector )</li>
  <li>ë§ˆì§€ë§‰ frameì˜ outputì„ d-vector ğ (â„ğ‘‡) ë¡œ ì •ì˜</li>
</ul>

<blockquote>
  <p>${\omega(j\tilde{}), (\omega(k_1), â€¦, \omega(k_N))}$ ; output<br />
Tuple $(\omega(k_1), â€¦, \omega(k_N))$ì„ í‰ê· ë‚´ì–´ centroid ê³„ì‚°</p>
</blockquote>

<p><br /></p>

<center><img src="https://user-images.githubusercontent.com/46676700/94425430-e447df80-01c6-11eb-9148-c79bd11b149b.png" alt="img" style="zoom:80%;" /></center>

<p><br /></p>

<p><strong>âœ”  Cosine Similarity Function ì •ì˜</strong></p>

<center><img src="https://user-images.githubusercontent.com/46676700/94425434-e611a300-01c6-11eb-990c-2a6bc83ad06b.png" alt="img" style="zoom:80%;" /></center>

<p><br /></p>

<p><strong>âœ”  Loss Function ì •ì˜</strong></p>

<center><img src="https://user-images.githubusercontent.com/46676700/94425445-e7db6680-01c6-11eb-9729-e41c138555a5.png" alt="img" style="zoom: 80%;" /></center>

<p><br /></p>

<hr />

<p><br /></p>

<h1 id="â…²-attention-based-model"><strong>â…¢. Attention-based Model</strong></h1>

<h3 id="31-basic-attention-layer"><strong>3.1 Basic attention layer</strong></h3>

<p><strong>âœ”  Baseline systemê³¼ ì°¨ì´ì </strong></p>

<ul>
  <li>ë§ˆì§€ë§‰ frameì˜ ì¶œë ¥ì„ d-vector(ğ)ë¡œ ì§ì ‘ ì‚¬ìš©</li>
  <li>Attention layerëŠ” ê° t frame ì—ì„œì˜ LSTM ì¶œë ¥ â„ğ‘¡ì— ëŒ€í•œ ìŠ¤ì¹¼ë¼ ì ìˆ˜ ğ‘’ğ‘¡ ë¥¼ í›ˆë ¨í•˜ì—¬ weighted sumí•œ ê²°ê³¼ë¡œ d-vector(ğ) ì •ì˜</li>
</ul>

<center><img src="https://user-images.githubusercontent.com/46676700/94430186-76071b00-01ce-11eb-8ae9-0fdf5abcf182.png" alt="img" style="zoom: 80%;" /></center>

<ul>
  <li>Normalized weight ğ›¼ğ‘¡ì™€ weighted sumí•œ ê²°ê³¼ d-vectorëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜</li>
</ul>

<center><img src="https://user-images.githubusercontent.com/46676700/94430336-ac449a80-01ce-11eb-8094-4fcf8644fec6.png" alt="img" style="zoom: 80%;" /></center>

<center><img src="https://user-images.githubusercontent.com/46676700/94430342-ae0e5e00-01ce-11eb-9395-90efff7c8674.png" alt="img" style="zoom: 80%;" /></center>

<p><br /></p>

<ul>
  <li><strong>aritectureë¡œ ë³´ëŠ” ì°¨ì´ì </strong></li>
</ul>
<center><img src="https://user-images.githubusercontent.com/46676700/94430460-eca41880-01ce-11eb-9807-6a7dea6d97fa.png" alt="img" style="zoom: 80%;" /></center>

<p><br /></p>

<h3 id="32-scoring-functions"><strong>3.2 Scoring functions</strong></h3>

<ul>
  <li>Bias-only attention
ì—¬ê¸°ì„œ bğ‘¡ëŠ” scalar. LSTM ì¶œë ¥ hğ‘¡ì— ì˜ì¡´í•˜ì§€ ì•ŠìŒ.</li>
</ul>

<center><img src="https://user-images.githubusercontent.com/46676700/94430647-34c33b00-01cf-11eb-87f5-e43a51edc41a.png" alt="img" style="zoom: 80%;" /></center>

<ul>
  <li>Linear attention
ì—¬ê¸°ì„œ wğ‘¡ëŠ” mì°¨ì› vector, bğ‘¡ëŠ” scalar. frameë§ˆë‹¤ ë‹¤ë¥¸ parameterê°€ ì‚¬ìš©</li>
</ul>

<center><img src="https://user-images.githubusercontent.com/46676700/94430651-368cfe80-01cf-11eb-85a2-d759801a1634.png" alt="img" style="zoom: 80%;" /></center>

<ul>
  <li>Shared-parameter linear attention
ëª¨ë“  frameì— ëŒ€í•´ mì°¨ì› vector  wì™€ scalar bê°€ ë™ì¼í•˜ê²Œ ì‚¬ìš©</li>
</ul>

<center><img src="https://user-images.githubusercontent.com/46676700/94430653-37be2b80-01cf-11eb-95d0-af0d4afd142b.png" alt="img" style="zoom: 80%;" /></center>

<ul>
  <li>Non-linear attention
ì—¬ê¸°ì„œ ğ‘¾ğ’•ëŠ” mâ€™ X m matrix, ğ›ğ‘¡ì™€ ğ¯ğ‘¡ëŠ” mâ€™ì°¨ì›ì˜ vector(ì°¨ì› mâ€™ì€ í›ˆë ¨ ë°ì´í„° ì…‹ì—ì„œ ì¡°ì •)</li>
</ul>

<center><img src="https://user-images.githubusercontent.com/46676700/94430710-50c6dc80-01cf-11eb-8673-5af3e52f4b04.png" alt="img" style="zoom: 80%;" /></center>

<ul>
  <li>Shared-parameter non-linear attention
ëª¨ë“  í”„ë ˆì„ì— ëŒ€í•´ ë™ì¼í•œ parameter ğ–, ğ›, ğ¯ ë¥¼ ê³µìœ </li>
</ul>

<center><img src="https://user-images.githubusercontent.com/46676700/94430715-51f80980-01cf-11eb-9b90-9a302bca378a.png" alt="img" style="zoom: 80%;" /></center>

<p><br /></p>

<h3 id="33-attention-layer-variants"><strong>3.3 Attention layer variants</strong></h3>

<ul>
  <li>ê¸°ë³¸ì ì¸ attention layerì™€ ë‹¬ë¦¬ ë‘ê°€ì§€ì˜ ë³€í˜•ëœ ê¸°ë²• Cross-layer attentionì™€ Divided-layer attention ì†Œê°œ</li>
</ul>

<p><strong>âœ” Cross-layer attention</strong></p>

<ul>
  <li>ê¸°ì¡´ì˜ ë°©ë²• : ë§ˆì§€ë§‰ LSTMì˜ layerì˜ ì¶œë ¥ hğ‘¡ (1â‰¤ğ‘¡â‰¤ğ‘‡)ë¥¼ ì‚¬ìš©í•˜ì—¬ score eğ‘¡ì™€ weight Î±ğ‘¡ë¥¼ ê³„ì‚°</li>
  <li>ë³€í˜•ëœ ë°©ë²• : ì¤‘ê°„ LSTM layerì˜ ì¶œë ¥ hâ€™ğ‘¡(1â‰¤ğ‘¡â‰¤ğ‘‡)ìœ¼ë¡œ ê³„ì‚° (ê·¸ë¦¼ 3.(a) outputì—ì„œ ë§ˆì§€ë§‰ 2ë²ˆì§¸ layerë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°)</li>
  <li>d-vector ğëŠ” ì—¬ì „íˆ ë§ˆì§€ë§‰ layer ì¶œë ¥ hğ‘¡ì™€ weighted sumìœ¼ë¡œ ê³„ì‚°</li>
</ul>

<center><img src="https://user-images.githubusercontent.com/46676700/94431728-9df77e00-01d0-11eb-83a4-7694a369266d.png" alt="img" style="zoom: 80%;" /></center>

<p><br /></p>

<p><strong>âœ” Divided-layer attention</strong></p>

<ul>
  <li>ë§ˆì§€ë§‰ LSTM layerì˜ ì¶œë ¥ hğ‘¡ì˜ ì°¨ì›ì„ 2ë°°ë¡œ ëŠ˜ë¦¬ê³  ê·¸ ì°¨ì›ì„ part aì™€ part b ë‘ ë¶€ë¶„ìœ¼ë¡œ ê· ë“±í•˜ê²Œ ë‚˜ëˆ”</li>
  <li>part bë¥¼ ì‚¬ìš©í•˜ì—¬ weightë¥¼ ê³„ì‚°í•˜ê³ , ë‚˜ë¨¸ì§€ part aì™€ weighted sumí•˜ì—¬ d-vector ìƒì„±</li>
</ul>

<center><img src="https://user-images.githubusercontent.com/46676700/94431901-e1ea8300-01d0-11eb-80d9-464a2cafaf02.png" alt="img" style="zoom: 80%;" /></center>

<p><br /></p>

<h3 id="34-weights-pooling"><strong>3.4 Weights pooling</strong></h3>

<p><strong>âœ” Basic attention layerì˜ ë˜ ë‹¤ë¥¸ ë³€í™”</strong></p>

<ul>
  <li>LSTMì˜ output â„ë¥¼ averageí•˜ê¸° ìœ„í•´ normalized weight ğ›¼ğ‘¡ ë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì§€ ì•Šê³ , maxpoolingìœ¼ë¡œ ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©</li>
</ul>

<p><strong>âœ” ë‘ ê°€ì§€ maxpooling ë°©ë²• ì‚¬ìš©</strong></p>

<ul>
  <li>Sliding Window maxpooling : Sliding windowì•ˆì˜ weight ì¤‘ í° ê°’ë§Œ ë‘ê³ , ë‚˜ë¨¸ì§€ëŠ” 0ìœ¼ë¡œ ë§Œë“¦</li>
  <li>Global top-K maxpooling : ê°€ì¥ í° Kê°œì˜ ê°’ë§Œ ë‘ê³ , ë‚˜ë¨¸ì§€ëŠ” 0ìœ¼ë¡œ ë§Œë“¦</li>
</ul>

<center><img src="https://user-images.githubusercontent.com/46676700/94432216-63421580-01d1-11eb-8235-ee4f90a727af.png" alt="img" style="zoom: 80%;" /></center>

<blockquote>
  <p>të²ˆì§¸ pixel : ê°€ì¤‘ì¹˜ $\alpha_t$<br />
ë°ì„ ìˆ˜ë¡ ê°€ì¤‘ì¹˜ê°€ í° ê°’ì„ ì˜ë¯¸</p>
</blockquote>

<p><br /></p>

<hr />

<p><br /></p>

<h1 id="â…³-experiments"><strong>â…£. Experiments</strong></h1>

<h3 id="41-datasets-and-basic-setup"><strong>4.1 Datasets and basic setup</strong></h3>

<p><strong>âœ”  ì‚¬ìš©í•œ Dataset</strong></p>

<ul>
  <li>â€œOk Googleâ€ê³¼ â€œHey Googleâ€ì´ í˜¼í•©ëœ ë°œí™” ë°ì´í„°</li>
  <li>ì•½ 630K í™”ìê°€ 150M ë°œí™” (í…ŒìŠ¤íŠ¸ ë°ì´í„° : 665ëª… í™”ì)</li>
  <li>í‰ê· ì ìœ¼ë¡œ enrollmentëŠ” 4.5ê°œ, evaluationì€ 10ê°œì˜ ë°œí™”ë¡œ êµ¬ì„±</li>
</ul>

<p><strong>âœ”  Basic setup</strong></p>

<ul>
  <li>ê¸°ë³¸ baselineì€ 3ê°œì˜ layerë¡œ ì´ë£¨ì–´ì§„ LSTM</li>
  <li>ê° layerëŠ” 128ì°¨ì›ì´ë©°, 64ì°¨ì›ìœ¼ë¡œ projectioní•˜ëŠ” linear layerë¥¼ ê°€ì§€ê³  ìˆìŒ</li>
  <li>Global passwordë§Œ í¬í•¨í•˜ëŠ” ê¸¸ì´ T=80 frame(800ms)ì˜ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë¶„ë¦¬í•˜ëŠ” keyword detection í›„ 40ì°¨ì›ì˜ log-mel-filterbank feature ìƒì„±</li>
  <li>MultiReaderê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ë‘ ê°œì˜ keywordë¥¼ í˜¼í•©í•˜ì—¬ ì‚¬ìš©</li>
</ul>

<h3 id="42-basic-attention-layer"><strong>4.2 Basic attention layer</strong></h3>

<ul>
  <li>ë‹¤ì–‘í•œ ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ Basic attention layerê³¼ ë¹„êµ</li>
</ul>

<center><img src="https://user-images.githubusercontent.com/46676700/94432416-b4eaa000-01d1-11eb-8141-99d48b30f3da.png" alt="img" style="zoom: 80%;" /></center>

<ul>
  <li>Bias-onlyì™€ linear attentionì€ EERì´ ê±°ì˜ ê°œì„ ë˜ì§€ ì•ŠìŒ</li>
  <li>Non-linear ì¤‘ íŠ¹íˆ, shared-parameterì˜ ê²½ìš° ì„±ëŠ¥ í–¥ìƒì´ ìˆìŒ</li>
</ul>

<h3 id="43-variants"><strong>4.3 Variants</strong></h3>

<ul>
  <li>Basic attention layerì™€ ë‘ ê°€ì§€ ë³€í˜•(cross-layer, divided-layer) ë¹„êµ</li>
  <li>ì´ì „ ì‹¤í—˜ì—ì„œ ìµœê³ ì˜ ì„±ëŠ¥ì„ ë‚¸ shared-parameter non-linear scoring functionì„ ì‚¬ìš©</li>
</ul>

<center><img src="https://user-images.githubusercontent.com/46676700/94432517-d8ade600-01d1-11eb-8325-50d593324e2b.png" alt="img" style="zoom: 80%;" /></center>

<ul>
  <li>cross-layerëŠ” ë§ˆì§€ë§‰ì—ì„œ 2ë²ˆì§¸ layerì—ì„œ scoreë¥¼ í›ˆë ¨</li>
  <li>divided-layer attentionì´ ë§ˆì§€ë§‰ LSTM layerì˜ ì°¨ì›ì´ 2ë°°ì´ì§€ë§Œ, Basic attentionê³¼ cross-layer attentionë³´ë‹¤ ì•½ê°„ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì„</li>
</ul>

<h3 id="44-weights-pooling"><strong>4.4 Weights pooling</strong></h3>

<ul>
  <li>Attention weightë¥¼ ë‹¤ì–‘í•œ poolingë°©ë²•ìœ¼ë¡œ ì‚¬ìš©í•œ ê²ƒê³¼ ë¹„êµ</li>
  <li>Shared-parameter non-linear scoring functionê³¼ divided-layer attention ì‚¬ìš©</li>
  <li>Sliding window maxpooling : 10 frame window sizeì™€ 5 frame step size</li>
  <li>Global top-K maxpooling : K = 5</li>
</ul>

<center><img src="https://user-images.githubusercontent.com/46676700/94432565-ea8f8900-01d1-11eb-856a-11c004b078e2.png" alt="img" style="zoom: 80%;" /></center>

<ul>
  <li>Sliding window maxpoolingì´ EERì´ ì•½ê°„ ë” ë‚®ì€ ê²ƒì„ í™•ì¸</li>
</ul>

<p><strong>âœ” ê° ë°©ë²•ì—ì„œ attention weightë¥¼ visualization</strong></p>

<center><img src="https://user-images.githubusercontent.com/46676700/94433266-03e50500-01d3-11eb-8044-2e31658644e1.png" alt="img" /></center>

<ul>
  <li>Poolingì´ ì—†ì„ ë•Œ, 4ìŒì†Œ(O-kay-Goo-gle) ë˜ëŠ” 3ìŒì†Œ(Hey-Goo-gle) íŒ¨í„´ì„ í™•ì¸</li>
  <li>Poolingì„ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ ì‹œì‘ë¶€ë¶„ ë³´ë‹¤ëŠ” ëë¶€ë¶„ì˜ ë°œí™”ê°€ ë” í° attention weightë¥¼ ê°€ì§</li>
  <li>LSTMì€ ì´ì „ ìƒíƒœ ê°’ì„ ëˆ„ì í•˜ì—¬ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì— ë§ˆì§€ë§‰ìœ¼ë¡œ ê°ˆìˆ˜ë¡ ë” ë§ì€ ì •ë³´ë¥¼ ê°€ì§ìœ¼ë¡œì¨ ë‚˜ì˜¤ê²Œ ë˜ëŠ” í˜„ìƒìœ¼ë¡œ íŒë‹¨</li>
</ul>

<p><br /></p>

<hr />

<p><br /></p>

<h1 id="â…´--conclusion"><strong>â…¤.  Conclusion</strong></h1>

<ul>
  <li>
    <p>ë³¸ ë…¼ë¬¸ì—ì„œëŠ” keyword ê¸°ë°˜ì˜ Text-dependent í™”ì ê²€ì¦ ì‹œìŠ¤í…œì„ ìœ„í•œ ë‹¤ì–‘í•œ Attention mechanismì„ ì‹¤í—˜</p>
  </li>
  <li>ê°€ì¥ ì¢‹ì€ ë°©ë²•
    <ol>
      <li>shared-parameter non-linear scoring function ì‚¬ìš©</li>
      <li>LSTMì˜ ë§ˆì§€ë§‰ layerì— divided-layer attention ì‚¬ìš©</li>
      <li>Sliding window maxpoolingì„ attention weightì— ì ìš©</li>
    </ol>
  </li>
  <li>
    <p>ìœ„ì˜ 3ê°€ì§€ë¥¼ ê²°í•©í•˜ì˜€ì„ ë•Œ ê¸°ë³¸ LSTMëª¨ë¸ EER 1.72%ì—ì„œ 14%ì˜ ìƒëŒ€ì  ì„±ëŠ¥ í–¥ìƒì„ ê°€ì ¸ì˜´</p>
  </li>
  <li><span style="color:#FF0000"><strong>ë™ì¼í•œ attention mechanism(íŠ¹íˆ, shared-parameter scoring function)ì€ Text-independentí•œ í™”ì ê²€ì¦ ë° í™”ì ì‹ë³„ì„ ê°œì„ í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë  ìˆ˜ ìˆìŒ</strong></span></li>
</ul>

        </article>
        <div class="post-content">
         <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<div id="gitalk-container"></div>

<script data-no-instant type="text/javascript">
const gitalk = new Gitalk({
  clientID: '5304ae17c5ba0ce3b2aa',
  clientSecret: '22801b52c86101ff074072faa1631475954d2936',
  repo: 'arabae.github.io',
  owner: 'arabae',
  admin: ['arabae'],
  id: location.pathname,      // Ensure uniqueness and length less than 50
  distractionFreeMode: true  // Facebook-like distraction free mode
})

gitalk.render('gitalk-container')
</script>
        </div>
    </div>
</div>

    
    <footer id="footer" class="footer bg-white">
    <div class="footer-social">
        <div class="footer-container clearfix">
            <div class="social-list">
                <a href="/"><span class='iconfont icon-home'></span>&nbsp;&nbsp;HOME</a>
                <a rel="nofollow" target="_blank" href="https://github.com/arabae"><span class='iconfont icon-github'></span>&nbsp;&nbsp;Github</a>
                <a target="_blank" href="/feed.xml"><span class='iconfont icon-rss'></span>&nbsp;&nbsp;RSS</a>
            </div>
        </div>
    </div>
    <div class="footer-meta">
        <div class="footer-container">
            <div class="meta-item meta-copyright">
                <div class="meta-copyright-info">
                    <a href="https://github.com/arabae" class="info-logo">
                        <img src="/style/image/logo.png" alt="wonder">
                    </a>
                    <div class="info-text">
                        <p>Copyright &copy; 2021 - 2021 <a href="https://github.com/arabae"><code>ARa Bae</code></a></p>
                        <p>Powered by <a href="http://jekyllrb.com" target="_blank" rel="nofollow"><code>jekyll</code></a>ï¼Œtheme is <a href="https://github.com/lightfish-zhang/pinghsu-jekyll" target="_blank" rel="nofollow"><code>pinghsu</code></a></p>
                    </div>
                </div>
            </div>

            <div class="meta-item meta-posts">
                <h3 class="meta-title">RECENT POSTS</h3>
                
                    <li>
                        <a href="/2019-10-13-Cross-Attentive-Pooling-for-SV">Cross attentive pooling for speaker verification</a>
                    </li>
                
                    <li>
                        <a href="/2019-10-06-Metric-Laerning-for-Keyword-Spotting">Metric Learning for Keyword Spotting</a>
                    </li>
                
                    <li>
                        <a href="/2019-07-30-Attention-based-models-for-TDSV">Attention-based Models For Text-dependent Speaker Verification</a>
                    </li>
                
                    <li>
                        <a href="/2019-07-24-TISV-with-Adversarial-Learning-on-Short-Utterances">Text-Independent Speaker Verification with Adversarial Learning on Short Utterances</a>
                    </li>
                
                    <li>
                        <a href="/2019-07-10-GE2E-loss-for-SV">Generalized End to End Loss For Speaker Verification</a>
                    </li>
                
                    <li>
                        <a href="/2019-06-03-Generative-Adversarial-Speaker-Embedding-Networks-for-Domain-Roubust-E2E-SV">Generative Adversarial Speaker Embedding Networks for Domain Robust End-to-End Speaker Verification</a>
                    </li>
                
                    <li>
                        <a href="/2019-05-22-E2E-DNN-based-Speaker-Recognition-Inspired-by-i-vector-and-PLDA">End-to-End DNN based Speaker Recognition Inspired by i-vector and PLDA</a>
                    </li>
                
            </div>

        </div>
    </div>
</footer>

<!-- #end -->
<script src="//cdn.bootcss.com/jquery/1.10.1/jquery.min.js"></script>
<script>
	!window.jQuery && document.write(unescape('%3Cscript src="/style/js/jquery.min.js"%3E%3C/script%3E'))
</script>
<script src="/style/js/headroom.min.js"></script>
<script src="/style/js/nav.min.js"></script>
<script type="text/javascript">
    var header = new Headroom(document.getElementById("header"), {
        tolerance: 10,
        offset : 80,
        classes: {
            initial: "animated",
            pinned: "slideDown",
            unpinned: "slideUp"
        }
    });
    header.init();
</script>

<script>window.SmoothScrollOptions = { stepSize: 36 }</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/smoothscroll/1.4.8/SmoothScroll.min.js"></script>
<script>
	!window.SmoothScroll && document.write(unescape('%3Cscript src="/style/js/SmoothScroll.min.js"%3E%3C/script%3E'))
</script>




  </body>
</html>

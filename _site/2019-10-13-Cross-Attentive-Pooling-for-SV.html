<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
    <meta name="renderer" content="webkit">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <meta http-equiv="Cache-Control" content="no-transform"/>
    <meta http-equiv="Cache-Control" content="no-siteapp"/>
    <title>Cross attentive pooling for speaker verification</title>
	<meta name="description" content="">
	<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="apple-touch-icon" href="/style/image/favicon.png"/>
    <link href="/style/css/highlight.min.css" rel="stylesheet">
    <link href="/style/css/style.min.css" rel="stylesheet">
	<link rel="stylesheet" href="/style/css/iconfont/iconfont.css">
</head>

<body class="bg-grey" gtools_scp_screen_capture_injected="true">

    <header id="header" class="header bg-white">
    <div class="navbar-container">
        <a href="/" class="navbar-logo">
            <img src="/style/image/logo.png" alt="ARa's DevBlog" />
            <span>ARa's DevBlog</span>
        </a>
        <div class="navbar-menu">
            <a href="/archives">Archives</a>
            <a href="/about">About</a>
        </div>
        <div class="navbar-mobile-menu" onclick="">
            <span class="icon-menu cross"><span class="middle"></span></span>
            <ul>
                <li><a href="/archives">Archives</a></li>
                <li><a href="/about">About</a></li>
            </ul>
        </div>
    </div>
</header>

    <article class="main-content page-page" itemscope itemtype="http://schema.org/Article">
    <div class="post-header">
        <h1 class="post-title" itemprop="name headline">
            Cross attentive pooling for speaker verification
        </h1>
        <div class="post-data">
            <time itemprop="datePublished">Oct 13, 2019</time>
        </div>
    </div>
</article>
<div class="main-container">
    <div class="post-container">
        <div class="navigation" id="navigation">
            <h1>Contents</h1>
            <div class="nav sidenav">
	    </div>
        </div>
        <article class="post-content">
            <ul id="markdown-toc">
  <li><a href="#-abstract" id="markdown-toc--abstract">📌 <strong>Abstract</strong></a></li>
  <li><a href="#ⅰ-introduction" id="markdown-toc-ⅰ-introduction"><strong>Ⅰ. Introduction</strong></a></li>
  <li><a href="#ⅱ-methods" id="markdown-toc-ⅱ-methods"><strong>Ⅱ. Methods</strong></a>    <ul>
      <li><a href="#21-few-shot-learning-framwork" id="markdown-toc-21-few-shot-learning-framwork"><strong>2.1 Few-shot learning framwork</strong></a></li>
      <li><a href="#22-instance-wise-aggregation" id="markdown-toc-22-instance-wise-aggregation"><strong>2.2 Instance-wise aggregation</strong></a></li>
      <li><a href="#23-pair-wise-aggregation" id="markdown-toc-23-pair-wise-aggregation"><strong>2.3 Pair-wise aggregation</strong></a></li>
    </ul>
  </li>
  <li><a href="#ⅲ-experiments" id="markdown-toc-ⅲ-experiments"><strong>Ⅲ. Experiments</strong></a></li>
</ul>

<p><span style="font-size:13pt">Seong Min Kye, Yoohwan Kwon, Joon Son Chung</span></p>

<h1 id="-abstract">📌 <strong>Abstract</strong></h1>

<ul>
  <li><strong>목표 : ‘in the wild’ video와 관련없는 signal을 포함하는 utterance를 사용하는 TI-SV(Text-Independent Speaker Verification)</strong></li>
  <li>SV는 pair-wise 문제(등록과 테스트 쌍을 비교), 기존의 embedding 추출은 instance-wise 문제(각 utterance에 대한 embedding을 추출하여 서로 비교)</li>
  <li><span style="background-color:#ffed54">본 논문에서는 reference-query pair 전체의 context 정보를 활용하여 <strong>pair-wise 문제에 가장 discriminative한 utterance-level의 embedding 추출을 생성</strong>하는 <strong>CAP(Cross Attention Pooling)</strong>을 제안</span></li>
  <li>VoxCeleb dataset을 사용하고, 다른 pooling 방법과 비교하여 우수한 성능을 보였음</li>
</ul>

<p><br /></p>

<hr />

<p><br /></p>

<h1 id="ⅰ-introduction"><strong>Ⅰ. Introduction</strong></h1>

<ul>
  <li>Automatic Speaker Recognition; 음성은 가장 쉽게 접근할 수 있는 생체 정보 중 하나이기 때문에 누군가의 신원을 확인하는데 매력적인 방법</li>
  <li>speaker recognition은 identification과 verification을 모두 포함하지만, 후자의 경우 더 실용적인 응용 분야를 가짐(ex. 콜센터, AI 스피커 등)</li>
  <li>closed-set identification과 달리 open-set verification은 훈련에서 보지 못했던 화자의 identity를 확인하는 것을 목표로 하기 때문에, speaker verification은 음성이 discriminative한 embedding 차원의 표현으로 mapping되어야하는 metric learning 문제</li>
  <li>다른 논문들에서 주로 classification loss를 사용하여 embedding을 학습하였으나 embedding similarity를 최적화하도록 설계되지 않음</li>
  <li>최근 연구들에서 class 간 분리를 강화하기 위해 verification의 성능을 향상시키는 것으로 알려진 margin variant를 추가한 softmax를 접목시킴 (AM-softmax)</li>
</ul>

<p><br /></p>

<ul>
  <li><strong>open-set verification</strong>은 network가 제한된 example을 갖으면서 unseen class에 대해 인식해야하므로 <strong>few-shot learning</strong> 문제라고 볼 수 있음</li>
  <li>few-shot learning 시나리오를 모방하는 <strong>prototypical network</strong>가 제안되었으며, <strong>최근 speaker verification에서 좋은 성능을 달성</strong>하는 것으로 나타남</li>
</ul>

<p><br /></p>

<ul>
  <li>similarity metric을 최적화하도록 network를 훈련시키기 위해서는 frame-level의 representation(feature)를 utterance-level로 모아야 함</li>
  <li>가장 단순한 방법은 frame-level을 평균하는 것(TAP, Temporal Average Pooling), 이때 frame들은 모두 같은 weight를 갖게 됨</li>
  <li>verification에 더 discriminative한 frame에 attention하도록 SAP(Self-Attentive Pooling)방법이 제안</li>
  <li>그러나 instance-level self-attention은 support set(training set)의 특정 sample이 아닌, 일반적으로(training set의 전체 data를 아우름) discriminative한 feature를 찾음; training dataset의 전체적인 특성이 반영되어 특정 sample에 대해서는 효과적이지 않을 수 있음</li>
</ul>

<p><br /></p>

<ul>
  <li>CAN(Cross Attention Network): few-shot learning에서 최근 support set의 example들과 관련있고, discriminative한 input image의 부분에 attention함으로써 unseen target class를 기반의 attention을 선택할 수 있도록 제안된 방법</li>
  <li>support set의 한 class(speaker)와 utterance를 비교하기 위한 discriminative한 특성이 다른 class와 비교하기 위해 생성되는 특징과 다를 것, 따라서 이 아이디어를 speaker verification에 적용할 수 있음</li>
  <li>본 논문에서는 frame-level의 정보를 효과적으로 utterance-level의 embedding으로 모으기 위해 support set의 example을 참조하여 attention을 계산하는 CAP(Cross Attentive Pooling)를 제안</li>
  <li><strong>이러한 방식으로 network는 support set의 특정 class에 대한 특정 특징을 제공하는 utterance을 식별하고 집중시킬 수 있음</strong></li>
  <li>이는 사람이 unseen class의 instance를 인식할 때, sample 쌍들의 공통적인 특성을 갖는 특징을 찾는 것과 유사함</li>
  <li>instance-level의 pooling과 달리, 제안된 attention module은 class(prototype) feature와 query feature의 관련성을 모델링하여 verification task에서 pair-wise 특성을 최대한 활용</li>
</ul>

<p><br /></p>

<hr />

<p><br /></p>

<h1 id="ⅱ-methods"><strong>Ⅱ. Methods</strong></h1>

<h3 id="21-few-shot-learning-framwork"><strong>2.1 Few-shot learning framwork</strong></h3>

<ul>
  <li>Speaker recognition을 위한 embedding을 훈련하기 위해 few-shot learning framework인 prototypical network 사용</li>
</ul>

<p><br /></p>

<p><strong>Batch formation</strong></p>

<ul>
  <li>각 mini-batch에는 support(training) set $S$와 query(test) set $Q$가 포함</li>
  <li>서로 다른 화자 N명마다 M개의 발화 포함</li>
</ul>

<center>

$S = {(x_i, y_i)}^{N \times 1}_{i=1}$  

$Q = {(\tilde{x_i}, \tilde{y_i})}^{N \times (M-1)}_{i=1}$  

</center>

<blockquote>
  <p>support set은 각 화자마다 1개의 발화를 사용하고, query set은 나머지 발화($2 \leq i \leq M$)를 사용<br />
$y, \tilde{y} \in {1, …, N}$; class label</p>
</blockquote>

<p><br /></p>

<p><strong>Training object</strong></p>

<ul>
  <li>support set은 단일 발화 $x$로 구성되어, prototype(centroid)는 각 화자 %y%의 support utterance와 같음</li>
</ul>
<center><img src="https://user-images.githubusercontent.com/46676700/95678027-a5714b00-0c04-11eb-816d-01da565f1eaa.png" alt="img" style="zoom: 80%;" /></center>

<p><br /></p>

<ul>
  <li>log-softmax function을 사용하는 cross-entropy loss는 같은 speaker의 segment 간 거리는 최소화하면서 다른 speaker 간의 거리는 최대화</li>
</ul>
<center><img src="https://user-images.githubusercontent.com/46676700/95678054-cb96eb00-0c04-11eb-9eb8-6e1a2c6ccb3a.png" alt="img" style="zoom: 80%;" /></center>

<p><br /></p>

<ul>
  <li>query embedding의 크기와 prototype과 query의 cosine similarity를 distance metric으로 사용 (<strong>Normalized prototypical, NP</strong>)</li>
</ul>
<center><img src="https://user-images.githubusercontent.com/46676700/95678059-d2bdf900-0c04-11eb-808e-efe28e67875f.png" alt="img" style="zoom: 80%;" /></center>

<ul>
  <li>kye et al.[16]은 speaker embedding을 보다 discriminative하게 만들기 위해 global classification loss와 함께 <span style="background-color:#d2d8d8">episodic training*</span>을 사용
(few-shot task와 유사한 형태의 훈련 task를 통해 모델 스스로 학습 규칙을 도출할 수 있게 함으로써 일반화 성능을 높일 수 있음 <a href="https://www.kakaobrain.com/blog/106">참조-kakaobrainBlog</a>)</li>
  <li>global classification은 support와 query set 모두에 적용</li>
  <li>softmax classification loss를 통합하여 mini-batch에 있는 class뿐만 아니라 모든 class에 대해 discriminative하도록 embedding을 훈련 가능</li>
  <li><strong>최종적인 objective function</strong>은 동일한 가중치를 적용한 <strong>NP와 softmax cross-entropy loss의 합</strong>(단순 sum)</li>
</ul>

<p><br /></p>

<h3 id="22-instance-wise-aggregation"><strong>2.2 Instance-wise aggregation</strong></h3>

<ul>
  <li>이상적인 utterance-level embedding은 frequency가 아닌 temporal 위치에 따라 달라져야함</li>
  <li>2D convolutional neural network는 2D activation map을 생성하기 때문에 frequency 축만 모두 연결되는 aggregation layer를 [1]에서 제안</li>
  <li>따라서 pooling layer에 들어가기 전 1xT feature map 생성</li>
</ul>

<p><br /></p>

<p><strong>Temporal Average Pooling(TAP)</strong></p>

<ul>
  <li>단순하게 temporal domain에 대해 feature의 평균을 취함</li>
</ul>
<center><img src="https://user-images.githubusercontent.com/46676700/95678556-70ff8e00-0c08-11eb-8d56-7d26175f42c7.png" alt="img" style="zoom: 80%;" /></center>

<p><br /></p>

<p><strong>Self-Attentive Pooling(SAP)</strong></p>

<ul>
  <li>각 시간에 대한 frame 모두 같은 weight를 갖는 TAP와 달리, utterance-level에 더 많은 정보를 제공하는 frame-level에 attention함</li>
</ul>
<center><img src="https://user-images.githubusercontent.com/46676700/95678562-7ceb5000-0c08-11eb-90d6-3498d822c878.png" alt="img" style="zoom: 80%;" /></center>

<blockquote>
  <p>frame-level 특징 $x_t$가 우선 parameter W와 b를 갖는 MLP의 입력으로 넣어 non-linear하게 projection(hidden representation으로 mapping)</p>
</blockquote>

<p><br /></p>

<ul>
  <li>hidden vector $h_t$와 훈련되는 context vector $\mu$ 사이의 유사도를 계산하여 score(hidden feature의 상대적인 중요도)로 사용</li>
  <li>softmax function을 통해 나온 결과를 각 frame의 중요도(attention weight)로 사용</li>
  <li>context vector는 speaker recognition에 중요한 정보를 제공하는 high-level representation으로 볼 수 있음</li>
</ul>
<center><img src="https://user-images.githubusercontent.com/46676700/95678567-807ed700-0c08-11eb-8766-296995b8de48.png" alt="img" style="zoom: 80%;" /></center>

<p><br /></p>

<ul>
  <li>utterance-level embedding $e$는 frame-level 특징과 frame-level의 attention weight와 가중합하여 얻을 수 있음</li>
</ul>
<center><img src="https://user-images.githubusercontent.com/46676700/95678569-84aaf480-0c08-11eb-8291-24f23db5b892.png" alt="img" style="zoom: 80%;" /></center>

<p><br /></p>

<h3 id="23-pair-wise-aggregation"><strong>2.3 Pair-wise aggregation</strong></h3>

<ul>
  <li>기존의 instance-wise aggregation과 달리 본 논문에서는 <span style="background-color:#ffed54"><strong>다른 utterance의 frame feature를 사용하여 frame-level feature를 모으는 방법</strong></span>을 제안</li>
  <li>
    <p>training과 testing의 목표를 맞추기 위헤 metric기반의 meta-learning framework인 prototypical network 사용</p>
  </li>
  <li>이 framework에서 support와 query set pair를 사용하여 CAP를 훈련</li>
  <li>test 시, support set과 query set은 enrollment와 test utterance에 해당</li>
</ul>

<p><br /></p>

<ul>
  <li>query와 support set의 모든 utterance pair에 대해 frame-level representation $s={s_1, s_2,\dots, s_{T_s}}, q={q_1, q_2,\dots, q_{T_q}}$ 추출</li>
  <li>meta-projection layer $g_{\Phi}(·)$를 사용하여 frame-level에서 hidden feature 추출</li>
  <li>non-linear projection을 통해 임의의 frame에 빠르게 적응할 수 있으므로 frame pair의 유사도를 잘 측정할 수 있음</li>
  <li>이 layer는 MLP와 ReLU activation function으로 구성</li>
</ul>
<center><img src="https://user-images.githubusercontent.com/46676700/95679512-50d2cd80-0c0e-11eb-846c-fa3f1bfe0bde.png" alt="img" style="zoom: 80%;" /></center>

<ul>
  <li>meta-projection layer 이후, 모든 frame에 대한 hidden representation인 $S, Q$를 얻을 수 있음</li>
</ul>

<blockquote>
  <p>$ S = {S_i}^{T_s}<em>{i=1}$<br />
$ Q = {Q_i}^{T_q}</em>{i=1}$<br />
$S_i, Q_i$ 는 각각 $g_{\Phi}(s_i), g_{\Phi}(q_i)$</p>
</blockquote>

<p><br /></p>

<p><strong>Correlation matrix</strong></p>

<ul>
  <li>Correlation matrix(상관행렬) R은 가능한 모든 frame pair에 대한 similarity를 요약</li>
</ul>

<center><img src="https://user-images.githubusercontent.com/46676700/95679513-55978180-0c0e-11eb-8991-dc7ca123ddc4.png" alt="img" style="zoom: 80%;" /></center>

<blockquote>
  <p>$R^Q = (R^S)^T$; 순서만 바뀌기 때문에 $R^S$의 transpose가 $R^Q$<br />
$R^S_{1, 1}$; support set의 1번째 frame hidden representation과 query set의 1번째 frame hidden representation의 similarity<br />
따라서 $R^S \in \mathbb{R}^{T_s \times T_q}$; [support set frame 수 x query set frame 수]</p>
</blockquote>

<p><br /></p>

<p><strong>Pair-adaptive attention</strong></p>

<ul>
  <li>pair-adaptive context vector를 얻기 위해 다음과 같이 time축에 대해 correlation matrix를 평균</li>
</ul>
<center><img src="https://user-images.githubusercontent.com/46676700/95679520-5a5c3580-0c0e-11eb-9c92-c802e2e3bcd0.png" alt="img" style="zoom: 80%;" /></center>

<blockquote>
  <p><strong>$\mu_s \in \mathbb{R}^{T_q}$</strong> 이고, $\mathbb{R}^S_{i,*}$은 $i$번째 row vector</p>
</blockquote>

<ul>
  <li>논문에서 $T_s$로 되어있는데, $T_s$가 아닌 $T_q$이 되어야 수식적으로 맞는 것 같음 (그림에서는 context vector의 size를 $T_q$로 표기)</li>
  <li>각 row vector는 다른 utterance의 모든 frame과의 유사도 정보가 있음</li>
  <li>따라서 다른 utterance의 각 frame에 대한 평균 상관관계를 $\mu$로 표시할 수 있고, 이는 다른 utterance와 얼마나 유사한지 계산하기 위해 context vector로 사용</li>
</ul>

<p><br /></p>

<ul>
  <li>attention weight는 모든 utterance에 대해 다음과 같이 계산</li>
</ul>
<center><img src="https://user-images.githubusercontent.com/46676700/95679528-60521680-0c0e-11eb-9a06-8745d6fa010b.png" alt="img" style="zoom: 80%;" /></center>

<blockquote>
  <p>$\tau$ : temperature scaling (attention distribution의 선명도 조절) - $\tau \rightarrow \infty$이면 동일한 attention weight를 갖음</p>
</blockquote>

<center><img src="https://user-images.githubusercontent.com/46676700/95679531-647e3400-0c0e-11eb-9e6c-b9b0e4c58a0e.png" alt="img" style="zoom: 80%;" /></center>

<ul>
  <li>Hou et al [22], utterance-level의 특징을 얻기 위해 residual attention mechanism을 사용</li>
  <li>다른 utterance에 대해서도 동일한 방법으로 utterance-level feature $q$로 $e_q$를 얻을 수 있음</li>
</ul>

<p><br /></p>

<p><strong>제안하는 방법의 procedure</strong></p>

<center><img src="https://user-images.githubusercontent.com/46676700/95680543-7a432780-0c15-11eb-80a4-709be1187867.png" alt="img" style="zoom: 80%;" /></center>
<center><img src="https://user-images.githubusercontent.com/46676700/95680550-829b6280-0c15-11eb-93fc-0ac5babd0115.png" alt="img" style="zoom: 80%;" /></center>

<p><br /></p>

<hr />

<p><br /></p>

<h1 id="ⅲ-experiments"><strong>Ⅲ. Experiments</strong></h1>

<p><strong>Model architecture</strong></p>

<p><img src="https://user-images.githubusercontent.com/46676700/95680589-cb531b80-0c15-11eb-9d17-c3ead5a27fd8.png" alt="img" /></p>

<p><br /></p>

<p><strong>Results</strong></p>

<p><img src="https://user-images.githubusercontent.com/46676700/95680595-d0b06600-0c15-11eb-8d5a-b8b7166ea620.png" alt="img" /></p>

        </article>
        <div class="post-content">
         <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<div id="gitalk-container"></div>

<script data-no-instant type="text/javascript">
const gitalk = new Gitalk({
  clientID: '5304ae17c5ba0ce3b2aa',
  clientSecret: '22801b52c86101ff074072faa1631475954d2936',
  repo: 'arabae.github.io',
  owner: 'arabae',
  admin: ['arabae'],
  id: location.pathname,      // Ensure uniqueness and length less than 50
  distractionFreeMode: true  // Facebook-like distraction free mode
})

gitalk.render('gitalk-container')
</script>
        </div>
    </div>
</div>

    
    <footer id="footer" class="footer bg-white">
    <div class="footer-social">
        <div class="footer-container clearfix">
            <div class="social-list">
                <a href="/"><span class='iconfont icon-home'></span>&nbsp;&nbsp;HOME</a>
                <a rel="nofollow" target="_blank" href="https://github.com/arabae"><span class='iconfont icon-github'></span>&nbsp;&nbsp;Github</a>
                <a target="_blank" href="/feed.xml"><span class='iconfont icon-rss'></span>&nbsp;&nbsp;RSS</a>
            </div>
        </div>
    </div>
    <div class="footer-meta">
        <div class="footer-container">
            <div class="meta-item meta-copyright">
                <div class="meta-copyright-info">
                    <a href="https://github.com/arabae" class="info-logo">
                        <img src="/style/image/logo.png" alt="wonder">
                    </a>
                    <div class="info-text">
                        <p>Copyright &copy; 2021 - 2021 <a href="https://github.com/arabae"><code>ARa Bae</code></a></p>
                        <p>Powered by <a href="http://jekyllrb.com" target="_blank" rel="nofollow"><code>jekyll</code></a>，theme is <a href="https://github.com/lightfish-zhang/pinghsu-jekyll" target="_blank" rel="nofollow"><code>pinghsu</code></a></p>
                    </div>
                </div>
            </div>

            <div class="meta-item meta-posts">
                <h3 class="meta-title">RECENT POSTS</h3>
                
                    <li>
                        <a href="/2019-10-13-Cross-Attentive-Pooling-for-SV">Cross attentive pooling for speaker verification</a>
                    </li>
                
                    <li>
                        <a href="/2019-10-06-Metric-Laerning-for-Keyword-Spotting">Metric Learning for Keyword Spotting</a>
                    </li>
                
                    <li>
                        <a href="/2019-07-30-Attention-based-models-for-TDSV">Attention-based Models For Text-dependent Speaker Verification</a>
                    </li>
                
                    <li>
                        <a href="/2019-07-24-TISV-with-Adversarial-Learning-on-Short-Utterances">Text-Independent Speaker Verification with Adversarial Learning on Short Utterances</a>
                    </li>
                
                    <li>
                        <a href="/2019-07-10-GE2E-loss-for-SV">Generalized End to End Loss For Speaker Verification</a>
                    </li>
                
                    <li>
                        <a href="/2019-06-03-Generative-Adversarial-Speaker-Embedding-Networks-for-Domain-Roubust-E2E-SV">Generative Adversarial Speaker Embedding Networks for Domain Robust End-to-End Speaker Verification</a>
                    </li>
                
                    <li>
                        <a href="/2019-05-22-E2E-DNN-based-Speaker-Recognition-Inspired-by-i-vector-and-PLDA">End-to-End DNN based Speaker Recognition Inspired by i-vector and PLDA</a>
                    </li>
                
            </div>

        </div>
    </div>
</footer>

<!-- #end -->
<script src="//cdn.bootcss.com/jquery/1.10.1/jquery.min.js"></script>
<script>
	!window.jQuery && document.write(unescape('%3Cscript src="/style/js/jquery.min.js"%3E%3C/script%3E'))
</script>
<script src="/style/js/headroom.min.js"></script>
<script src="/style/js/nav.min.js"></script>
<script type="text/javascript">
    var header = new Headroom(document.getElementById("header"), {
        tolerance: 10,
        offset : 80,
        classes: {
            initial: "animated",
            pinned: "slideDown",
            unpinned: "slideUp"
        }
    });
    header.init();
</script>

<script>window.SmoothScrollOptions = { stepSize: 36 }</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/smoothscroll/1.4.8/SmoothScroll.min.js"></script>
<script>
	!window.SmoothScroll && document.write(unescape('%3Cscript src="/style/js/SmoothScroll.min.js"%3E%3C/script%3E'))
</script>




  </body>
</html>

<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
    <meta name="renderer" content="webkit">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <meta http-equiv="Cache-Control" content="no-transform"/>
    <meta http-equiv="Cache-Control" content="no-siteapp"/>
    <title>Cross attentive pooling for speaker verification</title>
	<meta name="description" content="">
	<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="apple-touch-icon" href="/style/image/favicon.png"/>
    <link href="/style/css/highlight.min.css" rel="stylesheet">
    <link href="/style/css/style.min.css" rel="stylesheet">
	<link rel="stylesheet" href="/style/css/iconfont/iconfont.css">
</head>

<body class="bg-grey" gtools_scp_screen_capture_injected="true">

    <header id="header" class="header bg-white">
    <div class="navbar-container">
        <a href="/" class="navbar-logo">
            <img src="/style/image/logo.png" alt="ARa's DevBlog" />
            <span>ARa's DevBlog</span>
        </a>
        <div class="navbar-menu">
            <a href="/archives">Archives</a>
            <a href="/about">About</a>
        </div>
        <div class="navbar-mobile-menu" onclick="">
            <span class="icon-menu cross"><span class="middle"></span></span>
            <ul>
                <li><a href="/archives">Archives</a></li>
                <li><a href="/about">About</a></li>
            </ul>
        </div>
    </div>
</header>

    <article class="main-content page-page" itemscope itemtype="http://schema.org/Article">
    <div class="post-header">
        <h1 class="post-title" itemprop="name headline">
            Cross attentive pooling for speaker verification
        </h1>
        <div class="post-data">
            <time itemprop="datePublished">Oct 13, 2019</time>
        </div>
    </div>
</article>
<div class="main-container">
    <div class="post-container">
        <div class="navigation" id="navigation">
            <h1>Contents</h1>
            <div class="nav sidenav">
	    </div>
        </div>
        <article class="post-content">
            <ul id="markdown-toc">
  <li><a href="#-abstract" id="markdown-toc--abstract">ğŸ“Œ <strong>Abstract</strong></a></li>
  <li><a href="#â…°-introduction" id="markdown-toc-â…°-introduction"><strong>â… . Introduction</strong></a></li>
  <li><a href="#â…±-methods" id="markdown-toc-â…±-methods"><strong>â…¡. Methods</strong></a>    <ul>
      <li><a href="#21-few-shot-learning-framwork" id="markdown-toc-21-few-shot-learning-framwork"><strong>2.1 Few-shot learning framwork</strong></a></li>
      <li><a href="#22-instance-wise-aggregation" id="markdown-toc-22-instance-wise-aggregation"><strong>2.2 Instance-wise aggregation</strong></a></li>
      <li><a href="#23-pair-wise-aggregation" id="markdown-toc-23-pair-wise-aggregation"><strong>2.3 Pair-wise aggregation</strong></a></li>
    </ul>
  </li>
  <li><a href="#â…²-experiments" id="markdown-toc-â…²-experiments"><strong>â…¢. Experiments</strong></a></li>
</ul>

<p><span style="font-size:13pt">Seong Min Kye, Yoohwan Kwon, Joon Son Chung</span></p>

<h1 id="-abstract">ğŸ“Œ <strong>Abstract</strong></h1>

<ul>
  <li><strong>ëª©í‘œ : â€˜in the wildâ€™ videoì™€ ê´€ë ¨ì—†ëŠ” signalì„ í¬í•¨í•˜ëŠ” utteranceë¥¼ ì‚¬ìš©í•˜ëŠ” TI-SV(Text-Independent Speaker Verification)</strong></li>
  <li>SVëŠ” pair-wise ë¬¸ì œ(ë“±ë¡ê³¼ í…ŒìŠ¤íŠ¸ ìŒì„ ë¹„êµ), ê¸°ì¡´ì˜ embedding ì¶”ì¶œì€ instance-wise ë¬¸ì œ(ê° utteranceì— ëŒ€í•œ embeddingì„ ì¶”ì¶œí•˜ì—¬ ì„œë¡œ ë¹„êµ)</li>
  <li><span style="background-color:#ffed54">ë³¸ ë…¼ë¬¸ì—ì„œëŠ” reference-query pair ì „ì²´ì˜ context ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ <strong>pair-wise ë¬¸ì œì— ê°€ì¥ discriminativeí•œ utterance-levelì˜ embedding ì¶”ì¶œì„ ìƒì„±</strong>í•˜ëŠ” <strong>CAP(Cross Attention Pooling)</strong>ì„ ì œì•ˆ</span></li>
  <li>VoxCeleb datasetì„ ì‚¬ìš©í•˜ê³ , ë‹¤ë¥¸ pooling ë°©ë²•ê³¼ ë¹„êµí•˜ì—¬ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŒ</li>
</ul>

<p><br /></p>

<hr />

<p><br /></p>

<h1 id="â…°-introduction"><strong>â… . Introduction</strong></h1>

<ul>
  <li>Automatic Speaker Recognition; ìŒì„±ì€ ê°€ì¥ ì‰½ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆëŠ” ìƒì²´ ì •ë³´ ì¤‘ í•˜ë‚˜ì´ê¸° ë•Œë¬¸ì— ëˆ„êµ°ê°€ì˜ ì‹ ì›ì„ í™•ì¸í•˜ëŠ”ë° ë§¤ë ¥ì ì¸ ë°©ë²•</li>
  <li>speaker recognitionì€ identificationê³¼ verificationì„ ëª¨ë‘ í¬í•¨í•˜ì§€ë§Œ, í›„ìì˜ ê²½ìš° ë” ì‹¤ìš©ì ì¸ ì‘ìš© ë¶„ì•¼ë¥¼ ê°€ì§(ex. ì½œì„¼í„°, AI ìŠ¤í”¼ì»¤ ë“±)</li>
  <li>closed-set identificationê³¼ ë‹¬ë¦¬ open-set verificationì€ í›ˆë ¨ì—ì„œ ë³´ì§€ ëª»í–ˆë˜ í™”ìì˜ identityë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•˜ê¸° ë•Œë¬¸ì—, speaker verificationì€ ìŒì„±ì´ discriminativeí•œ embedding ì°¨ì›ì˜ í‘œí˜„ìœ¼ë¡œ mappingë˜ì–´ì•¼í•˜ëŠ” metric learning ë¬¸ì œ</li>
  <li>ë‹¤ë¥¸ ë…¼ë¬¸ë“¤ì—ì„œ ì£¼ë¡œ classification lossë¥¼ ì‚¬ìš©í•˜ì—¬ embeddingì„ í•™ìŠµí•˜ì˜€ìœ¼ë‚˜ embedding similarityë¥¼ ìµœì í™”í•˜ë„ë¡ ì„¤ê³„ë˜ì§€ ì•ŠìŒ</li>
  <li>ìµœê·¼ ì—°êµ¬ë“¤ì—ì„œ class ê°„ ë¶„ë¦¬ë¥¼ ê°•í™”í•˜ê¸° ìœ„í•´ verificationì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì§„ margin variantë¥¼ ì¶”ê°€í•œ softmaxë¥¼ ì ‘ëª©ì‹œí‚´ (AM-softmax)</li>
</ul>

<p><br /></p>

<ul>
  <li><strong>open-set verification</strong>ì€ networkê°€ ì œí•œëœ exampleì„ ê°–ìœ¼ë©´ì„œ unseen classì— ëŒ€í•´ ì¸ì‹í•´ì•¼í•˜ë¯€ë¡œ <strong>few-shot learning</strong> ë¬¸ì œë¼ê³  ë³¼ ìˆ˜ ìˆìŒ</li>
  <li>few-shot learning ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ëª¨ë°©í•˜ëŠ” <strong>prototypical network</strong>ê°€ ì œì•ˆë˜ì—ˆìœ¼ë©°, <strong>ìµœê·¼ speaker verificationì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë‹¬ì„±</strong>í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¨</li>
</ul>

<p><br /></p>

<ul>
  <li>similarity metricì„ ìµœì í™”í•˜ë„ë¡ networkë¥¼ í›ˆë ¨ì‹œí‚¤ê¸° ìœ„í•´ì„œëŠ” frame-levelì˜ representation(feature)ë¥¼ utterance-levelë¡œ ëª¨ì•„ì•¼ í•¨</li>
  <li>ê°€ì¥ ë‹¨ìˆœí•œ ë°©ë²•ì€ frame-levelì„ í‰ê· í•˜ëŠ” ê²ƒ(TAP, Temporal Average Pooling), ì´ë•Œ frameë“¤ì€ ëª¨ë‘ ê°™ì€ weightë¥¼ ê°–ê²Œ ë¨</li>
  <li>verificationì— ë” discriminativeí•œ frameì— attentioní•˜ë„ë¡ SAP(Self-Attentive Pooling)ë°©ë²•ì´ ì œì•ˆ</li>
  <li>ê·¸ëŸ¬ë‚˜ instance-level self-attentionì€ support set(training set)ì˜ íŠ¹ì • sampleì´ ì•„ë‹Œ, ì¼ë°˜ì ìœ¼ë¡œ(training setì˜ ì „ì²´ dataë¥¼ ì•„ìš°ë¦„) discriminativeí•œ featureë¥¼ ì°¾ìŒ; training datasetì˜ ì „ì²´ì ì¸ íŠ¹ì„±ì´ ë°˜ì˜ë˜ì–´ íŠ¹ì • sampleì— ëŒ€í•´ì„œëŠ” íš¨ê³¼ì ì´ì§€ ì•Šì„ ìˆ˜ ìˆìŒ</li>
</ul>

<p><br /></p>

<ul>
  <li>CAN(Cross Attention Network): few-shot learningì—ì„œ ìµœê·¼ support setì˜ exampleë“¤ê³¼ ê´€ë ¨ìˆê³ , discriminativeí•œ input imageì˜ ë¶€ë¶„ì— attentioní•¨ìœ¼ë¡œì¨ unseen target classë¥¼ ê¸°ë°˜ì˜ attentionì„ ì„ íƒí•  ìˆ˜ ìˆë„ë¡ ì œì•ˆëœ ë°©ë²•</li>
  <li>support setì˜ í•œ class(speaker)ì™€ utteranceë¥¼ ë¹„êµí•˜ê¸° ìœ„í•œ discriminativeí•œ íŠ¹ì„±ì´ ë‹¤ë¥¸ classì™€ ë¹„êµí•˜ê¸° ìœ„í•´ ìƒì„±ë˜ëŠ” íŠ¹ì§•ê³¼ ë‹¤ë¥¼ ê²ƒ, ë”°ë¼ì„œ ì´ ì•„ì´ë””ì–´ë¥¼ speaker verificationì— ì ìš©í•  ìˆ˜ ìˆìŒ</li>
  <li>ë³¸ ë…¼ë¬¸ì—ì„œëŠ” frame-levelì˜ ì •ë³´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ utterance-levelì˜ embeddingìœ¼ë¡œ ëª¨ìœ¼ê¸° ìœ„í•´ support setì˜ exampleì„ ì°¸ì¡°í•˜ì—¬ attentionì„ ê³„ì‚°í•˜ëŠ” CAP(Cross Attentive Pooling)ë¥¼ ì œì•ˆ</li>
  <li><strong>ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ networkëŠ” support setì˜ íŠ¹ì • classì— ëŒ€í•œ íŠ¹ì • íŠ¹ì§•ì„ ì œê³µí•˜ëŠ” utteranceì„ ì‹ë³„í•˜ê³  ì§‘ì¤‘ì‹œí‚¬ ìˆ˜ ìˆìŒ</strong></li>
  <li>ì´ëŠ” ì‚¬ëŒì´ unseen classì˜ instanceë¥¼ ì¸ì‹í•  ë•Œ, sample ìŒë“¤ì˜ ê³µí†µì ì¸ íŠ¹ì„±ì„ ê°–ëŠ” íŠ¹ì§•ì„ ì°¾ëŠ” ê²ƒê³¼ ìœ ì‚¬í•¨</li>
  <li>instance-levelì˜ poolingê³¼ ë‹¬ë¦¬, ì œì•ˆëœ attention moduleì€ class(prototype) featureì™€ query featureì˜ ê´€ë ¨ì„±ì„ ëª¨ë¸ë§í•˜ì—¬ verification taskì—ì„œ pair-wise íŠ¹ì„±ì„ ìµœëŒ€í•œ í™œìš©</li>
</ul>

<p><br /></p>

<hr />

<p><br /></p>

<h1 id="â…±-methods"><strong>â…¡. Methods</strong></h1>

<h3 id="21-few-shot-learning-framwork"><strong>2.1 Few-shot learning framwork</strong></h3>

<ul>
  <li>Speaker recognitionì„ ìœ„í•œ embeddingì„ í›ˆë ¨í•˜ê¸° ìœ„í•´ few-shot learning frameworkì¸ prototypical network ì‚¬ìš©</li>
</ul>

<p><br /></p>

<p><strong>Batch formation</strong></p>

<ul>
  <li>ê° mini-batchì—ëŠ” support(training) set $S$ì™€ query(test) set $Q$ê°€ í¬í•¨</li>
  <li>ì„œë¡œ ë‹¤ë¥¸ í™”ì Nëª…ë§ˆë‹¤ Mê°œì˜ ë°œí™” í¬í•¨</li>
</ul>

<center>

$S = {(x_i, y_i)}^{N \times 1}_{i=1}$  

$Q = {(\tilde{x_i}, \tilde{y_i})}^{N \times (M-1)}_{i=1}$  

</center>

<blockquote>
  <p>support setì€ ê° í™”ìë§ˆë‹¤ 1ê°œì˜ ë°œí™”ë¥¼ ì‚¬ìš©í•˜ê³ , query setì€ ë‚˜ë¨¸ì§€ ë°œí™”($2 \leq i \leq M$)ë¥¼ ì‚¬ìš©<br />
$y, \tilde{y} \in {1, â€¦, N}$; class label</p>
</blockquote>

<p><br /></p>

<p><strong>Training object</strong></p>

<ul>
  <li>support setì€ ë‹¨ì¼ ë°œí™” $x$ë¡œ êµ¬ì„±ë˜ì–´, prototype(centroid)ëŠ” ê° í™”ì %y%ì˜ support utteranceì™€ ê°™ìŒ</li>
</ul>
<center><img src="https://user-images.githubusercontent.com/46676700/95678027-a5714b00-0c04-11eb-816d-01da565f1eaa.png" alt="img" style="zoom: 80%;" /></center>

<p><br /></p>

<ul>
  <li>log-softmax functionì„ ì‚¬ìš©í•˜ëŠ” cross-entropy lossëŠ” ê°™ì€ speakerì˜ segment ê°„ ê±°ë¦¬ëŠ” ìµœì†Œí™”í•˜ë©´ì„œ ë‹¤ë¥¸ speaker ê°„ì˜ ê±°ë¦¬ëŠ” ìµœëŒ€í™”</li>
</ul>
<center><img src="https://user-images.githubusercontent.com/46676700/95678054-cb96eb00-0c04-11eb-9eb8-6e1a2c6ccb3a.png" alt="img" style="zoom: 80%;" /></center>

<p><br /></p>

<ul>
  <li>query embeddingì˜ í¬ê¸°ì™€ prototypeê³¼ queryì˜ cosine similarityë¥¼ distance metricìœ¼ë¡œ ì‚¬ìš© (<strong>Normalized prototypical, NP</strong>)</li>
</ul>
<center><img src="https://user-images.githubusercontent.com/46676700/95678059-d2bdf900-0c04-11eb-808e-efe28e67875f.png" alt="img" style="zoom: 80%;" /></center>

<ul>
  <li>kye et al.[16]ì€ speaker embeddingì„ ë³´ë‹¤ discriminativeí•˜ê²Œ ë§Œë“¤ê¸° ìœ„í•´ global classification lossì™€ í•¨ê»˜ <span style="background-color:#d2d8d8">episodic training*</span>ì„ ì‚¬ìš©
(few-shot taskì™€ ìœ ì‚¬í•œ í˜•íƒœì˜ í›ˆë ¨ taskë¥¼ í†µí•´ ëª¨ë¸ ìŠ¤ìŠ¤ë¡œ í•™ìŠµ ê·œì¹™ì„ ë„ì¶œí•  ìˆ˜ ìˆê²Œ í•¨ìœ¼ë¡œì¨ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë†’ì¼ ìˆ˜ ìˆìŒ <a href="https://www.kakaobrain.com/blog/106">ì°¸ì¡°-kakaobrainBlog</a>)</li>
  <li>global classificationì€ supportì™€ query set ëª¨ë‘ì— ì ìš©</li>
  <li>softmax classification lossë¥¼ í†µí•©í•˜ì—¬ mini-batchì— ìˆëŠ” classë¿ë§Œ ì•„ë‹ˆë¼ ëª¨ë“  classì— ëŒ€í•´ discriminativeí•˜ë„ë¡ embeddingì„ í›ˆë ¨ ê°€ëŠ¥</li>
  <li><strong>ìµœì¢…ì ì¸ objective function</strong>ì€ ë™ì¼í•œ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ <strong>NPì™€ softmax cross-entropy lossì˜ í•©</strong>(ë‹¨ìˆœ sum)</li>
</ul>

<p><br /></p>

<h3 id="22-instance-wise-aggregation"><strong>2.2 Instance-wise aggregation</strong></h3>

<ul>
  <li>ì´ìƒì ì¸ utterance-level embeddingì€ frequencyê°€ ì•„ë‹Œ temporal ìœ„ì¹˜ì— ë”°ë¼ ë‹¬ë¼ì ¸ì•¼í•¨</li>
  <li>2D convolutional neural networkëŠ” 2D activation mapì„ ìƒì„±í•˜ê¸° ë•Œë¬¸ì— frequency ì¶•ë§Œ ëª¨ë‘ ì—°ê²°ë˜ëŠ” aggregation layerë¥¼ [1]ì—ì„œ ì œì•ˆ</li>
  <li>ë”°ë¼ì„œ pooling layerì— ë“¤ì–´ê°€ê¸° ì „ 1xT feature map ìƒì„±</li>
</ul>

<p><br /></p>

<p><strong>Temporal Average Pooling(TAP)</strong></p>

<ul>
  <li>ë‹¨ìˆœí•˜ê²Œ temporal domainì— ëŒ€í•´ featureì˜ í‰ê· ì„ ì·¨í•¨</li>
</ul>
<center><img src="https://user-images.githubusercontent.com/46676700/95678556-70ff8e00-0c08-11eb-8d56-7d26175f42c7.png" alt="img" style="zoom: 80%;" /></center>

<p><br /></p>

<p><strong>Self-Attentive Pooling(SAP)</strong></p>

<ul>
  <li>ê° ì‹œê°„ì— ëŒ€í•œ frame ëª¨ë‘ ê°™ì€ weightë¥¼ ê°–ëŠ” TAPì™€ ë‹¬ë¦¬, utterance-levelì— ë” ë§ì€ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” frame-levelì— attentioní•¨</li>
</ul>
<center><img src="https://user-images.githubusercontent.com/46676700/95678562-7ceb5000-0c08-11eb-90d6-3498d822c878.png" alt="img" style="zoom: 80%;" /></center>

<blockquote>
  <p>frame-level íŠ¹ì§• $x_t$ê°€ ìš°ì„  parameter Wì™€ bë¥¼ ê°–ëŠ” MLPì˜ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ non-linearí•˜ê²Œ projection(hidden representationìœ¼ë¡œ mapping)</p>
</blockquote>

<p><br /></p>

<ul>
  <li>hidden vector $h_t$ì™€ í›ˆë ¨ë˜ëŠ” context vector $\mu$ ì‚¬ì´ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ score(hidden featureì˜ ìƒëŒ€ì ì¸ ì¤‘ìš”ë„)ë¡œ ì‚¬ìš©</li>
  <li>softmax functionì„ í†µí•´ ë‚˜ì˜¨ ê²°ê³¼ë¥¼ ê° frameì˜ ì¤‘ìš”ë„(attention weight)ë¡œ ì‚¬ìš©</li>
  <li>context vectorëŠ” speaker recognitionì— ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” high-level representationìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŒ</li>
</ul>
<center><img src="https://user-images.githubusercontent.com/46676700/95678567-807ed700-0c08-11eb-8766-296995b8de48.png" alt="img" style="zoom: 80%;" /></center>

<p><br /></p>

<ul>
  <li>utterance-level embedding $e$ëŠ” frame-level íŠ¹ì§•ê³¼ frame-levelì˜ attention weightì™€ ê°€ì¤‘í•©í•˜ì—¬ ì–»ì„ ìˆ˜ ìˆìŒ</li>
</ul>
<center><img src="https://user-images.githubusercontent.com/46676700/95678569-84aaf480-0c08-11eb-8291-24f23db5b892.png" alt="img" style="zoom: 80%;" /></center>

<p><br /></p>

<h3 id="23-pair-wise-aggregation"><strong>2.3 Pair-wise aggregation</strong></h3>

<ul>
  <li>ê¸°ì¡´ì˜ instance-wise aggregationê³¼ ë‹¬ë¦¬ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” <span style="background-color:#ffed54"><strong>ë‹¤ë¥¸ utteranceì˜ frame featureë¥¼ ì‚¬ìš©í•˜ì—¬ frame-level featureë¥¼ ëª¨ìœ¼ëŠ” ë°©ë²•</strong></span>ì„ ì œì•ˆ</li>
  <li>
    <p>trainingê³¼ testingì˜ ëª©í‘œë¥¼ ë§ì¶”ê¸° ìœ„í—¤ metricê¸°ë°˜ì˜ meta-learning frameworkì¸ prototypical network ì‚¬ìš©</p>
  </li>
  <li>ì´ frameworkì—ì„œ supportì™€ query set pairë¥¼ ì‚¬ìš©í•˜ì—¬ CAPë¥¼ í›ˆë ¨</li>
  <li>test ì‹œ, support setê³¼ query setì€ enrollmentì™€ test utteranceì— í•´ë‹¹</li>
</ul>

<p><br /></p>

<ul>
  <li>queryì™€ support setì˜ ëª¨ë“  utterance pairì— ëŒ€í•´ frame-level representation $s={s_1, s_2,\dots, s_{T_s}}, q={q_1, q_2,\dots, q_{T_q}}$ ì¶”ì¶œ</li>
  <li>meta-projection layer $g_{\Phi}(Â·)$ë¥¼ ì‚¬ìš©í•˜ì—¬ frame-levelì—ì„œ hidden feature ì¶”ì¶œ</li>
  <li>non-linear projectionì„ í†µí•´ ì„ì˜ì˜ frameì— ë¹ ë¥´ê²Œ ì ì‘í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ frame pairì˜ ìœ ì‚¬ë„ë¥¼ ì˜ ì¸¡ì •í•  ìˆ˜ ìˆìŒ</li>
  <li>ì´ layerëŠ” MLPì™€ ReLU activation functionìœ¼ë¡œ êµ¬ì„±</li>
</ul>
<center><img src="https://user-images.githubusercontent.com/46676700/95679512-50d2cd80-0c0e-11eb-846c-fa3f1bfe0bde.png" alt="img" style="zoom: 80%;" /></center>

<ul>
  <li>meta-projection layer ì´í›„, ëª¨ë“  frameì— ëŒ€í•œ hidden representationì¸ $S, Q$ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ</li>
</ul>

<blockquote>
  <p>$ S = {S_i}^{T_s}<em>{i=1}$<br />
$ Q = {Q_i}^{T_q}</em>{i=1}$<br />
$S_i, Q_i$ ëŠ” ê°ê° $g_{\Phi}(s_i), g_{\Phi}(q_i)$</p>
</blockquote>

<p><br /></p>

<p><strong>Correlation matrix</strong></p>

<ul>
  <li>Correlation matrix(ìƒê´€í–‰ë ¬) Rì€ ê°€ëŠ¥í•œ ëª¨ë“  frame pairì— ëŒ€í•œ similarityë¥¼ ìš”ì•½</li>
</ul>

<center><img src="https://user-images.githubusercontent.com/46676700/95679513-55978180-0c0e-11eb-8991-dc7ca123ddc4.png" alt="img" style="zoom: 80%;" /></center>

<blockquote>
  <p>$R^Q = (R^S)^T$; ìˆœì„œë§Œ ë°”ë€Œê¸° ë•Œë¬¸ì— $R^S$ì˜ transposeê°€ $R^Q$<br />
$R^S_{1, 1}$; support setì˜ 1ë²ˆì§¸ frame hidden representationê³¼ query setì˜ 1ë²ˆì§¸ frame hidden representationì˜ similarity<br />
ë”°ë¼ì„œ $R^S \in \mathbb{R}^{T_s \times T_q}$; [support set frame ìˆ˜ x query set frame ìˆ˜]</p>
</blockquote>

<p><br /></p>

<p><strong>Pair-adaptive attention</strong></p>

<ul>
  <li>pair-adaptive context vectorë¥¼ ì–»ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì´ timeì¶•ì— ëŒ€í•´ correlation matrixë¥¼ í‰ê· </li>
</ul>
<center><img src="https://user-images.githubusercontent.com/46676700/95679520-5a5c3580-0c0e-11eb-9c92-c802e2e3bcd0.png" alt="img" style="zoom: 80%;" /></center>

<blockquote>
  <p><strong>$\mu_s \in \mathbb{R}^{T_q}$</strong> ì´ê³ , $\mathbb{R}^S_{i,*}$ì€ $i$ë²ˆì§¸ row vector</p>
</blockquote>

<ul>
  <li>ë…¼ë¬¸ì—ì„œ $T_s$ë¡œ ë˜ì–´ìˆëŠ”ë°, $T_s$ê°€ ì•„ë‹Œ $T_q$ì´ ë˜ì–´ì•¼ ìˆ˜ì‹ì ìœ¼ë¡œ ë§ëŠ” ê²ƒ ê°™ìŒ (ê·¸ë¦¼ì—ì„œëŠ” context vectorì˜ sizeë¥¼ $T_q$ë¡œ í‘œê¸°)</li>
  <li>ê° row vectorëŠ” ë‹¤ë¥¸ utteranceì˜ ëª¨ë“  frameê³¼ì˜ ìœ ì‚¬ë„ ì •ë³´ê°€ ìˆìŒ</li>
  <li>ë”°ë¼ì„œ ë‹¤ë¥¸ utteranceì˜ ê° frameì— ëŒ€í•œ í‰ê·  ìƒê´€ê´€ê³„ë¥¼ $\mu$ë¡œ í‘œì‹œí•  ìˆ˜ ìˆê³ , ì´ëŠ” ë‹¤ë¥¸ utteranceì™€ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€ ê³„ì‚°í•˜ê¸° ìœ„í•´ context vectorë¡œ ì‚¬ìš©</li>
</ul>

<p><br /></p>

<ul>
  <li>attention weightëŠ” ëª¨ë“  utteranceì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°</li>
</ul>
<center><img src="https://user-images.githubusercontent.com/46676700/95679528-60521680-0c0e-11eb-9a06-8745d6fa010b.png" alt="img" style="zoom: 80%;" /></center>

<blockquote>
  <p>$\tau$ : temperature scaling (attention distributionì˜ ì„ ëª…ë„ ì¡°ì ˆ) - $\tau \rightarrow \infty$ì´ë©´ ë™ì¼í•œ attention weightë¥¼ ê°–ìŒ</p>
</blockquote>

<center><img src="https://user-images.githubusercontent.com/46676700/95679531-647e3400-0c0e-11eb-9e6c-b9b0e4c58a0e.png" alt="img" style="zoom: 80%;" /></center>

<ul>
  <li>Hou et al [22], utterance-levelì˜ íŠ¹ì§•ì„ ì–»ê¸° ìœ„í•´ residual attention mechanismì„ ì‚¬ìš©</li>
  <li>ë‹¤ë¥¸ utteranceì— ëŒ€í•´ì„œë„ ë™ì¼í•œ ë°©ë²•ìœ¼ë¡œ utterance-level feature $q$ë¡œ $e_q$ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ</li>
</ul>

<p><br /></p>

<p><strong>ì œì•ˆí•˜ëŠ” ë°©ë²•ì˜ procedure</strong></p>

<center><img src="https://user-images.githubusercontent.com/46676700/95680543-7a432780-0c15-11eb-80a4-709be1187867.png" alt="img" style="zoom: 80%;" /></center>
<center><img src="https://user-images.githubusercontent.com/46676700/95680550-829b6280-0c15-11eb-93fc-0ac5babd0115.png" alt="img" style="zoom: 80%;" /></center>

<p><br /></p>

<hr />

<p><br /></p>

<h1 id="â…²-experiments"><strong>â…¢. Experiments</strong></h1>

<p><strong>Model architecture</strong></p>

<p><img src="https://user-images.githubusercontent.com/46676700/95680589-cb531b80-0c15-11eb-9d17-c3ead5a27fd8.png" alt="img" /></p>

<p><br /></p>

<p><strong>Results</strong></p>

<p><img src="https://user-images.githubusercontent.com/46676700/95680595-d0b06600-0c15-11eb-8d5a-b8b7166ea620.png" alt="img" /></p>

        </article>
        <div class="post-content">
         <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<div id="gitalk-container"></div>

<script data-no-instant type="text/javascript">
const gitalk = new Gitalk({
  clientID: '5304ae17c5ba0ce3b2aa',
  clientSecret: '22801b52c86101ff074072faa1631475954d2936',
  repo: 'arabae.github.io',
  owner: 'arabae',
  admin: ['arabae'],
  id: location.pathname,      // Ensure uniqueness and length less than 50
  distractionFreeMode: true  // Facebook-like distraction free mode
})

gitalk.render('gitalk-container')
</script>
        </div>
    </div>
</div>

    
    <footer id="footer" class="footer bg-white">
    <div class="footer-social">
        <div class="footer-container clearfix">
            <div class="social-list">
                <a href="/"><span class='iconfont icon-home'></span>&nbsp;&nbsp;HOME</a>
                <a rel="nofollow" target="_blank" href="https://github.com/arabae"><span class='iconfont icon-github'></span>&nbsp;&nbsp;Github</a>
                <a target="_blank" href="/feed.xml"><span class='iconfont icon-rss'></span>&nbsp;&nbsp;RSS</a>
            </div>
        </div>
    </div>
    <div class="footer-meta">
        <div class="footer-container">
            <div class="meta-item meta-copyright">
                <div class="meta-copyright-info">
                    <a href="https://github.com/arabae" class="info-logo">
                        <img src="/style/image/logo.png" alt="wonder">
                    </a>
                    <div class="info-text">
                        <p>Copyright &copy; 2021 - 2021 <a href="https://github.com/arabae"><code>ARa Bae</code></a></p>
                        <p>Powered by <a href="http://jekyllrb.com" target="_blank" rel="nofollow"><code>jekyll</code></a>ï¼Œtheme is <a href="https://github.com/lightfish-zhang/pinghsu-jekyll" target="_blank" rel="nofollow"><code>pinghsu</code></a></p>
                    </div>
                </div>
            </div>

            <div class="meta-item meta-posts">
                <h3 class="meta-title">RECENT POSTS</h3>
                
                    <li>
                        <a href="/2019-10-13-Cross-Attentive-Pooling-for-SV">Cross attentive pooling for speaker verification</a>
                    </li>
                
                    <li>
                        <a href="/2019-10-06-Metric-Laerning-for-Keyword-Spotting">Metric Learning for Keyword Spotting</a>
                    </li>
                
                    <li>
                        <a href="/2019-07-30-Attention-based-models-for-TDSV">Attention-based Models For Text-dependent Speaker Verification</a>
                    </li>
                
                    <li>
                        <a href="/2019-07-24-TISV-with-Adversarial-Learning-on-Short-Utterances">Text-Independent Speaker Verification with Adversarial Learning on Short Utterances</a>
                    </li>
                
                    <li>
                        <a href="/2019-07-10-GE2E-loss-for-SV">Generalized End to End Loss For Speaker Verification</a>
                    </li>
                
                    <li>
                        <a href="/2019-06-03-Generative-Adversarial-Speaker-Embedding-Networks-for-Domain-Roubust-E2E-SV">Generative Adversarial Speaker Embedding Networks for Domain Robust End-to-End Speaker Verification</a>
                    </li>
                
                    <li>
                        <a href="/2019-05-22-E2E-DNN-based-Speaker-Recognition-Inspired-by-i-vector-and-PLDA">End-to-End DNN based Speaker Recognition Inspired by i-vector and PLDA</a>
                    </li>
                
            </div>

        </div>
    </div>
</footer>

<!-- #end -->
<script src="//cdn.bootcss.com/jquery/1.10.1/jquery.min.js"></script>
<script>
	!window.jQuery && document.write(unescape('%3Cscript src="/style/js/jquery.min.js"%3E%3C/script%3E'))
</script>
<script src="/style/js/headroom.min.js"></script>
<script src="/style/js/nav.min.js"></script>
<script type="text/javascript">
    var header = new Headroom(document.getElementById("header"), {
        tolerance: 10,
        offset : 80,
        classes: {
            initial: "animated",
            pinned: "slideDown",
            unpinned: "slideUp"
        }
    });
    header.init();
</script>

<script>window.SmoothScrollOptions = { stepSize: 36 }</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/smoothscroll/1.4.8/SmoothScroll.min.js"></script>
<script>
	!window.SmoothScroll && document.write(unescape('%3Cscript src="/style/js/SmoothScroll.min.js"%3E%3C/script%3E'))
</script>




  </body>
</html>

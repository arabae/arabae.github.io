<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <description>Speech/Speaker Recognition Study</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sat, 09 Jan 2021 19:21:09 +0900</pubDate>
    <lastBuildDate>Sat, 09 Jan 2021 19:21:09 +0900</lastBuildDate>
    <generator>Jekyll v3.8.7</generator>
    
      <item>
        <title>Cross attentive pooling for speaker verification</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#-abstract&quot; id=&quot;markdown-toc--abstract&quot;&gt;ğŸ“Œ &lt;strong&gt;Abstract&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#â…°-introduction&quot; id=&quot;markdown-toc-â…°-introduction&quot;&gt;&lt;strong&gt;â… . Introduction&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#â…±-methods&quot; id=&quot;markdown-toc-â…±-methods&quot;&gt;&lt;strong&gt;â…¡. Methods&lt;/strong&gt;&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#21-few-shot-learning-framwork&quot; id=&quot;markdown-toc-21-few-shot-learning-framwork&quot;&gt;&lt;strong&gt;2.1 Few-shot learning framwork&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#22-instance-wise-aggregation&quot; id=&quot;markdown-toc-22-instance-wise-aggregation&quot;&gt;&lt;strong&gt;2.2 Instance-wise aggregation&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#23-pair-wise-aggregation&quot; id=&quot;markdown-toc-23-pair-wise-aggregation&quot;&gt;&lt;strong&gt;2.3 Pair-wise aggregation&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#â…²-experiments&quot; id=&quot;markdown-toc-â…²-experiments&quot;&gt;&lt;strong&gt;â…¢. Experiments&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span style=&quot;font-size:13pt&quot;&gt;Seong Min Kye, Yoohwan Kwon, Joon Son Chung&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;-abstract&quot;&gt;ğŸ“Œ &lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;ëª©í‘œ : â€˜in the wildâ€™ videoì™€ ê´€ë ¨ì—†ëŠ” signalì„ í¬í•¨í•˜ëŠ” utteranceë¥¼ ì‚¬ìš©í•˜ëŠ” TI-SV(Text-Independent Speaker Verification)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;SVëŠ” pair-wise ë¬¸ì œ(ë“±ë¡ê³¼ í…ŒìŠ¤íŠ¸ ìŒì„ ë¹„êµ), ê¸°ì¡´ì˜ embedding ì¶”ì¶œì€ instance-wise ë¬¸ì œ(ê° utteranceì— ëŒ€í•œ embeddingì„ ì¶”ì¶œí•˜ì—¬ ì„œë¡œ ë¹„êµ)&lt;/li&gt;
  &lt;li&gt;&lt;span style=&quot;background-color:#ffed54&quot;&gt;ë³¸ ë…¼ë¬¸ì—ì„œëŠ” reference-query pair ì „ì²´ì˜ context ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ &lt;strong&gt;pair-wise ë¬¸ì œì— ê°€ì¥ discriminativeí•œ utterance-levelì˜ embedding ì¶”ì¶œì„ ìƒì„±&lt;/strong&gt;í•˜ëŠ” &lt;strong&gt;CAP(Cross Attention Pooling)&lt;/strong&gt;ì„ ì œì•ˆ&lt;/span&gt;&lt;/li&gt;
  &lt;li&gt;VoxCeleb datasetì„ ì‚¬ìš©í•˜ê³ , ë‹¤ë¥¸ pooling ë°©ë²•ê³¼ ë¹„êµí•˜ì—¬ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŒ&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;â…°-introduction&quot;&gt;&lt;strong&gt;â… . Introduction&lt;/strong&gt;&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Automatic Speaker Recognition; ìŒì„±ì€ ê°€ì¥ ì‰½ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆëŠ” ìƒì²´ ì •ë³´ ì¤‘ í•˜ë‚˜ì´ê¸° ë•Œë¬¸ì— ëˆ„êµ°ê°€ì˜ ì‹ ì›ì„ í™•ì¸í•˜ëŠ”ë° ë§¤ë ¥ì ì¸ ë°©ë²•&lt;/li&gt;
  &lt;li&gt;speaker recognitionì€ identificationê³¼ verificationì„ ëª¨ë‘ í¬í•¨í•˜ì§€ë§Œ, í›„ìì˜ ê²½ìš° ë” ì‹¤ìš©ì ì¸ ì‘ìš© ë¶„ì•¼ë¥¼ ê°€ì§(ex. ì½œì„¼í„°, AI ìŠ¤í”¼ì»¤ ë“±)&lt;/li&gt;
  &lt;li&gt;closed-set identificationê³¼ ë‹¬ë¦¬ open-set verificationì€ í›ˆë ¨ì—ì„œ ë³´ì§€ ëª»í–ˆë˜ í™”ìì˜ identityë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•˜ê¸° ë•Œë¬¸ì—, speaker verificationì€ ìŒì„±ì´ discriminativeí•œ embedding ì°¨ì›ì˜ í‘œí˜„ìœ¼ë¡œ mappingë˜ì–´ì•¼í•˜ëŠ” metric learning ë¬¸ì œ&lt;/li&gt;
  &lt;li&gt;ë‹¤ë¥¸ ë…¼ë¬¸ë“¤ì—ì„œ ì£¼ë¡œ classification lossë¥¼ ì‚¬ìš©í•˜ì—¬ embeddingì„ í•™ìŠµí•˜ì˜€ìœ¼ë‚˜ embedding similarityë¥¼ ìµœì í™”í•˜ë„ë¡ ì„¤ê³„ë˜ì§€ ì•ŠìŒ&lt;/li&gt;
  &lt;li&gt;ìµœê·¼ ì—°êµ¬ë“¤ì—ì„œ class ê°„ ë¶„ë¦¬ë¥¼ ê°•í™”í•˜ê¸° ìœ„í•´ verificationì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì§„ margin variantë¥¼ ì¶”ê°€í•œ softmaxë¥¼ ì ‘ëª©ì‹œí‚´ (AM-softmax)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;open-set verification&lt;/strong&gt;ì€ networkê°€ ì œí•œëœ exampleì„ ê°–ìœ¼ë©´ì„œ unseen classì— ëŒ€í•´ ì¸ì‹í•´ì•¼í•˜ë¯€ë¡œ &lt;strong&gt;few-shot learning&lt;/strong&gt; ë¬¸ì œë¼ê³  ë³¼ ìˆ˜ ìˆìŒ&lt;/li&gt;
  &lt;li&gt;few-shot learning ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ëª¨ë°©í•˜ëŠ” &lt;strong&gt;prototypical network&lt;/strong&gt;ê°€ ì œì•ˆë˜ì—ˆìœ¼ë©°, &lt;strong&gt;ìµœê·¼ speaker verificationì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë‹¬ì„±&lt;/strong&gt;í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¨&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;similarity metricì„ ìµœì í™”í•˜ë„ë¡ networkë¥¼ í›ˆë ¨ì‹œí‚¤ê¸° ìœ„í•´ì„œëŠ” frame-levelì˜ representation(feature)ë¥¼ utterance-levelë¡œ ëª¨ì•„ì•¼ í•¨&lt;/li&gt;
  &lt;li&gt;ê°€ì¥ ë‹¨ìˆœí•œ ë°©ë²•ì€ frame-levelì„ í‰ê· í•˜ëŠ” ê²ƒ(TAP, Temporal Average Pooling), ì´ë•Œ frameë“¤ì€ ëª¨ë‘ ê°™ì€ weightë¥¼ ê°–ê²Œ ë¨&lt;/li&gt;
  &lt;li&gt;verificationì— ë” discriminativeí•œ frameì— attentioní•˜ë„ë¡ SAP(Self-Attentive Pooling)ë°©ë²•ì´ ì œì•ˆ&lt;/li&gt;
  &lt;li&gt;ê·¸ëŸ¬ë‚˜ instance-level self-attentionì€ support set(training set)ì˜ íŠ¹ì • sampleì´ ì•„ë‹Œ, ì¼ë°˜ì ìœ¼ë¡œ(training setì˜ ì „ì²´ dataë¥¼ ì•„ìš°ë¦„) discriminativeí•œ featureë¥¼ ì°¾ìŒ; training datasetì˜ ì „ì²´ì ì¸ íŠ¹ì„±ì´ ë°˜ì˜ë˜ì–´ íŠ¹ì • sampleì— ëŒ€í•´ì„œëŠ” íš¨ê³¼ì ì´ì§€ ì•Šì„ ìˆ˜ ìˆìŒ&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CAN(Cross Attention Network): few-shot learningì—ì„œ ìµœê·¼ support setì˜ exampleë“¤ê³¼ ê´€ë ¨ìˆê³ , discriminativeí•œ input imageì˜ ë¶€ë¶„ì— attentioní•¨ìœ¼ë¡œì¨ unseen target classë¥¼ ê¸°ë°˜ì˜ attentionì„ ì„ íƒí•  ìˆ˜ ìˆë„ë¡ ì œì•ˆëœ ë°©ë²•&lt;/li&gt;
  &lt;li&gt;support setì˜ í•œ class(speaker)ì™€ utteranceë¥¼ ë¹„êµí•˜ê¸° ìœ„í•œ discriminativeí•œ íŠ¹ì„±ì´ ë‹¤ë¥¸ classì™€ ë¹„êµí•˜ê¸° ìœ„í•´ ìƒì„±ë˜ëŠ” íŠ¹ì§•ê³¼ ë‹¤ë¥¼ ê²ƒ, ë”°ë¼ì„œ ì´ ì•„ì´ë””ì–´ë¥¼ speaker verificationì— ì ìš©í•  ìˆ˜ ìˆìŒ&lt;/li&gt;
  &lt;li&gt;ë³¸ ë…¼ë¬¸ì—ì„œëŠ” frame-levelì˜ ì •ë³´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ utterance-levelì˜ embeddingìœ¼ë¡œ ëª¨ìœ¼ê¸° ìœ„í•´ support setì˜ exampleì„ ì°¸ì¡°í•˜ì—¬ attentionì„ ê³„ì‚°í•˜ëŠ” CAP(Cross Attentive Pooling)ë¥¼ ì œì•ˆ&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ networkëŠ” support setì˜ íŠ¹ì • classì— ëŒ€í•œ íŠ¹ì • íŠ¹ì§•ì„ ì œê³µí•˜ëŠ” utteranceì„ ì‹ë³„í•˜ê³  ì§‘ì¤‘ì‹œí‚¬ ìˆ˜ ìˆìŒ&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;ì´ëŠ” ì‚¬ëŒì´ unseen classì˜ instanceë¥¼ ì¸ì‹í•  ë•Œ, sample ìŒë“¤ì˜ ê³µí†µì ì¸ íŠ¹ì„±ì„ ê°–ëŠ” íŠ¹ì§•ì„ ì°¾ëŠ” ê²ƒê³¼ ìœ ì‚¬í•¨&lt;/li&gt;
  &lt;li&gt;instance-levelì˜ poolingê³¼ ë‹¬ë¦¬, ì œì•ˆëœ attention moduleì€ class(prototype) featureì™€ query featureì˜ ê´€ë ¨ì„±ì„ ëª¨ë¸ë§í•˜ì—¬ verification taskì—ì„œ pair-wise íŠ¹ì„±ì„ ìµœëŒ€í•œ í™œìš©&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;â…±-methods&quot;&gt;&lt;strong&gt;â…¡. Methods&lt;/strong&gt;&lt;/h1&gt;

&lt;h3 id=&quot;21-few-shot-learning-framwork&quot;&gt;&lt;strong&gt;2.1 Few-shot learning framwork&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Speaker recognitionì„ ìœ„í•œ embeddingì„ í›ˆë ¨í•˜ê¸° ìœ„í•´ few-shot learning frameworkì¸ prototypical network ì‚¬ìš©&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Batch formation&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ê° mini-batchì—ëŠ” support(training) set $S$ì™€ query(test) set $Q$ê°€ í¬í•¨&lt;/li&gt;
  &lt;li&gt;ì„œë¡œ ë‹¤ë¥¸ í™”ì Nëª…ë§ˆë‹¤ Mê°œì˜ ë°œí™” í¬í•¨&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;

$S = {(x_i, y_i)}^{N \times 1}_{i=1}$  

$Q = {(\tilde{x_i}, \tilde{y_i})}^{N \times (M-1)}_{i=1}$  

&lt;/center&gt;

&lt;blockquote&gt;
  &lt;p&gt;support setì€ ê° í™”ìë§ˆë‹¤ 1ê°œì˜ ë°œí™”ë¥¼ ì‚¬ìš©í•˜ê³ , query setì€ ë‚˜ë¨¸ì§€ ë°œí™”($2 \leq i \leq M$)ë¥¼ ì‚¬ìš©&lt;br /&gt;
$y, \tilde{y} \in {1, â€¦, N}$; class label&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Training object&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;support setì€ ë‹¨ì¼ ë°œí™” $x$ë¡œ êµ¬ì„±ë˜ì–´, prototype(centroid)ëŠ” ê° í™”ì %y%ì˜ support utteranceì™€ ê°™ìŒ&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/95678027-a5714b00-0c04-11eb-816d-01da565f1eaa.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;log-softmax functionì„ ì‚¬ìš©í•˜ëŠ” cross-entropy lossëŠ” ê°™ì€ speakerì˜ segment ê°„ ê±°ë¦¬ëŠ” ìµœì†Œí™”í•˜ë©´ì„œ ë‹¤ë¥¸ speaker ê°„ì˜ ê±°ë¦¬ëŠ” ìµœëŒ€í™”&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/95678054-cb96eb00-0c04-11eb-9eb8-6e1a2c6ccb3a.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;query embeddingì˜ í¬ê¸°ì™€ prototypeê³¼ queryì˜ cosine similarityë¥¼ distance metricìœ¼ë¡œ ì‚¬ìš© (&lt;strong&gt;Normalized prototypical, NP&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/95678059-d2bdf900-0c04-11eb-808e-efe28e67875f.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;kye et al.[16]ì€ speaker embeddingì„ ë³´ë‹¤ discriminativeí•˜ê²Œ ë§Œë“¤ê¸° ìœ„í•´ global classification lossì™€ í•¨ê»˜ &lt;span style=&quot;background-color:#d2d8d8&quot;&gt;episodic training*&lt;/span&gt;ì„ ì‚¬ìš©
(few-shot taskì™€ ìœ ì‚¬í•œ í˜•íƒœì˜ í›ˆë ¨ taskë¥¼ í†µí•´ ëª¨ë¸ ìŠ¤ìŠ¤ë¡œ í•™ìŠµ ê·œì¹™ì„ ë„ì¶œí•  ìˆ˜ ìˆê²Œ í•¨ìœ¼ë¡œì¨ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë†’ì¼ ìˆ˜ ìˆìŒ &lt;a href=&quot;https://www.kakaobrain.com/blog/106&quot;&gt;ì°¸ì¡°-kakaobrainBlog&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;global classificationì€ supportì™€ query set ëª¨ë‘ì— ì ìš©&lt;/li&gt;
  &lt;li&gt;softmax classification lossë¥¼ í†µí•©í•˜ì—¬ mini-batchì— ìˆëŠ” classë¿ë§Œ ì•„ë‹ˆë¼ ëª¨ë“  classì— ëŒ€í•´ discriminativeí•˜ë„ë¡ embeddingì„ í›ˆë ¨ ê°€ëŠ¥&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ìµœì¢…ì ì¸ objective function&lt;/strong&gt;ì€ ë™ì¼í•œ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ &lt;strong&gt;NPì™€ softmax cross-entropy lossì˜ í•©&lt;/strong&gt;(ë‹¨ìˆœ sum)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;22-instance-wise-aggregation&quot;&gt;&lt;strong&gt;2.2 Instance-wise aggregation&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;ì´ìƒì ì¸ utterance-level embeddingì€ frequencyê°€ ì•„ë‹Œ temporal ìœ„ì¹˜ì— ë”°ë¼ ë‹¬ë¼ì ¸ì•¼í•¨&lt;/li&gt;
  &lt;li&gt;2D convolutional neural networkëŠ” 2D activation mapì„ ìƒì„±í•˜ê¸° ë•Œë¬¸ì— frequency ì¶•ë§Œ ëª¨ë‘ ì—°ê²°ë˜ëŠ” aggregation layerë¥¼ [1]ì—ì„œ ì œì•ˆ&lt;/li&gt;
  &lt;li&gt;ë”°ë¼ì„œ pooling layerì— ë“¤ì–´ê°€ê¸° ì „ 1xT feature map ìƒì„±&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Temporal Average Pooling(TAP)&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ë‹¨ìˆœí•˜ê²Œ temporal domainì— ëŒ€í•´ featureì˜ í‰ê· ì„ ì·¨í•¨&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/95678556-70ff8e00-0c08-11eb-8d56-7d26175f42c7.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Self-Attentive Pooling(SAP)&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ê° ì‹œê°„ì— ëŒ€í•œ frame ëª¨ë‘ ê°™ì€ weightë¥¼ ê°–ëŠ” TAPì™€ ë‹¬ë¦¬, utterance-levelì— ë” ë§ì€ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” frame-levelì— attentioní•¨&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/95678562-7ceb5000-0c08-11eb-90d6-3498d822c878.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/center&gt;

&lt;blockquote&gt;
  &lt;p&gt;frame-level íŠ¹ì§• $x_t$ê°€ ìš°ì„  parameter Wì™€ bë¥¼ ê°–ëŠ” MLPì˜ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ non-linearí•˜ê²Œ projection(hidden representationìœ¼ë¡œ mapping)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;hidden vector $h_t$ì™€ í›ˆë ¨ë˜ëŠ” context vector $\mu$ ì‚¬ì´ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ score(hidden featureì˜ ìƒëŒ€ì ì¸ ì¤‘ìš”ë„)ë¡œ ì‚¬ìš©&lt;/li&gt;
  &lt;li&gt;softmax functionì„ í†µí•´ ë‚˜ì˜¨ ê²°ê³¼ë¥¼ ê° frameì˜ ì¤‘ìš”ë„(attention weight)ë¡œ ì‚¬ìš©&lt;/li&gt;
  &lt;li&gt;context vectorëŠ” speaker recognitionì— ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” high-level representationìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŒ&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/95678567-807ed700-0c08-11eb-8766-296995b8de48.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;utterance-level embedding $e$ëŠ” frame-level íŠ¹ì§•ê³¼ frame-levelì˜ attention weightì™€ ê°€ì¤‘í•©í•˜ì—¬ ì–»ì„ ìˆ˜ ìˆìŒ&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/95678569-84aaf480-0c08-11eb-8291-24f23db5b892.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;23-pair-wise-aggregation&quot;&gt;&lt;strong&gt;2.3 Pair-wise aggregation&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;ê¸°ì¡´ì˜ instance-wise aggregationê³¼ ë‹¬ë¦¬ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” &lt;span style=&quot;background-color:#ffed54&quot;&gt;&lt;strong&gt;ë‹¤ë¥¸ utteranceì˜ frame featureë¥¼ ì‚¬ìš©í•˜ì—¬ frame-level featureë¥¼ ëª¨ìœ¼ëŠ” ë°©ë²•&lt;/strong&gt;&lt;/span&gt;ì„ ì œì•ˆ&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;trainingê³¼ testingì˜ ëª©í‘œë¥¼ ë§ì¶”ê¸° ìœ„í—¤ metricê¸°ë°˜ì˜ meta-learning frameworkì¸ prototypical network ì‚¬ìš©&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;ì´ frameworkì—ì„œ supportì™€ query set pairë¥¼ ì‚¬ìš©í•˜ì—¬ CAPë¥¼ í›ˆë ¨&lt;/li&gt;
  &lt;li&gt;test ì‹œ, support setê³¼ query setì€ enrollmentì™€ test utteranceì— í•´ë‹¹&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;queryì™€ support setì˜ ëª¨ë“  utterance pairì— ëŒ€í•´ frame-level representation $s={s_1, s_2,\dots, s_{T_s}}, q={q_1, q_2,\dots, q_{T_q}}$ ì¶”ì¶œ&lt;/li&gt;
  &lt;li&gt;meta-projection layer $g_{\Phi}(Â·)$ë¥¼ ì‚¬ìš©í•˜ì—¬ frame-levelì—ì„œ hidden feature ì¶”ì¶œ&lt;/li&gt;
  &lt;li&gt;non-linear projectionì„ í†µí•´ ì„ì˜ì˜ frameì— ë¹ ë¥´ê²Œ ì ì‘í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ frame pairì˜ ìœ ì‚¬ë„ë¥¼ ì˜ ì¸¡ì •í•  ìˆ˜ ìˆìŒ&lt;/li&gt;
  &lt;li&gt;ì´ layerëŠ” MLPì™€ ReLU activation functionìœ¼ë¡œ êµ¬ì„±&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/95679512-50d2cd80-0c0e-11eb-846c-fa3f1bfe0bde.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;meta-projection layer ì´í›„, ëª¨ë“  frameì— ëŒ€í•œ hidden representationì¸ $S, Q$ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;$ S = {S_i}^{T_s}&lt;em&gt;{i=1}$&lt;br /&gt;
$ Q = {Q_i}^{T_q}&lt;/em&gt;{i=1}$&lt;br /&gt;
$S_i, Q_i$ ëŠ” ê°ê° $g_{\Phi}(s_i), g_{\Phi}(q_i)$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Correlation matrix&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Correlation matrix(ìƒê´€í–‰ë ¬) Rì€ ê°€ëŠ¥í•œ ëª¨ë“  frame pairì— ëŒ€í•œ similarityë¥¼ ìš”ì•½&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/95679513-55978180-0c0e-11eb-8991-dc7ca123ddc4.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/center&gt;

&lt;blockquote&gt;
  &lt;p&gt;$R^Q = (R^S)^T$; ìˆœì„œë§Œ ë°”ë€Œê¸° ë•Œë¬¸ì— $R^S$ì˜ transposeê°€ $R^Q$&lt;br /&gt;
$R^S_{1, 1}$; support setì˜ 1ë²ˆì§¸ frame hidden representationê³¼ query setì˜ 1ë²ˆì§¸ frame hidden representationì˜ similarity&lt;br /&gt;
ë”°ë¼ì„œ $R^S \in \mathbb{R}^{T_s \times T_q}$; [support set frame ìˆ˜ x query set frame ìˆ˜]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pair-adaptive attention&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;pair-adaptive context vectorë¥¼ ì–»ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì´ timeì¶•ì— ëŒ€í•´ correlation matrixë¥¼ í‰ê· &lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/95679520-5a5c3580-0c0e-11eb-9c92-c802e2e3bcd0.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/center&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;$\mu_s \in \mathbb{R}^{T_q}$&lt;/strong&gt; ì´ê³ , $\mathbb{R}^S_{i,*}$ì€ $i$ë²ˆì§¸ row vector&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;ë…¼ë¬¸ì—ì„œ $T_s$ë¡œ ë˜ì–´ìˆëŠ”ë°, $T_s$ê°€ ì•„ë‹Œ $T_q$ì´ ë˜ì–´ì•¼ ìˆ˜ì‹ì ìœ¼ë¡œ ë§ëŠ” ê²ƒ ê°™ìŒ (ê·¸ë¦¼ì—ì„œëŠ” context vectorì˜ sizeë¥¼ $T_q$ë¡œ í‘œê¸°)&lt;/li&gt;
  &lt;li&gt;ê° row vectorëŠ” ë‹¤ë¥¸ utteranceì˜ ëª¨ë“  frameê³¼ì˜ ìœ ì‚¬ë„ ì •ë³´ê°€ ìˆìŒ&lt;/li&gt;
  &lt;li&gt;ë”°ë¼ì„œ ë‹¤ë¥¸ utteranceì˜ ê° frameì— ëŒ€í•œ í‰ê·  ìƒê´€ê´€ê³„ë¥¼ $\mu$ë¡œ í‘œì‹œí•  ìˆ˜ ìˆê³ , ì´ëŠ” ë‹¤ë¥¸ utteranceì™€ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€ ê³„ì‚°í•˜ê¸° ìœ„í•´ context vectorë¡œ ì‚¬ìš©&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;attention weightëŠ” ëª¨ë“  utteranceì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/95679528-60521680-0c0e-11eb-9a06-8745d6fa010b.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/center&gt;

&lt;blockquote&gt;
  &lt;p&gt;$\tau$ : temperature scaling (attention distributionì˜ ì„ ëª…ë„ ì¡°ì ˆ) - $\tau \rightarrow \infty$ì´ë©´ ë™ì¼í•œ attention weightë¥¼ ê°–ìŒ&lt;/p&gt;
&lt;/blockquote&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/95679531-647e3400-0c0e-11eb-9e6c-b9b0e4c58a0e.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;Hou et al [22], utterance-levelì˜ íŠ¹ì§•ì„ ì–»ê¸° ìœ„í•´ residual attention mechanismì„ ì‚¬ìš©&lt;/li&gt;
  &lt;li&gt;ë‹¤ë¥¸ utteranceì— ëŒ€í•´ì„œë„ ë™ì¼í•œ ë°©ë²•ìœ¼ë¡œ utterance-level feature $q$ë¡œ $e_q$ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ì œì•ˆí•˜ëŠ” ë°©ë²•ì˜ procedure&lt;/strong&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/95680543-7a432780-0c15-11eb-80a4-709be1187867.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/center&gt;
&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/95680550-829b6280-0c15-11eb-93fc-0ac5babd0115.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;â…²-experiments&quot;&gt;&lt;strong&gt;â…¢. Experiments&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Model architecture&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/95680589-cb531b80-0c15-11eb-9d17-c3ead5a27fd8.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/95680595-d0b06600-0c15-11eb-8d5a-b8b7166ea620.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 13 Oct 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2019-10-13-Cross-Attentive-Pooling-for-SV</link>
        <guid isPermaLink="true">http://localhost:4000/2019-10-13-Cross-Attentive-Pooling-for-SV</guid>
        
        
        <category>review</category>
        
      </item>
    
      <item>
        <title>Metric Learning for Keyword Spotting</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#-abstract&quot; id=&quot;markdown-toc--abstract&quot;&gt;ğŸ“Œ &lt;strong&gt;Abstract&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#â…°-introduction&quot; id=&quot;markdown-toc-â…°-introduction&quot;&gt;&lt;strong&gt;â… . Introduction&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#â…±-metric-learning-framework&quot; id=&quot;markdown-toc-â…±-metric-learning-framework&quot;&gt;&lt;strong&gt;â…¡. Metric Learning Framework&lt;/strong&gt;&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#21-loss-functions&quot; id=&quot;markdown-toc-21-loss-functions&quot;&gt;&lt;strong&gt;2.1 Loss functions&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#22-pair-selection-strategy&quot; id=&quot;markdown-toc-22-pair-selection-strategy&quot;&gt;&lt;strong&gt;2.2 Pair selection strategy&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#23-prototypical-networks-with-fixed-target-classes&quot; id=&quot;markdown-toc-23-prototypical-networks-with-fixed-target-classes&quot;&gt;&lt;strong&gt;2.3 Prototypical networks with fixed target classes&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span style=&quot;font-size:13pt&quot;&gt;Jaesung Huh, Minjae Lee, Heesoo Heo, Seongkyu Mun, Joon Son Chung&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;-abstract&quot;&gt;ğŸ“Œ &lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;ëª©í‘œ : Metric learningì„ í†µí•´ keyword spotting(ìŒì„± ì…ë ¥ ì¤‘ì— íŠ¹ì • ë‹¨ì–´ë¥¼ ë°œí™”í•˜ì˜€ëŠ”ì§€ ê²€ì¶œ)ì„ ìœ„í•œ íš¨ê³¼ì ì¸ representationsë¥¼ í›ˆë ¨í•˜ëŠ” ê²ƒ&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;ê¸°ì¡´ ë°©ë²•&lt;/strong&gt; : target/non-target keywordë“¤ì´ ëª¨ë‘ ì‚¬ì „ì— ì •ì˜ëœ closed-set classification ë¬¸ì œë§Œ ë‹¤ë£¨ê¸°ë•Œë¬¸ì— unseen non-targetì— ëŒ€í•´ ì„±ëŠ¥ì´ ì €í•˜ë˜ì–´ real-worldì—ì„œ ë†’ì€ FAR(False Alarm Rate)ì„ ë³´ì„&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;keyword spottingì€ ë‹¤ì–‘í•œ unknown soundì—ì„œ ì‚¬ì „ì— ì •ì˜ëœ target keywordë¥¼ detectioní•˜ëŠ” ë¬¸ì œë¡œ, unseen/unknown non-targetì´ target keyworkdì™€ ëª…í™•íˆ êµ¬ë³„ë˜ì–´ì•¼ í•œë‹¤ëŠ” ì ì´ metric learningê³¼ ìœ ì‚¬í•œ ì ì´ ë§ìŒ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ì£¼ìš” ì°¨ì´ì ì€ &lt;strong&gt;target keywordê°€ ì•Œë ¤ì ¸ ìˆê³ , ì´ë¯¸ ì •ì˜ë˜ì–´ ìˆë‹¤ëŠ” ì &lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;span style=&quot;background-color:#ffed54&quot;&gt;ë”°ë¼ì„œ &lt;strong&gt;target keywordì™€ non-target keyword ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ìµœëŒ€í™”&lt;/strong&gt; í•˜ê³  ë¶„ë¥˜ ëª©í‘œì— ë”°ë¼ &lt;strong&gt;target keywordì— ëŒ€í•œ class ë³„ ê°€ì¤‘ì¹˜ë¥¼ í•™ìŠµ&lt;/strong&gt; í•˜ëŠ” metric learningê¸°ë°˜ì˜ ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì•ˆ&lt;/span&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Goodle Speech Commands datasestìœ¼ë¡œ ì‹¤í—˜ì„ ìˆ˜í–‰í•˜ì˜€ìœ¼ë©° ì „ì²´ì ì¸ classification ì •í™•ë„ë¥¼ ìœ ì§€í•˜ë©´ì„œ unseen non-target keywordë“¤ì— ëŒ€í•œ FA(False Alarm)ì„ í¬ê²Œ ê°ì†Œì‹œí‚´&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;â…°-introduction&quot;&gt;&lt;strong&gt;â… . Introduction&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;âœ” Keyword Spotting(KWS)&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ë‹¤ì–‘í•œ mobile deviceë¥¼ ë™ì‘ì‹œí‚¤ëŠ” ë‹¨ì–´(wake-up words, â€œOK Googleâ€, â€œHey Siriâ€, â€œAlexaâ€) ë˜ëŠ” ìì£¼ ì‚¬ìš©í•˜ëŠ” ì§§ì€ ëª…ë ¹ê³¼ ê°™ì´ ë¯¸ë¦¬ ì •ì˜ëœ ì‘ì€ ìŒì„± ì‹ í˜¸ ì§‘í•©ì„ detectioní•˜ëŠ” ì‘ì—…&lt;/li&gt;
  &lt;li&gt;ìµœê·¼ CNNê¸°ë°˜ì˜ architectureë“¤ì´ ì´ ë¶„ì•¼ì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ì˜€ê³ , ì£¼ë¡œ target keywordì™€ ì¼ë°˜ì ì¸ ìŒì„±ì´ë‚˜ ì¡ìŒê°™ì€ non-target soundë¥¼ êµ¬ë¶„í•˜ëŠ” classifierë¥¼ ê¸°ë°˜í•¨&lt;/li&gt;
  &lt;li&gt;non-target classëŠ” ë§¤ìš° ë‹¤ì–‘í•  ìˆ˜ ìˆì§€ë§Œ ì´ì „ ì‘ì—…ë“¤ì—ì„œëŠ” ì œí•œëœ ìˆ˜ì˜ non-target classë§Œ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ í™˜ê²½ì„ ì¶©ë¶„íˆ ë°˜ì˜í•˜ì§€ ëª»í•˜ì˜€ìŒ (ì „í†µì ì¸ ë°©ë²•ì€ í›„ì²˜ë¦¬ë¥¼ í†µí•´ FAì„ ì¤„ì´ë ¤í–ˆì§€ë§Œ deep learning ì ‘ê·¼ë²•ê³¼ í•¨ê»˜ ì‚¬ìš©ë˜ì§€ ì•Šì•˜ìŒ)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ì‹¤ì œ keyword spottingì€ ì‚¬ì „ì— ì •ì˜ëœ keywordê°€ ì•Œë ¤ì§€ì§€ ì•Šì€ ë‹¤ì–‘í•œ ì†Œë¦¬ì—ì„œ ë°œê²¬ë˜ëŠ” classification ë¬¸ì œê°€ ì•„ë‹Œ detection ë¬¸ì œì™€ ìœ ì‚¬í•˜ì§€ë§Œ ì´ì „ì˜ ë§ì€ ì‘ì—…ì—ì„œëŠ” non-target soundë¥¼ ë‹¨ì¼ classë¡œ ê°„ì£¼í•˜ì˜€ìŒ&lt;/li&gt;
  &lt;li&gt;ë³¸ ë…¼ë¬¸ì—ì„œëŠ” &lt;strong&gt;target ë°œí™”ë¥¼ acceptí•˜ê±°ë‚˜ reject&lt;/strong&gt;í•˜ëŠ”ë° ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” &lt;strong&gt;discriminativeí•œ embeddingì„ í•™ìŠµ&lt;/strong&gt;í•˜ëŠ” &lt;strong&gt;metric learning&lt;/strong&gt;ì—ì„œ ì˜ê°ì„ ë°›ì•˜ìœ¼ë©° í™”ìê²€ì¦ê³¼ ìœ ì‚¬í•˜ì§€ë§Œ keywordê°€ ë¯¸ë¦¬ ì •ì˜ë˜ì–´ ìˆë‹¤ëŠ” ì ì´ ë‹¤ë¦„&lt;/li&gt;
  &lt;li&gt;metric learning ë°©ë²•ì€ input signalì„ embedding ê³µê°„ì— mappingí•˜ì—¬ class ê°„ ë¶„ì‚°ì„ í¬ê²Œí•˜ê³ , class ë‚´ì˜ ë¶„ì‚°ì€ ì‘ê²Œ í•¨ (â€œcontrastive lossâ€ - face recognition, speaker verificationì—ì„œ ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ìµœê·¼ metric learning ê¸°ìˆ ì€ constrastive, triplet lossì˜ ë‹¨ì (pair ì„ íƒì˜ ì–´ë ¤ì›€)ì„ ê·¹ë³µí•˜ê¸° ìœ„í•´ ë„ì…&lt;/li&gt;
  &lt;li&gt;[17, 18]ë…¼ë¬¸ì—ì„œ í›ˆë ¨ ì¤‘ ì—¬ëŸ¬ê°œì˜ positiveì™€ negativeë¥¼ ì‚¬ìš©í•´ careful pair ì„ íƒì´ í•„ìš”í•˜ì§€ ì•Šì€ í›ˆë ¨ ë°©ë²•ì„ ì œì•ˆí•˜ì˜€ìœ¼ë©°, Siamese neural network(dynamic time wrapingê¸°ë°˜ speech recognition)ì— ì‚¬ìš©ë˜ëŠ” frame-wise embedingì„ í›ˆë ¨í•˜ëŠ” í™”ì ê²€ì¦ì—ì„œ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì„&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ” metric learningì—ì„œ ì˜ê°ì„ ë°›ì€ keyword spottingì„ ìœ„í•œ ì—¬ëŸ¬ ë°©ë²•ì„ ì œì•ˆ&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Network architecture ë¥¼ ìœ ì§€í•˜ë©´ì„œ loss functionsì„ classificationì—ì„œ ë‹¤ì–‘í•œ metric learningìœ¼ë¡œ ë³€ê²½&lt;/li&gt;
  &lt;li&gt;target class ë‚´ ê±°ë¦¬ë¥¼ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ embeddingì„ í›ˆë ¨, non-target embeddingê³¼ ê±°ë¦¬ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ(ì‹¤ì œ keyword spottingì—ì„œëŠ” ì´ ë¶€ë¶„ì€ ë‹¤ë£¨ì§€ ì•Šê¸° ë•Œë¬¸)&lt;/li&gt;
  &lt;li&gt;ì ì¬ì ìœ¼ë¡œ ë¬´í•œí•œ non-target soundì™€ ìœ ì‚¬ì„±ì„ ë¹„êµí•˜ì—¬ ì‚¬ìš©í•˜ëŠ” classificationê³¼ ëŒ€ì¡°ì ì¸ ë°©ë²•&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ” ê¸°ì—¬í•œ ë°”&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Google Speech Command Datasetì‚¬ìš©, ì œì•ˆí•˜ëŠ” ë°©ë²•ì´ classification taskì— ëŒ€í•´ ì •í™•ì„±ì€ ìœ ì§€í•˜ë©´ì„œ detectionì— ëŒ€í•´ classification ê¸°ë°˜ baseline systemë“¤ ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì„&lt;/li&gt;
  &lt;li&gt;1) keyword spottingì´ ì´ì „ taskì™€ ë‹¬ë¦¬ detectionì˜ ë¬¸ì œ ì¤‘ í•˜ë‚˜ë¡œ ì •ì˜&lt;/li&gt;
  &lt;li&gt;2) non-target ketwordì˜ ì •í™•ë„ë¥¼ í¬ê²Œ ë†’ì¼ ìˆ˜ ìˆëŠ” mectirc learning ê¸°ë°˜ ë°©ë²• ì œì•ˆ&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;â…±-metric-learning-framework&quot;&gt;&lt;strong&gt;â…¡. Metric Learning Framework&lt;/strong&gt;&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;metric learningì— ì‚¬ìš©ë˜ëŠ” ê¸°ì¡´ loss fuctionì— ëŒ€í•´ ì„¤ëª…í•˜ê³ , ì „ì²´ì ì¸ classification ì •í™•ë„ë¥¼ ìœ ì§€í•˜ë©´ì„œ non-targetì˜ ì •í™•ë„ë„ ë†’ì´ê¸° ìœ„í•œ ìˆ˜ì •ëœ ë°©ë²•ì„ ì œì•ˆ&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;21-loss-functions&quot;&gt;&lt;strong&gt;2.1 Loss functions&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Triplet loss&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ê°€ì¥ ì¼ë°˜ì ì¸ ranking loss fuction ì¤‘ í•˜ë‚˜&lt;/li&gt;
  &lt;li&gt;ë™ì¼í•œ classì˜ embedding ì‚¬ì´ ê±°ë¦¬ê°€ ì¤„ì–´ë“¤ê³ , ë™ì‹œì— ë‹¤ë¥¸ classì˜ embeddingê³¼ëŠ” ê±°ë¦¬ê°€ ë©€ì–´ì§€ê²Œ í•™ìŠµë¨&lt;/li&gt;
  &lt;li&gt;$f(x;w) âˆˆ R^D$ : inputì„ embedding ê³µê°„ìœ¼ë¡œ mappingí•˜ëŠ” í•¨ìˆ˜ë¼ê³  ê°€ì •&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/95653158-5a3a3800-0b31-11eb-94ce-6f077868a0f7.png&quot; alt=&quot;img&quot; style=&quot;zoom: 50%;&quot; /&gt;&lt;/center&gt;

&lt;blockquote&gt;
  &lt;p&gt;$x_i, {xâ€™}_i$ : ê°™ì€ class $i$ìœ¼ë¡œë¶€í„° ì–»ì€ input samples&lt;br /&gt;
$x_j$ : ë‹¤ë¥¸ class $j(j{\neq}i)$ë¡œë¶€í„° ì–»ì€ sample&lt;br /&gt;
$|{x-y}|$ : $x$ì™€ $y$ê°„ pairwise-distance&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;triplet $P_{i,j} = (x_i, {xâ€™}_i, x_j)$ì¼ë•Œ, triplet loss Lì€ batchì— ëŒ€í•´ minimizedë˜ì–´ í›ˆë ¨
    &lt;blockquote&gt;
      &lt;p&gt;ì—¬ê¸°ì„œ $\alpha$ëŠ” constant margin (e.g. $\alpha=1$)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;$|f(x_i)-f({xâ€™}_j)| &amp;lt; |f(x_i)-f(x_j)| + \alpha$; â€œê°™ì€ classì—ì„œ ë‚˜ì˜¨ sampleë“¤ì˜ ê±°ë¦¬ê°€ ë‹¤ë¥¸ classì˜ sampleë³´ë‹¤ ê°€ê¹Œìš¸ ê²ƒì´ë‹¤.â€ ì—ì„œ ë°œì „ëœ loss&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Prototypical networks&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;open classificationì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ mectirc spaceë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•´ ì œì•ˆ&lt;/li&gt;
  &lt;li&gt;ê° classì˜ prototype representations(embedding) ê°„ì˜ distanceë¥¼ ê³„ì‚°&lt;/li&gt;
  &lt;li&gt;GE2E lossì˜ distance metricì„ ë³€í˜•í•œ prototypical lossì˜ angular ë³€í˜•ì„ ì‹¤í—˜ì— ì‚¬ìš©&lt;/li&gt;
  &lt;li&gt;ê° mini-batchëŠ” ì„œë¡œ ë‹¤ë¥¸ class Nê°œë‹¹ Mê°œì˜ ë°œí™”ê°€ ìˆëŠ” NxMì„ input featureë¡œ ì‚¬ìš©&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/95653587-b0f54100-0b34-11eb-81a3-9df0ff078997.png&quot; alt=&quot;img&quot; style=&quot;zoom: 50%;&quot; /&gt;&lt;/center&gt;

&lt;blockquote&gt;
  &lt;p&gt;$e_{j,M}$ : ê° batchì—ì„œ class $j$ì˜ query(embedding)&lt;br /&gt;
$c_k$ : class kì˜ centroid ($S$ì—ì„œ targetì´ ë˜ëŠ” utterance index(M)ëŠ” ì œì™¸í•œ embeddingì˜ í‰ê· )&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;cosineì„ ê¸°ë°˜ì˜ Similarity metricì„ ì‚¬ìš©í•˜ëŠ” angular prototypical lossëŠ” L2 distanceë³´ë‹¤ stable(ì•ˆì •ì )í•˜ê³ , robust(ê°•ì¸í•¨)&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;learnable parameter $w &amp;gt; 0$ì™€ $b$ë¥¼ ì‚¬ìš©&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;ê° batchì—ì„œ angular prototypical lossì˜ ëª©ì ì€ í•´ë‹¹ embeddingê³¼ ê°™ì€ classì˜ centroidì™€ ìœ ì‚¬ì„±ì€ ìµœëŒ€í™”í•˜ë©´ì„œ ë‹¤ë¥¸ classì˜ centroidì™€ëŠ” ìµœì†Œí™”í•˜ëŠ” ê²ƒì´ë¯€ë¡œ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•˜ì—¬ ì‚¬ìš©&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/95653779-3a594300-0b36-11eb-93b0-d7202056d818.png&quot; alt=&quot;img&quot; style=&quot;zoom: 50%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;22-pair-selection-strategy&quot;&gt;&lt;strong&gt;2.2 Pair selection strategy&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;2.1ì—ì„œ ì†Œê°œí•œ loss functionì„ ì‚¬ìš©í•˜ì—¬ networkë¥¼ í›ˆë ¨ì‹œí‚¤ëŠ” ë°©ë²• ì†Œê°œ&lt;/li&gt;
  &lt;li&gt;â€˜targetâ€™ keywordì™€ unknown â€˜non-targetâ€™ soundì„ íš¨ê³¼ì ìœ¼ë¡œ êµ¬ë³„í•˜ê¸°ìœ„í•´ positive, negative pairë¥¼ ì„ íƒí•˜ëŠ” ë°©ë²•ì„ ì£¼ë¡œ ë‹¤ë£¸&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;mectirc learning with an unknown cluster&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;2.1ì˜ baseline metric learning ì ‘ê·¼ë²•ì„ ì‚¬ìš©í•˜ëŠ” ë°©ì‹&lt;/li&gt;
  &lt;li&gt;ì´ ì ‘ê·¼ ë°©ë²•ì—ì„œëŠ” target keywordì™€ non-target keyword ëª¨ë‘ triplet ë˜ëŠ” prototypical lossë¥¼ ì‚¬ìš©í•˜ì—¬ embedding spaceì˜ anchor ë˜ëŠ” centroidë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê° classì— ë§ê²Œ clusterë¨
(2ê°œì˜ target/non-target classë¡œ sampleë“¤ì„ ë¶„ë¥˜í•¨)&lt;/li&gt;
  &lt;li&gt;target keywordì™€ non-target keywordë¥¼ ë‹¨ìˆœí•˜ê²Œ í•˜ë‚˜ì˜ classë¡œ ì·¨ê¸‰í•˜ì§€ë§Œ, non-target keywordì˜ varianceëŠ” ë§¤ìš° ë†’ì„ ìˆ˜ ë°–ì— ì—†ìŒ(targetì€ íŠ¹ì •ë˜ì—ˆëŠ”ë° non-targetì€ ë§¤ìš° ë‹¤ë¥¸ soundë“¤ì˜ ì§‘í•©ì´ê¸° ë•Œë¬¸ì—)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ë³¸ ë…¼ë¬¸ì—ì„œëŠ” classificationì„ ëª©í‘œë¡œ ë‘ê³  í›ˆë ¨í•˜ì§€ ì•Šê¸°ë•Œë¬¸ì— target/non-targetì¸ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ ì¶”ë¡  ë°©ë²•ì„ ì œì•ˆ&lt;/li&gt;
  &lt;li&gt;&lt;span style=&quot;background-color:#ffed54&quot;&gt;&lt;strong&gt;networkë¥¼ í›ˆë ¨ ì‹œí‚¨ í›„, ì „ì²´ training dataì˜ embeddingì„ ì¶”ì¶œí•˜ì—¬ í‰ê· ìœ¼ë¡œ centroidë¥¼ ê³„ì‚°&lt;/strong&gt;&lt;/span&gt;&lt;/li&gt;
  &lt;li&gt;&lt;span style=&quot;background-color:#ffed54&quot;&gt;&lt;strong&gt;test ë‹¨ê³„ì—ì„œ ë§ˆì°¬ê°€ì§€ë¡œ embeddingì„ ì¶”ì¶œí•˜ê³  ìœ„ì—ì„œ ê³„ì‚°ëœ centroidë“¤ê³¼ ìœ ì‚¬ì„±ì„ ê³„ì‚°í•˜ì—¬ ì–´ë–¤ classì— ì†í•˜ëŠ”ì§€ ê²°ì •&lt;/strong&gt;&lt;/span&gt;&lt;/li&gt;
  &lt;li&gt;ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œë„ targetì´ ì´ë¯¸ ì•Œë ¤ì ¸ ìˆìœ¼ë¯€ë¡œ ê° classì— í•´ë‹¹í•˜ëŠ” centroidë¥¼ ë¯¸ë¦¬ ê³„ì‚°í•˜ì—¬ ëª¨ë¸ì˜ parameterë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆì–´, inputì´ ì£¼ì–´ì¡Œì„ ë•Œ í›ˆë ¨ëœ ëª¨ë¸ì—ì„œ embeddingì„ ì–»ì€ ë’¤ ê° centroidì™€ ê±°ë¦¬ë¥¼ ê³„ì‚°í•´ ì–´ë–¤ classì— ì†í•˜ëŠ”ì§€ ë¶„ë¥˜í•  ìˆ˜ ìˆìŒ&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Metric learning without an unknown cluster&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;non-target keywordëŠ” target keywordë“¤ì„ ì œì™¸í•œ ëª¨ë“  ì†Œë¦¬ì™€ ìŒì„±ì„ í¬í•¨í•˜ê¸° ë•Œë¬¸ì— ë²”ìœ„ê°€ í›¨ì”¬ í¼&lt;/li&gt;
  &lt;li&gt;ê·¸ëŸ¬ë‚˜ ê¸°ì¡´ì˜ ì ‘ê·¼ ë°©ì‹ì—ì„œëŠ” non-target keywordë¥¼ í•˜ë‚˜ì˜ ë‹¨ì¼ classë¡œ ë‘ ë•Œë¬¸ì— ë‹¤ì–‘í•œ non-target embeddingë“¤ì´ ì´ ë‹¨ì¼ classì— ë§ë„ë¡ í›ˆë ¨ë¨ (varianceë¥¼ ê³ ë ¤í•˜ì§€ ì•ŠìŒ)&lt;/li&gt;
  &lt;li&gt;ì œí•œëœ non-target keywordë¡œ unseen wordë“¤ì„ ì¼ë°˜í™”í•˜ëŠ” ê²ƒì€ ì–´ë µê¸° ë•Œë¬¸ì—, &lt;strong&gt;í•™ìŠµ ì¤‘ non-target keywordë¥¼ í•˜ë‚˜ì˜ point(class)ë¡œ clusteringí•˜ì§€ ì•Šë„ë¡ ìˆ˜ì •ì„ ì œì•ˆ&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/95654278-f2d4b600-0b39-11eb-9905-5a6b7c6c65a8.png&quot; alt=&quot;img&quot; style=&quot;zoom: 50%;&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;non-targetì¸ì§€ êµ¬ë³„í•  ë•Œ, centroidë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì—(ëª¨ë“  sampleë“¤ì„ ì•Œ ìˆ˜ ì—†ì–´ì„œ) ì¶”ê°€ì ì¸ ë‹¨ê³„ê°€ í•„ìš”&lt;/li&gt;
  &lt;li&gt;metric learningìœ¼ë¡œ embedding extractorë¥¼ í›ˆë ¨í•œ ë’¤, training dataì˜ embeddingìœ¼ë¡œ 1ëŒ€ ë‚˜ë¨¸ì§€ë¥¼ êµ¬ë¶„í•  ìˆ˜ ìˆë„ë¡ RBF(Radial Basis Function) kernel SVMì„ í›ˆë ¨&lt;/li&gt;
  &lt;li&gt;ì´ SVMì„ ì‚¬ìš©í•˜ì—¬ test setì˜ classë¥¼ ê²°ì •&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;23-prototypical-networks-with-fixed-target-classes&quot;&gt;&lt;strong&gt;2.3 Prototypical networks with fixed target classes&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;target keyword spottingì„ ìœ„í•œ ìˆ˜ì •ëœ prototypical loss ì œì•ˆ&lt;/li&gt;
  &lt;li&gt;prototypical networkì˜ ì›ë˜ frameworkì—ì„œ centroidëŠ” few-shot learning settingì˜ inferenceë™ì•ˆ ê³„ì‚°ë¨&lt;/li&gt;
  &lt;li&gt;ê·¸ëŸ¬ë‚˜ ì–¼êµ´ ë° í™”ì ê²€ì¦ê³¼ ê°™ì€ prototypical networkì™€ ë‹¬ë¦¬ target keywordê°€ ê³ ì •ë˜ì–´ ìˆë‹¤ëŠ” ì‚¬ì‹¤ì„ í™œìš©í•  ìˆ˜ ìˆìŒ&lt;/li&gt;
  &lt;li&gt;ë”°ë¼ì„œ ì•Œë ¤ì§„ keywordì˜ ê²½ìš° ì¦‰ì„ì—ì„œ ê³„ì‚°ë˜ëŠ” ê° classì˜ ì¤‘ì‹¬ $c_k$ë¥¼ í•™ìŠµë˜ëŠ” classë³„ ê°€ì¤‘ì¹˜ $W_k$ë¡œ ëŒ€ì²´&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ì‹¤í—˜ì— ë”°ë¥´ë©´ classifierê¸°ë°˜ì˜ keyword spotting systemì€  mectirc learningê¸°ë°˜ systemë³´ë‹¤ target keywordì— ëŒ€í•´ ë” ë†’ì€ ì •í™•ë„ë¥¼ ê°–ëŠ” ë°˜ë©´, non-target keywordì— ëŒ€í•œ FARì´ ë” ë‚®ì•˜ìŒ&lt;/li&gt;
  &lt;li&gt;ì œì•ˆëœ ë°©ë²•ì€ í•™ìŠ¬ëœ classë³„ weightë¥¼ ì‚¬ìš©í•˜ì—¬ ì•Œë ¤ì§„ keywordë¥¼ ê°ì§€í•¨ìœ¼ë¡œì¨ ë‘ ë°©ë²•ì˜ ì¥ì ì„ í†µí•©í•˜ëŠ” ë™ì‹œ, mectirc learningê³¼ ìœ ì‚¬í•œ ë°©ì‹ìœ¼ë¡œ non-targetì„ rejectí•  ìˆ˜ ìˆìŒ&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;background-color:#ffed54&quot;&gt;&lt;strong&gt;AP-FC(Angular Prototypical with Fixed Classes); ì œì•ˆí•˜ëŠ” ë°©ë²•&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/95655613-9f676580-0b43-11eb-8598-233384fd1329.png&quot; alt=&quot;img&quot; style=&quot;zoom: 50%;&quot; /&gt;&lt;/center&gt;

&lt;blockquote&gt;
  &lt;p&gt;$S_{i,j,k}$ : class $j$ì˜ $i$ë²ˆì§¸ embeddingê³¼ centroidëŒ€ì‹  ì‚¬ìš©ë˜ëŠ” $k$ë²ˆì§¸ target keywordì˜ í›ˆë ¨ë˜ëŠ” parameter $W_k$ ì‚¬ì´ì˜ scaled cosine similarity&lt;br /&gt;
$W_k$ : target keywordì— ëŒ€í•´ í•™ìŠµë˜ëŠ” ìœ ì¼í•œ parameter&lt;br /&gt;
classifierì—ì„œ output layerì˜ ì—­í• ì„ í•¨($W_k$ì™€ ê³„ì‚°í•´ì„œ ë‚˜ì˜¨ ê²°ê³¼ê°€ class ë¶„ë¥˜ì— ì‚¬ìš©ë˜ë¯€ë¡œ)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/95655621-a7270a00-0b43-11eb-9ece-3afd4ec1d84f.png&quot; alt=&quot;img&quot; style=&quot;zoom: 50%;&quot; /&gt;&lt;/center&gt;

&lt;blockquote&gt;
  &lt;p&gt;$Nâ€™$ : í•˜ë‚˜ì˜ mini-batchì— í¬í•¨ëœ non-target keywordì˜ sample ìˆ˜&lt;br /&gt;
eq.8ì—ì„œ non-targetì˜ class indexëŠ” Nì´ë¼ê³  ê°€ì •&lt;br /&gt;
ì‹¤í—˜ì—ì„œ $Nâ€™$ì˜ ê°’ì„ 6ìœ¼ë¡œ ì„¤ì •&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;ëª¨ë“  $k âˆˆ {target}$ì— ëŒ€í•œ í•™ìŠµë˜ëŠ” parameter $W_k$ëŠ” ê° target keywordì˜ centroid ì—­í• ì„ í•˜ë„ë¡ í›ˆë ¨ë˜ì—ˆì„ ê²ƒì´ë¼ ê¸°ëŒ€&lt;/li&gt;
  &lt;li&gt;í•˜ë‚˜ì˜ mini-batchì—ëŠ” targetê³¼ non-targetì˜ ê· í˜•ì„ ì¡°ì •í•˜ê¸° ìœ„í•´ ê° target keywordì— ëŒ€í•œ í•˜ë‚˜ì˜ sampleê³¼ non-target keywordì˜ ì—¬ëŸ¬ sampleì„ í¬í•¨&lt;/li&gt;
  &lt;li&gt;Lì„ ìµœì†Œí™”í•˜ë©´ ë¶„ìì— ìˆëŠ” ê°’(kë²ˆì§¸ classì˜ embeddingê³¼ parameterì˜ ê±°ë¦¬)ì´ ì‘ì•„ì§€ë¯€ë¡œ í•´ë‹¹ classì— ì†í•œ embeddingê³¼ parameterê°€ ì ì  ê°€ê¹Œì›Œì§ˆ ê²ƒ&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 06 Oct 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2019-10-06-Metric-Laerning-for-Keyword-Spotting</link>
        <guid isPermaLink="true">http://localhost:4000/2019-10-06-Metric-Laerning-for-Keyword-Spotting</guid>
        
        
        <category>review</category>
        
      </item>
    
      <item>
        <title>Attention-based Models For Text-dependent Speaker Verification</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#-abstract&quot; id=&quot;markdown-toc--abstract&quot;&gt;ğŸ“Œ &lt;strong&gt;Abstract&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#â…°-introduction&quot; id=&quot;markdown-toc-â…°-introduction&quot;&gt;&lt;strong&gt;â… . Introduction&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#â…±-baseline-architecture&quot; id=&quot;markdown-toc-â…±-baseline-architecture&quot;&gt;&lt;strong&gt;â…¡. Baseline Architecture&lt;/strong&gt;&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#te2e-model&quot; id=&quot;markdown-toc-te2e-model&quot;&gt;&lt;strong&gt;TE2E model&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#â…²-attention-based-model&quot; id=&quot;markdown-toc-â…²-attention-based-model&quot;&gt;&lt;strong&gt;â…¢. Attention-based Model&lt;/strong&gt;&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#31-basic-attention-layer&quot; id=&quot;markdown-toc-31-basic-attention-layer&quot;&gt;&lt;strong&gt;3.1 Basic attention layer&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#32-scoring-functions&quot; id=&quot;markdown-toc-32-scoring-functions&quot;&gt;&lt;strong&gt;3.2 Scoring functions&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#33-attention-layer-variants&quot; id=&quot;markdown-toc-33-attention-layer-variants&quot;&gt;&lt;strong&gt;3.3 Attention layer variants&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#34-weights-pooling&quot; id=&quot;markdown-toc-34-weights-pooling&quot;&gt;&lt;strong&gt;3.4 Weights pooling&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#â…³-experiments&quot; id=&quot;markdown-toc-â…³-experiments&quot;&gt;&lt;strong&gt;â…£. Experiments&lt;/strong&gt;&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#41-datasets-and-basic-setup&quot; id=&quot;markdown-toc-41-datasets-and-basic-setup&quot;&gt;&lt;strong&gt;4.1 Datasets and basic setup&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#42-basic-attention-layer&quot; id=&quot;markdown-toc-42-basic-attention-layer&quot;&gt;&lt;strong&gt;4.2 Basic attention layer&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#43-variants&quot; id=&quot;markdown-toc-43-variants&quot;&gt;&lt;strong&gt;4.3 Variants&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#44-weights-pooling&quot; id=&quot;markdown-toc-44-weights-pooling&quot;&gt;&lt;strong&gt;4.4 Weights pooling&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#â…´--conclusion&quot; id=&quot;markdown-toc-â…´--conclusion&quot;&gt;&lt;strong&gt;â…¤.  Conclusion&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span style=&quot;font-size:13pt&quot;&gt;F A Rezaur Rahman Chowdhury, Quan Wang, Ignacio Lopez Moreno, Li Wan&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;-abstract&quot;&gt;ğŸ“Œ &lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Attention ê¸°ë°˜ ëª¨ë¸ : ì…ë ¥ sequenceì˜ ì „ì²´ ê¸¸ì´ë¥¼ ìš”ì•½í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥&lt;/li&gt;
  &lt;li&gt;ìŒì„± ì¸ì‹, ê¸°ê³„ ë²ˆì—­, ì´ë¯¸ì§€ ìº¡ì…˜ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ê³³ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì„&lt;/li&gt;
  &lt;li&gt;End-to-End Text-dependent í™”ì ì¸ì‹ ì‹œìŠ¤í…œì—ì„œ attention mechanism ì‚¬ìš©ì„ ë¶„ì„&lt;/li&gt;
  &lt;li&gt;ë‹¤ì–‘í•œ attention layerì˜ ë³€í˜•ì„ ì—°êµ¬í•˜ê³  attention weightì— ëŒ€í•œ ë‹¤ì–‘í•œ poolingë°©ë²•ì„ ë¹„êµ&lt;/li&gt;
  &lt;li&gt;Attention mechanismì„ ì‚¬ìš©í•˜ì§€ ì•Šì€ LSTMê³¼ ì„±ëŠ¥ ë¹„êµ&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;â…°-introduction&quot;&gt;&lt;strong&gt;â… . Introduction&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;âœ” Global Password Text-dependent Speaker Verification(SV) ì‹œìŠ¤í…œ&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ë“±ë¡ ë° í…ŒìŠ¤íŠ¸ ë°œí™”ê°€ íŠ¹ì • ë‹¨ì–´ë¡œ ì œí•œ (Text-dependent)&lt;/li&gt;
  &lt;li&gt;â€œOk-Googleâ€ê³¼ â€œHey Googleâ€ ì‚¬ìš© ( Global password)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ” í˜„ì¬ ê°€ì¥ ë§ì´ ì ‘ê·¼í•˜ê³  ìˆëŠ” í›ˆë ¨ ë°©ë²•&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ë“±ë¡ ë° í…ŒìŠ¤íŠ¸í•˜ëŠ” ë‹¨ê³„ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” End-to-End êµ¬ì¡°&lt;/li&gt;
  &lt;li&gt;[6]ë…¼ë¬¸ â€œi-vector+PLDA ì‹œìŠ¤í…œì„ ê·¸ëŒ€ë¡œ ëª¨ë°©í•œ êµ¬ì¡°â€ì˜ ê²½ìš°, ë” ë‚˜ì€ ì„±ëŠ¥ì„ ìœ„í•´ ëª¨ë¸ì„ ê·œì œí•˜ì˜€ìœ¼ë‚˜ ì´ˆê¸°í™”ë¥¼ ìœ„í•´ ê¸°ì¡´ì˜ i-vectorì™€ PLDA ëª¨ë¸ì´ í•„ìš”&lt;/li&gt;
  &lt;li&gt;[7] ë…¼ë¬¸, TD-SV taskì—ì„œ LSTM ë„¤íŠ¸ì›Œí¬ê°€ ê¸°ì¡´ End-to-End DNNë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œ&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ”ì´ì „ ë…¼ë¬¸ì—ì„œì˜ ë¬¸ì œì &lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ë¬µìŒê³¼ ë°°ê²½ ì¡ìŒì´ ë§ì´ ì—†ìŒ&lt;/li&gt;
  &lt;li&gt;ë³¸ ë…¼ë¬¸ì—ì„œëŠ” keyword ê²€ì¶œì— ì˜í•´ ë¶„í• ëœ 800msì˜ ì§§ì€ frameì´ì§€ë§Œ, ë¬µìŒê³¼ ì¡ìŒì´ ìˆìŒ&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ”ì´ìƒì ì¸ Embedding ìƒì„±&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ìŒì†Œì— í•´ë‹¹í•˜ëŠ” frameì„ ì‚¬ìš©í•˜ì—¬ ì œì‘&lt;/li&gt;
  &lt;li&gt;ì…ë ¥ sequence ì¤‘ ê´€ë ¨ì„±ì´ ë†’ì€ ìš”ì†Œë¥¼ ê°•ì¡°í•˜ê¸° ìœ„í•´ attention layer ì‚¬ìš©&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;â…±-baseline-architecture&quot;&gt;&lt;strong&gt;â…¡. Baseline Architecture&lt;/strong&gt;&lt;/h1&gt;

&lt;h3 id=&quot;te2e-model&quot;&gt;&lt;strong&gt;TE2E model&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;âœ”  baseline end-to-end training architecture&lt;/strong&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94424981-1573e000-01c6-11eb-8bf5-4890542a60db.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;í›ˆë ¨ ë‹¨ê³„ì—ì„œ, í•˜ë‚˜ì˜ í‰ê°€ìš© ë°œí™” ğ’™ğ‘—~ì™€ Nê°œì˜ ë“±ë¡ ë°œí™” ğ’™ğ‘˜ğ‘› (ğ‘“ğ‘œğ‘Ÿ ğ‘›=1, â€¦, ğ‘) tupleì´ LSTM networkì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;${x_{j\tilde{}}, (x_{k_1}, â€¦, x_{k_N})}$ ; input&lt;br /&gt;
$x$ : ê³ ì • ê¸¸ì´ì˜ log-mel fiterbank feature&lt;br /&gt;
$j, k$ : ë°œí™”í•œ í™”ì ($j$ì™€ $k$ëŠ” ê°™ì„ ìˆ˜ ìˆìŒ)&lt;br /&gt;
ë§Œì•½ $x_{j\tilde{}}$ì™€ $M$ ê°œì˜ ë“±ë¡ ë°œí™”ê°€ ê°™ì€ í™”ìë¼ë©´ tuple positive $(j=k)$, ë‹¤ë¥´ë©´ negative&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;â„ğ‘¡ : të²ˆì§¸ frameì—ì„œ LSTMì˜ ë§ˆì§€ë§‰ layerì˜ ì¶œë ¥ ( ê³ ì • ì°¨ì›ì˜ vector )&lt;/li&gt;
  &lt;li&gt;ë§ˆì§€ë§‰ frameì˜ outputì„ d-vector ğ (â„ğ‘‡) ë¡œ ì •ì˜&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;${\omega(j\tilde{}), (\omega(k_1), â€¦, \omega(k_N))}$ ; output&lt;br /&gt;
Tuple $(\omega(k_1), â€¦, \omega(k_N))$ì„ í‰ê· ë‚´ì–´ centroid ê³„ì‚°&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94425430-e447df80-01c6-11eb-9148-c79bd11b149b.png&quot; alt=&quot;img&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ”  Cosine Similarity Function ì •ì˜&lt;/strong&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94425434-e611a300-01c6-11eb-990c-2a6bc83ad06b.png&quot; alt=&quot;img&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ”  Loss Function ì •ì˜&lt;/strong&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94425445-e7db6680-01c6-11eb-9729-e41c138555a5.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;â…²-attention-based-model&quot;&gt;&lt;strong&gt;â…¢. Attention-based Model&lt;/strong&gt;&lt;/h1&gt;

&lt;h3 id=&quot;31-basic-attention-layer&quot;&gt;&lt;strong&gt;3.1 Basic attention layer&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;âœ”  Baseline systemê³¼ ì°¨ì´ì &lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ë§ˆì§€ë§‰ frameì˜ ì¶œë ¥ì„ d-vector(ğ)ë¡œ ì§ì ‘ ì‚¬ìš©&lt;/li&gt;
  &lt;li&gt;Attention layerëŠ” ê° t frame ì—ì„œì˜ LSTM ì¶œë ¥ â„ğ‘¡ì— ëŒ€í•œ ìŠ¤ì¹¼ë¼ ì ìˆ˜ ğ‘’ğ‘¡ ë¥¼ í›ˆë ¨í•˜ì—¬ weighted sumí•œ ê²°ê³¼ë¡œ d-vector(ğ) ì •ì˜&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94430186-76071b00-01ce-11eb-8ae9-0fdf5abcf182.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;Normalized weight ğ›¼ğ‘¡ì™€ weighted sumí•œ ê²°ê³¼ d-vectorëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94430336-ac449a80-01ce-11eb-8094-4fcf8644fec6.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/center&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94430342-ae0e5e00-01ce-11eb-9395-90efff7c8674.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;aritectureë¡œ ë³´ëŠ” ì°¨ì´ì &lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94430460-eca41880-01ce-11eb-9807-6a7dea6d97fa.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;32-scoring-functions&quot;&gt;&lt;strong&gt;3.2 Scoring functions&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Bias-only attention
ì—¬ê¸°ì„œ bğ‘¡ëŠ” scalar. LSTM ì¶œë ¥ hğ‘¡ì— ì˜ì¡´í•˜ì§€ ì•ŠìŒ.&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94430647-34c33b00-01cf-11eb-87f5-e43a51edc41a.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;Linear attention
ì—¬ê¸°ì„œ wğ‘¡ëŠ” mì°¨ì› vector, bğ‘¡ëŠ” scalar. frameë§ˆë‹¤ ë‹¤ë¥¸ parameterê°€ ì‚¬ìš©&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94430651-368cfe80-01cf-11eb-85a2-d759801a1634.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;Shared-parameter linear attention
ëª¨ë“  frameì— ëŒ€í•´ mì°¨ì› vector  wì™€ scalar bê°€ ë™ì¼í•˜ê²Œ ì‚¬ìš©&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94430653-37be2b80-01cf-11eb-95d0-af0d4afd142b.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;Non-linear attention
ì—¬ê¸°ì„œ ğ‘¾ğ’•ëŠ” mâ€™ X m matrix, ğ›ğ‘¡ì™€ ğ¯ğ‘¡ëŠ” mâ€™ì°¨ì›ì˜ vector(ì°¨ì› mâ€™ì€ í›ˆë ¨ ë°ì´í„° ì…‹ì—ì„œ ì¡°ì •)&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94430710-50c6dc80-01cf-11eb-8673-5af3e52f4b04.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;Shared-parameter non-linear attention
ëª¨ë“  í”„ë ˆì„ì— ëŒ€í•´ ë™ì¼í•œ parameter ğ–, ğ›, ğ¯ ë¥¼ ê³µìœ &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94430715-51f80980-01cf-11eb-9b90-9a302bca378a.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;33-attention-layer-variants&quot;&gt;&lt;strong&gt;3.3 Attention layer variants&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;ê¸°ë³¸ì ì¸ attention layerì™€ ë‹¬ë¦¬ ë‘ê°€ì§€ì˜ ë³€í˜•ëœ ê¸°ë²• Cross-layer attentionì™€ Divided-layer attention ì†Œê°œ&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;âœ” Cross-layer attention&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ê¸°ì¡´ì˜ ë°©ë²• : ë§ˆì§€ë§‰ LSTMì˜ layerì˜ ì¶œë ¥ hğ‘¡ (1â‰¤ğ‘¡â‰¤ğ‘‡)ë¥¼ ì‚¬ìš©í•˜ì—¬ score eğ‘¡ì™€ weight Î±ğ‘¡ë¥¼ ê³„ì‚°&lt;/li&gt;
  &lt;li&gt;ë³€í˜•ëœ ë°©ë²• : ì¤‘ê°„ LSTM layerì˜ ì¶œë ¥ hâ€™ğ‘¡(1â‰¤ğ‘¡â‰¤ğ‘‡)ìœ¼ë¡œ ê³„ì‚° (ê·¸ë¦¼ 3.(a) outputì—ì„œ ë§ˆì§€ë§‰ 2ë²ˆì§¸ layerë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°)&lt;/li&gt;
  &lt;li&gt;d-vector ğëŠ” ì—¬ì „íˆ ë§ˆì§€ë§‰ layer ì¶œë ¥ hğ‘¡ì™€ weighted sumìœ¼ë¡œ ê³„ì‚°&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94431728-9df77e00-01d0-11eb-83a4-7694a369266d.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ” Divided-layer attention&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ë§ˆì§€ë§‰ LSTM layerì˜ ì¶œë ¥ hğ‘¡ì˜ ì°¨ì›ì„ 2ë°°ë¡œ ëŠ˜ë¦¬ê³  ê·¸ ì°¨ì›ì„ part aì™€ part b ë‘ ë¶€ë¶„ìœ¼ë¡œ ê· ë“±í•˜ê²Œ ë‚˜ëˆ”&lt;/li&gt;
  &lt;li&gt;part bë¥¼ ì‚¬ìš©í•˜ì—¬ weightë¥¼ ê³„ì‚°í•˜ê³ , ë‚˜ë¨¸ì§€ part aì™€ weighted sumí•˜ì—¬ d-vector ìƒì„±&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94431901-e1ea8300-01d0-11eb-80d9-464a2cafaf02.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;34-weights-pooling&quot;&gt;&lt;strong&gt;3.4 Weights pooling&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;âœ” Basic attention layerì˜ ë˜ ë‹¤ë¥¸ ë³€í™”&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;LSTMì˜ output â„ë¥¼ averageí•˜ê¸° ìœ„í•´ normalized weight ğ›¼ğ‘¡ ë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì§€ ì•Šê³ , maxpoolingìœ¼ë¡œ ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;âœ” ë‘ ê°€ì§€ maxpooling ë°©ë²• ì‚¬ìš©&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Sliding Window maxpooling : Sliding windowì•ˆì˜ weight ì¤‘ í° ê°’ë§Œ ë‘ê³ , ë‚˜ë¨¸ì§€ëŠ” 0ìœ¼ë¡œ ë§Œë“¦&lt;/li&gt;
  &lt;li&gt;Global top-K maxpooling : ê°€ì¥ í° Kê°œì˜ ê°’ë§Œ ë‘ê³ , ë‚˜ë¨¸ì§€ëŠ” 0ìœ¼ë¡œ ë§Œë“¦&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94432216-63421580-01d1-11eb-8235-ee4f90a727af.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/center&gt;

&lt;blockquote&gt;
  &lt;p&gt;të²ˆì§¸ pixel : ê°€ì¤‘ì¹˜ $\alpha_t$&lt;br /&gt;
ë°ì„ ìˆ˜ë¡ ê°€ì¤‘ì¹˜ê°€ í° ê°’ì„ ì˜ë¯¸&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;â…³-experiments&quot;&gt;&lt;strong&gt;â…£. Experiments&lt;/strong&gt;&lt;/h1&gt;

&lt;h3 id=&quot;41-datasets-and-basic-setup&quot;&gt;&lt;strong&gt;4.1 Datasets and basic setup&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;âœ”  ì‚¬ìš©í•œ Dataset&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;â€œOk Googleâ€ê³¼ â€œHey Googleâ€ì´ í˜¼í•©ëœ ë°œí™” ë°ì´í„°&lt;/li&gt;
  &lt;li&gt;ì•½ 630K í™”ìê°€ 150M ë°œí™” (í…ŒìŠ¤íŠ¸ ë°ì´í„° : 665ëª… í™”ì)&lt;/li&gt;
  &lt;li&gt;í‰ê· ì ìœ¼ë¡œ enrollmentëŠ” 4.5ê°œ, evaluationì€ 10ê°œì˜ ë°œí™”ë¡œ êµ¬ì„±&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;âœ”  Basic setup&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ê¸°ë³¸ baselineì€ 3ê°œì˜ layerë¡œ ì´ë£¨ì–´ì§„ LSTM&lt;/li&gt;
  &lt;li&gt;ê° layerëŠ” 128ì°¨ì›ì´ë©°, 64ì°¨ì›ìœ¼ë¡œ projectioní•˜ëŠ” linear layerë¥¼ ê°€ì§€ê³  ìˆìŒ&lt;/li&gt;
  &lt;li&gt;Global passwordë§Œ í¬í•¨í•˜ëŠ” ê¸¸ì´ T=80 frame(800ms)ì˜ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë¶„ë¦¬í•˜ëŠ” keyword detection í›„ 40ì°¨ì›ì˜ log-mel-filterbank feature ìƒì„±&lt;/li&gt;
  &lt;li&gt;MultiReaderê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ë‘ ê°œì˜ keywordë¥¼ í˜¼í•©í•˜ì—¬ ì‚¬ìš©&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;42-basic-attention-layer&quot;&gt;&lt;strong&gt;4.2 Basic attention layer&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;ë‹¤ì–‘í•œ ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ Basic attention layerê³¼ ë¹„êµ&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94432416-b4eaa000-01d1-11eb-8141-99d48b30f3da.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;Bias-onlyì™€ linear attentionì€ EERì´ ê±°ì˜ ê°œì„ ë˜ì§€ ì•ŠìŒ&lt;/li&gt;
  &lt;li&gt;Non-linear ì¤‘ íŠ¹íˆ, shared-parameterì˜ ê²½ìš° ì„±ëŠ¥ í–¥ìƒì´ ìˆìŒ&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;43-variants&quot;&gt;&lt;strong&gt;4.3 Variants&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Basic attention layerì™€ ë‘ ê°€ì§€ ë³€í˜•(cross-layer, divided-layer) ë¹„êµ&lt;/li&gt;
  &lt;li&gt;ì´ì „ ì‹¤í—˜ì—ì„œ ìµœê³ ì˜ ì„±ëŠ¥ì„ ë‚¸ shared-parameter non-linear scoring functionì„ ì‚¬ìš©&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94432517-d8ade600-01d1-11eb-8325-50d593324e2b.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;cross-layerëŠ” ë§ˆì§€ë§‰ì—ì„œ 2ë²ˆì§¸ layerì—ì„œ scoreë¥¼ í›ˆë ¨&lt;/li&gt;
  &lt;li&gt;divided-layer attentionì´ ë§ˆì§€ë§‰ LSTM layerì˜ ì°¨ì›ì´ 2ë°°ì´ì§€ë§Œ, Basic attentionê³¼ cross-layer attentionë³´ë‹¤ ì•½ê°„ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì„&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;44-weights-pooling&quot;&gt;&lt;strong&gt;4.4 Weights pooling&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Attention weightë¥¼ ë‹¤ì–‘í•œ poolingë°©ë²•ìœ¼ë¡œ ì‚¬ìš©í•œ ê²ƒê³¼ ë¹„êµ&lt;/li&gt;
  &lt;li&gt;Shared-parameter non-linear scoring functionê³¼ divided-layer attention ì‚¬ìš©&lt;/li&gt;
  &lt;li&gt;Sliding window maxpooling : 10 frame window sizeì™€ 5 frame step size&lt;/li&gt;
  &lt;li&gt;Global top-K maxpooling : K = 5&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94432565-ea8f8900-01d1-11eb-856a-11c004b078e2.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;Sliding window maxpoolingì´ EERì´ ì•½ê°„ ë” ë‚®ì€ ê²ƒì„ í™•ì¸&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;âœ” ê° ë°©ë²•ì—ì„œ attention weightë¥¼ visualization&lt;/strong&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94433266-03e50500-01d3-11eb-8044-2e31658644e1.png&quot; alt=&quot;img&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;Poolingì´ ì—†ì„ ë•Œ, 4ìŒì†Œ(O-kay-Goo-gle) ë˜ëŠ” 3ìŒì†Œ(Hey-Goo-gle) íŒ¨í„´ì„ í™•ì¸&lt;/li&gt;
  &lt;li&gt;Poolingì„ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ ì‹œì‘ë¶€ë¶„ ë³´ë‹¤ëŠ” ëë¶€ë¶„ì˜ ë°œí™”ê°€ ë” í° attention weightë¥¼ ê°€ì§&lt;/li&gt;
  &lt;li&gt;LSTMì€ ì´ì „ ìƒíƒœ ê°’ì„ ëˆ„ì í•˜ì—¬ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì— ë§ˆì§€ë§‰ìœ¼ë¡œ ê°ˆìˆ˜ë¡ ë” ë§ì€ ì •ë³´ë¥¼ ê°€ì§ìœ¼ë¡œì¨ ë‚˜ì˜¤ê²Œ ë˜ëŠ” í˜„ìƒìœ¼ë¡œ íŒë‹¨&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;â…´--conclusion&quot;&gt;&lt;strong&gt;â…¤.  Conclusion&lt;/strong&gt;&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ë³¸ ë…¼ë¬¸ì—ì„œëŠ” keyword ê¸°ë°˜ì˜ Text-dependent í™”ì ê²€ì¦ ì‹œìŠ¤í…œì„ ìœ„í•œ ë‹¤ì–‘í•œ Attention mechanismì„ ì‹¤í—˜&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;ê°€ì¥ ì¢‹ì€ ë°©ë²•
    &lt;ol&gt;
      &lt;li&gt;shared-parameter non-linear scoring function ì‚¬ìš©&lt;/li&gt;
      &lt;li&gt;LSTMì˜ ë§ˆì§€ë§‰ layerì— divided-layer attention ì‚¬ìš©&lt;/li&gt;
      &lt;li&gt;Sliding window maxpoolingì„ attention weightì— ì ìš©&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ìœ„ì˜ 3ê°€ì§€ë¥¼ ê²°í•©í•˜ì˜€ì„ ë•Œ ê¸°ë³¸ LSTMëª¨ë¸ EER 1.72%ì—ì„œ 14%ì˜ ìƒëŒ€ì  ì„±ëŠ¥ í–¥ìƒì„ ê°€ì ¸ì˜´&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;span style=&quot;color:#FF0000&quot;&gt;&lt;strong&gt;ë™ì¼í•œ attention mechanism(íŠ¹íˆ, shared-parameter scoring function)ì€ Text-independentí•œ í™”ì ê²€ì¦ ë° í™”ì ì‹ë³„ì„ ê°œì„ í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë  ìˆ˜ ìˆìŒ&lt;/strong&gt;&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 30 Jul 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2019-07-30-Attention-based-models-for-TDSV</link>
        <guid isPermaLink="true">http://localhost:4000/2019-07-30-Attention-based-models-for-TDSV</guid>
        
        
        <category>review</category>
        
      </item>
    
      <item>
        <title>Text-Independent Speaker Verification with Adversarial Learning on Short Utterances</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#-abstract&quot; id=&quot;markdown-toc--abstract&quot;&gt;ğŸ“Œ &lt;strong&gt;Abstract&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#â…°-introduction-&quot; id=&quot;markdown-toc-â…°-introduction-&quot;&gt;&lt;strong&gt;â… . Introduction&lt;/strong&gt; ğŸŒ±&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#â…±-related-work-&quot; id=&quot;markdown-toc-â…±-related-work-&quot;&gt;&lt;strong&gt;â…¡. Related Work&lt;/strong&gt; ğŸŒ¿&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#â…²-proposed-approach-&quot; id=&quot;markdown-toc-â…²-proposed-approach-&quot;&gt;&lt;strong&gt;â…¢. Proposed Approach&lt;/strong&gt; ğŸŒ³&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#â…³-experiments-and-results-&quot; id=&quot;markdown-toc-â…³-experiments-and-results-&quot;&gt;&lt;strong&gt;â…£. Experiments and Results&lt;/strong&gt; ğŸŒº&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#â…´-conclusion-&quot; id=&quot;markdown-toc-â…´-conclusion-&quot;&gt;&lt;strong&gt;â…¤. Conclusion&lt;/strong&gt; ğŸŒ&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span style=&quot;font-size:13pt&quot;&gt;Kai Liu, Huan Zhou&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;-abstract&quot;&gt;ğŸ“Œ &lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;ë¬¸ì œì :&lt;/strong&gt; Text-independent speaker verificationì€ ì§§ì€ ë°œí™” ì¡°ê±´ì—ì„œ ì‹¬ê°í•œ ì„±ëŠ¥ ì €í•˜ë¥¼ ê²ªìŒ
&lt;strong&gt;í•´ê²°ë°©ë²•:&lt;/strong&gt; short embeddingì„ enhanced embeddingì— ì§ì ‘ ë§¤í•‘í•˜ì—¬ íŒë³„ë ¥(discriminability)ì„ ë†’ì´ë„ë¡ adversarialí•˜ê²Œ í›ˆë ¨ëœ embedding model ì œì•ˆ&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;íŠ¹íˆ, loss criteria(ê¸°ì¤€)ì´ ë§ì€ &lt;span style=&quot;background-color:#AED6F1&quot;&gt;&lt;strong&gt;Wasserstein GAN&lt;/strong&gt;&lt;/span&gt; ì‚¬ìš©&lt;/li&gt;
  &lt;li&gt;ì—¬ëŸ¬ loss functionì€ ëšœë ·í•˜ê²Œ ìµœì í™”í•˜ë ¤ëŠ” ëª©í‘œë¥¼ ê°€ì§€ê³  ìˆìœ¼ë‚˜ ê·¸ ì¤‘ ì¼ë¶€ëŠ” í™”ì ê²€ì¦ ì—°êµ¬ì— ë„ì›€ì´ ë˜ì§€ ì•ŠìŒ&lt;/li&gt;
  &lt;li&gt;ëŒ€ë¶€ë¶„ì˜ ì´ì „ ì—°êµ¬ì™€ ë‹¬ë¦¬  &lt;span style=&quot;background-color:#AED6F1&quot;&gt;&lt;strong&gt;ì´ ì—°êµ¬ì˜ ì£¼ìš” ëª©í‘œ&lt;/strong&gt; ëŠ” &lt;strong&gt;ìˆ˜ë§ì€ ablation ì—°êµ¬&lt;/strong&gt; ë¡œ ë¶€í„° loss criteriaì˜ íš¨ê³¼ë¥¼ ê²€ì¦&lt;/span&gt;
ã€€â†’ ìœ„ì—ì„œ ë§í•˜ëŠ” SVì—ì„œ ë„ì›€ì´ ë˜ì§€ ì•ŠëŠ” lossë“¤ì„ ì œê±°í•˜ë©´ì„œ lossì— ë”°ë¥¸ ì˜í–¥ì„ ì¡°ì‚¬&lt;/li&gt;
  &lt;li&gt;VoxCeleb datasetì— ëŒ€í•œ ì‹¤í—˜ì—ì„œ ì¼ë¶€ criteriaëŠ” SV ì„±ëŠ¥ì— ì´ë¡œìš´ ë°˜ë©´ ì¼ë¶€ criteriaëŠ” ì‚¬ì†Œí•œ ì˜í–¥ì„ ë¯¸ì¹œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤Œ&lt;/li&gt;
  &lt;li&gt;ë§ˆì§€ë§‰ìœ¼ë¡œ, finetuningì—†ì´ ì‚¬ìš©í•œ Wasserstein GANì€ baselineì„ ë„˜ì–´ ì˜ë¯¸ ìˆëŠ” ì„±ëŠ¥ í–¥ìƒì„ ë‹¬ì„±í•˜ë©°, EERì—ì„œëŠ” 4%ì˜ ìƒëŒ€ì  ê°œì„ ê³¼ 2ì´ˆê°„ì˜ ì§§ì€ ë°œí™”ì˜ challengeí•œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œëŠ” 7%ì˜ minDCFë¥¼ ë‹¬ì„±&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;â…°-introduction-&quot;&gt;&lt;strong&gt;â… . Introduction&lt;/strong&gt; ğŸŒ±&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;TI-SV: ë“±ë¡ëœ í™”ìì™€ í…ŒìŠ¤íŠ¸ ìŒì„±(ë‚´ìš© ì œì•½ X)ì„ í†µí•´ í™”ìì˜ ì‹ ì›ì„ ê²€ì¦&lt;/li&gt;
  &lt;li&gt;ì¤‘ìš”í•œ ë‹¨ê³„: ì„ì˜ì˜ ì§€ì†ì‹œê°„ì„ ê°–ëŠ” ìŒì„±ì„ ê³ ì • ì°¨ì›ì˜ speaker representationìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” ê²ƒ (acoustic feature â†’ speaker feature)&lt;/li&gt;
  &lt;li&gt;Baseline System: GhostVLAD-aggregated embedding(G-vector); ê¸´ ë°œí™”, ì§§ì€ ë°œí™”ì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìœ¼ë©°, ì¡ìŒ í™˜ê²½ì—ì„œ x-vectorë³´ë‹¤ ì´ì ì´ ìˆì–´ SV ì‹œìŠ¤í…œì— ë” ìœ ë¦¬&lt;/li&gt;
  &lt;li&gt;NIST-SRE 2010 test setì—ì„œ &lt;strong&gt;full-durationì´ 5ì´ˆë¡œ ë‹¨ì¶•&lt;/strong&gt;ë˜ì—ˆì„ ë•Œ i-vector/PLDA system &lt;strong&gt;ì„±ëŠ¥ì´ 2.48%ì—ì„œ 24.78%&lt;/strong&gt; ë¡œ ê°ì†Œ, &lt;strong&gt;ìµœê·¼ ë”¥ëŸ¬ë‹ ê¸°ìˆ  ì‚¬ìš©í•˜ì—¬ ì´ë¥¼ ë³´ì™„í•˜ëŠ” ì—°êµ¬ê°€ ë§ì´ ì§„í–‰ ì¤‘&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;ë³¸ ë…¼ë¬¸ì—ì„œëŠ” Wasserstein GANì˜ adversarial í•™ìŠµì„ ì´ìš©í•˜ì—¬ í–¥ìƒëœ ì°¨ë³„ì„±ì„ ê°€ì§„ ìƒˆë¡œìš´ embeddingì„ ì œì•ˆ
(ê°™ì€ í™”ìì˜ ì§§ì€ ë°œí™”ì™€ ê¸´ ë°œí™”ì—ì„œ ì¶”ì¶œí•œ G-vectorë¥¼ í™œìš©í•˜ì—¬)&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;â…±-related-work-&quot;&gt;&lt;strong&gt;â…¡. Related Work&lt;/strong&gt; ğŸŒ¿&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;âœ” GAN ì´ë€&lt;/strong&gt;: ìƒì„±ì(Generator)ì™€ ì‹ë³„ì(Discriminator)ê°€ ì‹¸ìš°ë©´ì„œ í•™ìŠµí•˜ëŠ” ëª¨ë¸&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Generator : Discriminatorë¥¼ ì†ì´ë„ë¡ í•™ìŠµ&lt;/li&gt;
  &lt;li&gt;Discriminator : real sample ğ‘¦ì™€ noise ğœ‚ë¡œë¶€í„° ìƒì„±ëœ fake sample ğ‘”ì˜ ì°¨ì´ë¥¼ í•™ìŠµ&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;lt;/br&amp;gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ” Adversarial Learning&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;minmax loss functionì´ êµëŒ€ë¡œ ìµœì í™” ê³¼ì •ì„ ìˆ˜í–‰ (ë‘ ëª¨ë¸ì˜ lossê°€ ê°™ì•„ì§€ëŠ” ìƒíƒœê°€ ë  ë•Œê¹Œì§€)&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/101442311-735b3b80-395e-11eb-87da-130ab93a5834.png&quot; alt=&quot;img&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;Gradients diminishing, exploding ë¬¸ì œë¡œ í›ˆë ¨í•˜ê¸° ì–´ë ¤ìš´ë° ì´ë¥¼ Wasserstein GAN(WGAN)ì—ì„œ ìˆ˜í•™ì ìœ¼ë¡œ ë‹¤ë£¨ì—ˆìŒ&lt;/li&gt;
  &lt;li&gt;DiscriminatorëŠ” ì¢‹ì€ $ğ‘“_ğ‘¤$ë¥¼ ì°¾ë„ë¡ ì„¤ê³„ë˜ì—ˆìœ¼ë©°, ìƒˆë¡œìš´ loss functionì€ Wasserstein ê±°ë¦¬ë¥¼ ì¸¡ì •í•˜ë„ë¡ êµ¬ì„±&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/101442322-76eec280-395e-11eb-8f23-77c965f91d6a.png&quot; alt=&quot;img&quot; /&gt;&lt;/center&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;â…²-proposed-approach-&quot;&gt;&lt;strong&gt;â…¢. Proposed Approach&lt;/strong&gt; ğŸŒ³&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;ì œì•ˆí•˜ëŠ” ì „ê¸‰ ë°©ì‹ì€ ì•„ë˜ì˜ êµ¬ì¡°ì™€ ê°™ìŒ&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/101442513-ccc36a80-395e-11eb-923b-4ca1aa2ac183.png&quot; alt=&quot;img&quot; /&gt;&lt;/center&gt;

&lt;blockquote&gt;
  &lt;p&gt;$ğ‘¥, ğ‘¦$ : ê°™ì€ speakerì˜ ê°ê° ì§§ê³  ê¸´ ë°œí™”ì— í•´ë‹¹í•˜ëŠ” Dì°¨ì›ì˜ G-vector&lt;br /&gt;
$ğ‘§$ : speaker ID label&lt;br /&gt;
$ğº_ğ‘“$ : embedding generator&lt;br /&gt;
$ğº_ğ‘$ : speaker label predictor&lt;br /&gt;
$ğº_ğ‘‘$ : Distance calculator&lt;br /&gt;
$ğ·_ğ‘¤$ : Wasserstein discriminator&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ì œì•ˆëœ ë°©ë²•ì˜ &lt;strong&gt;í•µì‹¬ì ì¸ task&lt;/strong&gt;ëŠ” &lt;strong&gt;discriminabilityì´ í–¥ìƒëœ embeddingì„ í•™ìŠµ&lt;/strong&gt;í•˜ëŠ” ê²ƒ&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span style=&quot;background-color:#E4C4F0&quot;&gt;&lt;strong&gt;âœ” loss functions&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;WGAN loss&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/101443118-02b51e80-3960-11eb-86ce-40b44aed35fc.png&quot; alt=&quot;img&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Conditional WGAN loss&lt;/strong&gt;: GANì— Wasserstein ê±°ë¦¬ë¥¼ ì´ìš©í•œ ìƒˆë¡œìš´ loss function ì •ì˜&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;$ğ‘¥$ (ì§§ì€ ë°œí™” embedding)ì´ ì£¼ì–´ì¡Œì„ ë•Œ, $ğ·_ğ‘¤$ì™€ $ğº_ğ‘“$ ë¶„í¬ì˜ ì°¨ì´ ($ğ‘¥$ì™€ real sample, fake sampleì„ ì—°ê²°í•˜ì—¬ í•™ìŠµ)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/101443121-047ee200-3960-11eb-85e3-d6cdb120eb2f.png&quot; alt=&quot;img&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;âš¡ï¸ WGAGN loss / Conditional WGAN loss ì¤‘ í•˜ë‚˜ë§Œ ì‚¬ìš©í•˜ê³ , ê·¸ ì°¨ì´ë¥¼ ì„±ëŠ¥ í‰ê°€ ì‹¤ì‹œ&lt;/p&gt;

&lt;p&gt;&amp;lt;/br&amp;gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;FID loss&lt;/strong&gt;: FrÃ©chet Inception Distance&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Real sampleê³¼ fake sampleì˜ ë²¡í„° ì‚¬ì´ì˜ ê±°ë¦¬ ê³„ì‚°ì„ ìœ„í•œ metric&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/101443125-05b00f00-3960-11eb-83a1-b11abcfe6840.png&quot; alt=&quot;img&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;class loss&lt;/strong&gt;: Multi-class cross-entropy loss&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Speakerì— ë”°ë¥¸ embedding ì°¨ì´ë¥¼ ìœ„í•œ loss ì •ì˜&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/101443129-08126900-3960-11eb-98d6-16200989d2ff.png&quot; alt=&quot;img&quot; /&gt;&lt;/center&gt;

&lt;blockquote&gt;
  &lt;p&gt;$ğ‘$ : Batch size&lt;br /&gt;
$ğ‘$ : Class ìˆ˜&lt;br /&gt;
$ğ‘”_ğ‘–$ : ië²ˆì§¸ ìƒì„±ëœ embedding&lt;br /&gt;
$ğ‘§_ğ‘–$ : í•´ë‹¹ label index&lt;br /&gt;
$ğ‘Šâˆˆâ„œ^(ğ·âˆ—ğ‘), ğ‘âˆˆâ„œ^ğ‘$ : weight matrix, bias&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Triplet loss&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Class ë¶„ë¥˜ ì‹œ errorì— ëŒ€í•œ íŒ¨ë„í‹°&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/101443133-09dc2c80-3960-11eb-882f-caf3570671b9.png&quot; alt=&quot;img&quot; /&gt;&lt;/center&gt;

&lt;blockquote&gt;
  &lt;p&gt;$\Gamma$ : training setì—ì„œ ê°€ëŠ¥í•œ ëª¨ë“  embeddingì˜ triplet $\gamma=(ğ‘”_ğ‘, ğ‘”_ğ‘, ğ‘”_ğ‘›)$ì˜ set&lt;br /&gt;
$ğ‘”_ğ‘$ : anchor input&lt;br /&gt;
$ğ‘”_ğ‘$ : positive input&lt;br /&gt;
$ğ‘”_ğ‘›$ : negative input&lt;br /&gt;
$\Psiâˆˆâ„œ^+$ : positiveì™€ negative ì‚¬ì´ì˜ safety margin&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Center loss&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Class ë‚´ variation ìµœì†Œí™”&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/101443140-0c3e8680-3960-11eb-8896-37b5be81d367.png&quot; alt=&quot;img&quot; /&gt;&lt;/center&gt;

&lt;blockquote&gt;
  &lt;p&gt;$ğ‘_(ğ‘¦_ğ‘–)$ : deep featureì˜ ğ‘¦_ğ‘–ë²ˆì§¸ class center&lt;br /&gt;
$ğ‘¥_ğ‘–$ : $ğ‘¦_ğ‘–$ë²ˆì§¸ classì— ì†í•˜ëŠ” ğ‘–ë²ˆì§¸ deep feature&lt;br /&gt;
$ğ‘š$ : mini-batch size&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Cosine distance loss&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Generator modelë¡œ ì–»ì€ í–¥ìƒëœ embeddingê³¼ real sample(target) ì‚¬ì´ì˜ ìœ ì‚¬ë„ë¥¼ ê³ ë ¤&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/101443144-0ea0e080-3960-11eb-8437-04997a2f26bc.png&quot; alt=&quot;img&quot; /&gt;&lt;/center&gt;

&lt;blockquote&gt;
  &lt;p&gt;$\bar ğ‘’$: normalized embedding&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;:star: &lt;span style=&quot;background-color:#FFED81&quot;&gt;&lt;strong&gt;âœ” Generatorì™€ Discriminatorì˜ ìµœì¢… Loss&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$G_f$&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/101444427-d3ec7780-3962-11eb-8967-3f0ff2912fad.png&quot; alt=&quot;img&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;$D_w$&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/101444430-d5b63b00-3962-11eb-97a0-807326d8a4a4.png&quot; alt=&quot;img&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;WGAN í›ˆë ¨ í›„ generative model $ğº_ğ‘“$ ìœ ì§€&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Test ë‹¨ê³„ì—ì„œ ì§§ì€ ë°œí™” embedding $ğ‘¥$ë¥¼ $ğº_ğ‘“$ì— ë„£ì–´ enhanced embedding($g$)ë¥¼ ì–»ìŒ&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;â…³-experiments-and-results-&quot;&gt;&lt;strong&gt;â…£. Experiments and Results&lt;/strong&gt; ğŸŒº&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;âœ” Experimental setup&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Train:&lt;/strong&gt;  VoxCeleb2ì˜ subset (1,057ëª… í™”ìì˜ 164,716ê°œ ë°œí™”)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Test:&lt;/strong&gt;   VoxCeleb1ì˜ subset (40ëª… í™”ìì˜ 13,265ê°œ ë°œí™”)&lt;/li&gt;
  &lt;li&gt;ì§§ì€ ë°œí™”ë¥¼ ìœ„í•´ &lt;strong&gt;randomí•˜ê²Œ 2ì´ˆ ì˜ë¼ì„œ&lt;/strong&gt; ì‚¬ìš©&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;âœ” Baseline system&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;G-vector (VGG-Restnet34s)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;âœ” Hyper Parameter&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Learning rate 0.0001&lt;/li&gt;
  &lt;li&gt;Adam Optimizer&lt;/li&gt;
  &lt;li&gt;Weight clipping -0.01 ~ 0.01 threshold ($ğ·_ğ‘¤$)&lt;/li&gt;
  &lt;li&gt;Batch size 128&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;background-color:#AED6F1&quot;&gt;&lt;strong&gt;âœ” ë‹¤ì–‘í•œ loss functionì˜ ì˜í–¥ ì—°êµ¬&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/101445011-0d71b280-3964-11eb-8d77-389d6aa37ee3.png&quot; alt=&quot;img&quot; /&gt;&lt;/center&gt;
&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/101445016-0f3b7600-3964-11eb-94f9-767587338bcc.png&quot; alt=&quot;img&quot; /&gt;&lt;/center&gt;

&lt;center&gt; - FID lossì€ ê¸ì •ì ì¸ ì˜í–¥ (v1 vs v2) &lt;/center&gt;
&lt;center&gt; - Conditional WGANì´ WGANë³´ë‹¤ ë‚˜ìŒ (v3 vs v4) &lt;/center&gt;
&lt;center&gt; - Triplet lossë¥¼ ë„£ìœ¼ë©´ ì¡°ê¸ˆ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ë³´ì„ (v7 vs v2) &lt;/center&gt;
&lt;center&gt; - Triplet b(fake)ë³´ë‹¤ Triplet a(real, fake ëª¨ë‘)ê°€ í¬ê²Œ ì„±ëŠ¥ í–¥ìƒ (v3 vs v8) &lt;/center&gt;
&lt;center&gt; - SoftmaxëŠ” ê¸ì •ì ì¸ ì˜í–¥ (v3 vs v5) &lt;/center&gt;
&lt;center&gt; - Center lossì€ ë¶€ì •ì ì¸ ì˜í–¥ (v6 vs v7) &lt;/center&gt;
&lt;center&gt; - Cosine lossì€ ê¸ì •ì  ì˜í–¥ (v6 vs v8) &lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;ì¶”ê°€ì ì¸ training function&lt;/strong&gt;(softmax, cosine, triplet)ì´ ëª¨ë‘ &lt;strong&gt;í›ˆë ¨ì— ê¸ì •ì ì¸&lt;/strong&gt; ì˜í–¥ì„ ë¯¸ì¹¨&lt;/li&gt;
  &lt;li&gt;SVì‹œìŠ¤í…œì— FID, conditional WGANì€ ë§¤ìš° ìœ ìš©, ì¶”ê°€ ì¡°ì‚¬ ê°€ì¹˜ê°€ ìˆìŒ&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ” Baseline systemê³¼ ë¹„êµ&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ì‹¤í—˜ ì¤‘ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì•˜ë˜ v3 systemê³¼ G-vector baseline system ë¹„êµ
    &lt;ul&gt;
      &lt;li&gt;EERê³¼ minDCF&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/101445333-a6083280-3964-11eb-86af-900deb097f6e.png&quot; alt=&quot;img&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Baselineë³´ë‹¤ ì§§ì€ durationì— ëŒ€í•´ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì„
    &lt;ul&gt;
      &lt;li&gt;ìƒëŒ€ì ìœ¼ë¡œ EERì€ 4.2% ê°œì„ í•˜ì˜€ìœ¼ë©°, minDCFëŠ” 7.2% ê°œì„  â€“ 1ì´ˆ taskì—ì„œë„ ìƒëŒ€ì  EER 3.8% í–¥ìƒ&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ì‹œê°„ ì œì•½ìœ¼ë¡œ FID lossëŠ” ìµœì¢… systemì— ì¶”ê°€í•˜ì§€ ì•Šì•˜ìœ¼ë©° hyper-parameter, loss weight($\alpha, \beta, \gamma, \lambda, \epsilon$)ì™€ triplet margin $\Psi$ì— ëŒ€í•œ ë¯¸ì„¸ì¡°ì •ì´ ì—†ì—ˆìŒ
    &lt;ul&gt;
      &lt;li&gt;ì œì•ˆí•œ systemì˜ ê°œì„ ë  ì—¬ì§€ê°€ ë§ì´ ë‚¨ì•„ìˆìŒ&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;â…´-conclusion-&quot;&gt;&lt;strong&gt;â…¤. Conclusion&lt;/strong&gt; ğŸŒ&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;ë³¸ ë…¼ë¬¸ì—ì„œëŠ” &lt;strong&gt;WGANì„ ì ìš©&lt;/strong&gt; í•˜ì—¬ &lt;strong&gt;ë°œí™”ê°€ ì§§ì€&lt;/strong&gt; speaker verification applicationì˜ &lt;strong&gt;í–¥ìƒëœ embeddingì„ ì„±ê³µì ìœ¼ë¡œ í•™ìŠµ&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;ì œì•ˆëœ WGAN ê¸°ë°˜ ì»¤ë„ ì‹œìŠ¤í…œ ê·¸ë¦¬ê³  ê·¸ ìœ„ì—, GAN í›ˆë ¨ì—ì„œ &lt;strong&gt;ë§ì€ loss criteriaì˜ íš¨ê³¼ë¥¼ ê²€ì¦&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;ìµœì¢… ì œì•ˆ ì‹œìŠ¤í…œì€ ë„ì „ì ì¸ ì§§ì€ ìŠ¤í”¼ì»¤ ê²€ì¦ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ baseline systemì„ ëŠ¥ê°€&lt;/li&gt;
  &lt;li&gt;ì „ë°˜ì ìœ¼ë¡œ, ìƒë‹¹í•œ ì§„ë³´ì™€ ì—°êµ¬ê°€ ì§„ì „ë˜ëŠ” ì ì¬ì  ë°©í–¥ì„ ë³´ì—¬ì¤Œ&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 24 Jul 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2019-07-24-TISV-with-Adversarial-Learning-on-Short-Utterances</link>
        <guid isPermaLink="true">http://localhost:4000/2019-07-24-TISV-with-Adversarial-Learning-on-Short-Utterances</guid>
        
        
        <category>review</category>
        
      </item>
    
      <item>
        <title>Generalized End to End Loss For Speaker Verification</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#-abstract&quot; id=&quot;markdown-toc--abstract&quot;&gt;ğŸ“Œ &lt;strong&gt;Abstract&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#â…°-introduction&quot; id=&quot;markdown-toc-â…°-introduction&quot;&gt;&lt;strong&gt;â… . Introduction&lt;/strong&gt;&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#11--background&quot; id=&quot;markdown-toc-11--background&quot;&gt;&lt;strong&gt;1.1  Background&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#12-te2e&quot; id=&quot;markdown-toc-12-te2e&quot;&gt;&lt;strong&gt;1.2 TE2E&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#â…±-ge2e-model&quot; id=&quot;markdown-toc-â…±-ge2e-model&quot;&gt;&lt;strong&gt;â…¡. GE2E Model&lt;/strong&gt;&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#21-training-method&quot; id=&quot;markdown-toc-21-training-method&quot;&gt;&lt;strong&gt;2.1 Training Method&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#22-comparison-between-te2e-and-ge2e&quot; id=&quot;markdown-toc-22-comparison-between-te2e-and-ge2e&quot;&gt;&lt;strong&gt;2.2 Comparison between TE2E and GE2E&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#23-training-with-multireader&quot; id=&quot;markdown-toc-23-training-with-multireader&quot;&gt;&lt;strong&gt;2.3 Training with MultiReader&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#â…²-experiments&quot; id=&quot;markdown-toc-â…²-experiments&quot;&gt;&lt;strong&gt;â…¢. Experiments&lt;/strong&gt;&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#31-td-sv&quot; id=&quot;markdown-toc-31-td-sv&quot;&gt;&lt;strong&gt;3.1 TD-SV&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#32-ti-sv&quot; id=&quot;markdown-toc-32-ti-sv&quot;&gt;&lt;strong&gt;3.2 TI-SV&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#â…³--conclusion&quot; id=&quot;markdown-toc-â…³--conclusion&quot;&gt;&lt;strong&gt;â…£.  Conclusion&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span style=&quot;font-size:13pt&quot;&gt;Li Wan, Quan Wang, Alan Papir, Ignacio Lopez Moreno&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;-abstract&quot;&gt;ğŸ“Œ &lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;ìƒˆë¡œìš´ loss function(Generalized End-to-End, GE2E) ì œì•ˆ&lt;/li&gt;
  &lt;li&gt;ë³¸ ì €ìë“¤ì´ ì´ì „ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•˜ì˜€ë˜ Tuple-based End-to-End (TE2E) loss functionë³´ë‹¤ speaker verification ëª¨ë¸ì„ ë” íš¨ìœ¨ì ìœ¼ë¡œ í›ˆë ¨&lt;/li&gt;
  &lt;li&gt;EERì„ 10%ì´ìƒ ê°ì†Œ ì‹œí‚¤ë©´ì„œ, ë™ì‹œì— í›ˆë ¨ ì‹œê°„ì„ 60%ê¹Œì§€ ë‹¨ì¶•&lt;/li&gt;
  &lt;li&gt;ë˜í•œ ë‹¤ì¤‘ í‚¤ì›Œë“œ (â€œOK googleâ€, â€œHey googleâ€)ì™€ ì—¬ëŸ¬ ë°©ì–¸ì„ ì§€ì›í•˜ëŠ” Domain ì ì‘ì„ ìœ„í•œ MultiReader ê¸°ìˆ ì„ ì†Œê°œ
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;â…°-introduction&quot;&gt;&lt;strong&gt;â… . Introduction&lt;/strong&gt;&lt;/h1&gt;

&lt;h3 id=&quot;11--background&quot;&gt;&lt;strong&gt;1.1  Background&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;âœ” Speaker Verfication&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;í™”ìì˜ ì•Œë ¤ì§„ ë°œí™” (ë“±ë¡ ë°œí™”, Enrollment)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ë°œí™”ê°€ íŠ¹ì • í™”ìì— ì†í•˜ëŠ”ì§€ í™•ì¸&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ” TD-SV / TI-SV&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;ë“±ë¡ê³¼ ê²€ì¦ì— ì‚¬ìš©ë˜ëŠ” ë°œí™”ì˜ ì œí•œì— ë”°ë¼ ë‘ ê°€ì§€ ì¹´í…Œê³ ë¦¬ë¡œ ë‚˜ë‰¨&lt;/li&gt;
  &lt;li&gt;TD-SV : Text-Dependent Speaker Verification (ê°™ì€ ë‚´ìš©ì„ ë°œí™”)&lt;/li&gt;
  &lt;li&gt;TI-SV : Text-Independent Speaker Verification (ë‹¤ë¥¸ ë‚´ìš©ì„ ë°œí™”)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ” i-vector system&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;TD-SVì™€ TI-SVì—ì„œ ëª¨ë‘ íš¨ê³¼ì ì¸ ì ‘ê·¼ ë°©ì‹ìœ¼ë¡œ, ìµœê·¼ Nerual Networkë¥¼ ì´ìš©í•˜ì—¬ ì´ë¥¼ ëŒ€ì²´í•˜ëŠ”ë° ì§‘ì¤‘&lt;/li&gt;
  &lt;li&gt;Nerual Networkë¥¼ ì´ìš©í•´ ì¶”ì¶œí•œ vectorë¥¼ embedding vector(d-vector) ë¼ê³  í•˜ë©° i-vectorì™€ ìœ ì‚¬í•˜ê²Œ ê³ ì • ì°¨ì›ìœ¼ë¡œ í‘œí˜„ì´ ê°€ëŠ¥&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;12-te2e&quot;&gt;&lt;strong&gt;1.2 TE2E&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;âœ”  LSTM network&lt;/strong&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94104922-2a254080-fe73-11ea-9685-5a2244dce1c9.png&quot; alt=&quot;img&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;ê° í›ˆë ¨ ë‹¨ê³„ì—ì„œ, í•˜ë‚˜ì˜ í…ŒìŠ¤íŠ¸ìš© ë°œí™” ğ’™ğ‘—~ì™€ ë“±ë¡ ë°œí™” ğ’™ğ‘˜ğ‘š tupleì„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;$x$: ê³ ì • ê¸¸ì´ì˜ log melfiterbank&lt;br /&gt;
$j, k$: ë°œí™”í•œ í™”ì&lt;br /&gt;
($j$ ì™€ $k$ ëŠ” ê°™ì„ ìˆ˜ ìˆìŒë§Œì•½ $x_{ğ‘—\tilde{}}$ì™€ ğ‘€ê°œì˜ ë“±ë¡ ë°œí™”ê°€ ê°™ì€ í™”ìë¼ë©´ tuple positive ($ğ‘—=ğ‘˜$), ë‹¤ë¥´ë©´ negative&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ê° tupleì— ëŒ€í•´ LSTM outputì„ L2 ì •ê·œí™”
    &lt;blockquote&gt;
      &lt;p&gt;${ğ’†_ğ‘—~,(ğ’†{k_1},â€¦,ğ’†_{k_M})}$  - $e$ : &lt;em&gt;embedding&lt;/em&gt;&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Tupleì˜ centroidëŠ” Mê°œì˜ ë°œí™”ë¡œë¶€í„° ìƒì„±í•œ voiceprint&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94106000-770a1680-fe75-11ea-9da3-ec0fb39d3869.png&quot; alt=&quot;img&quot; style=&quot;zoom:60%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Cosine Simliarity Function&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94106049-91dc8b00-fe75-11ea-980c-5e7b6d001a22.png&quot; alt=&quot;img&quot; /&gt;&lt;/center&gt;
&lt;blockquote&gt;
  &lt;p&gt;ğ‘¤,ğ‘ ëŠ” í•™ìŠµë˜ëŠ” ë³€ìˆ˜&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;TE2E loss&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94106059-93a64e80-fe75-11ea-8b31-a8b2629cfc6f.png&quot; alt=&quot;img&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/center&gt;
&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94106117-ae78c300-fe75-11ea-8bfb-cff4fa7b2e8a.png&quot; alt=&quot;img&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/center&gt;

&lt;blockquote&gt;
  &lt;p&gt;$\sigma(ğ‘¥) = 1/(1+ğ‘’^{âˆ’ğ‘¥})$ : sigmoid function &lt;br /&gt;
$\delta(j, k) = 1~&lt;del&gt;(ğ‘—=ğ‘˜)&lt;/del&gt;or~~0~~~(ğ‘—â‰ ğ‘˜)$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;â…±-ge2e-model&quot;&gt;&lt;strong&gt;â…¡. GE2E Model&lt;/strong&gt;&lt;/h1&gt;

&lt;h3 id=&quot;21-training-method&quot;&gt;&lt;strong&gt;2.1 Training Method&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;âœ”  GE2E training&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Fig. 1ì— ë‚˜íƒ€ë‚œ ê²ƒê³¼ ê°™ì´ N( í™”ì ìˆ˜ ) X ë°œí™” ìˆ˜ batch í˜•íƒœë¡œ ë§ì€ ìˆ˜ì˜ ë°œí™”ë¥¼ í•œë²ˆì— ì²˜ë¦¬&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94107053-a588f100-fe77-11ea-9812-931dfe797405.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/center&gt;

&lt;blockquote&gt;
  &lt;p&gt;$x_{ji}$: í™”ì $j$ ì˜ $i$ ë²ˆì§¸ ë°œí™”ë¥¼ ì¶”ì¶œí•œ íŠ¹ì§• ë²¡í„°&lt;br /&gt;
$ğ‘“(ğ’™_{ji}; ğ’˜)$: LSTM ê³¼ linear layer ë¥¼ ê±°ì¹˜ê³  ë‚˜ì˜¨ ë§ˆì§€ë§‰ ì¶œë ¥&lt;br /&gt;
$e_{ji}$: L2 ì •ê·œí™” í›„ embedding ë²¡í„°&lt;/p&gt;
&lt;/blockquote&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94107148-d2d59f00-fe77-11ea-80f8-973a38b624f0.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94107170-dff28e00-fe77-11ea-87a5-be7ed870bfc8.png&quot; alt=&quot;img&quot; style=&quot;zoom:40%;&quot; /&gt;&lt;/center&gt;

&lt;blockquote&gt;
  &lt;p&gt;Embedding vector $e_{ji}$, ëª¨ë“  centroid $c_k$ë¡œê³„ì‚° $(1â‰¤ğ‘—,ğ‘˜â‰¤ğ‘,1â‰¤ğ‘–â‰¤ğ‘€)$&lt;br /&gt;
$ğ‘¤ &amp;gt; 0$ : cosine similarity ê°’ì´ í´ìˆ˜ë¡ similarity ë¥¼ í¬ê²Œ í•˜ê¸° ìœ„í•˜ì—¬ ì–‘ìˆ˜ë¡œ ì„¤ì •&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ”  TE2Eì™€ GE2Eì˜ ì°¨ì´ì &lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;TE2Eì˜ similarity (equation 2 ëŠ” embedding vector ğ’†ğ‘—~ì™€ í•˜ë‚˜ì˜ tuple centroid ğ’„ğ‘˜ì‚¬ì´ì˜ ìœ ì‚¬í•¨ì„ ê³„ì‚° (scalar)&lt;/li&gt;
  &lt;li&gt;GE2E (equation 5)ëŠ” ê° embedding vector ğ’†ğ‘—ğ‘–ì™€ ëª¨ë“  ì¤‘ì‹¬ ğ’„ğ‘˜ì˜ ìœ ì‚¬í•¨ì„ ê³„ì‚°í•˜ì—¬ í–‰ë ¬ë¡œ êµ¬ì¶•&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ”  ëª©ì &lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;í›ˆë ¨ë™ì•ˆ , ê° ë°œí™”ì˜ embedding ì´ ë³¸ì¸ ë°œí™”ì˜ centroid ì™€ëŠ” ìœ ì‚¬í•¨ê³¼ ë™ì‹œì— ë‹¤ë¥¸ í™”ìì˜ centroid ì™€ì˜ ê±°ë¦¬ëŠ” ë©€ê²Œ (fig 1. ì—ì„œ ìƒ‰ìƒì˜ ê°’ì€ í¬ê³  íšŒìƒ‰ì˜ ê°’ì€ ì‘ê²Œ&lt;/li&gt;
  &lt;li&gt;Fig. 2ì—ì„œ íŒŒë€ìƒ‰ embedding vector ê°€ ê·¸ í™”ìì˜ centroid(íŒŒë€ìƒ‰ ì‚¼ê°í˜•)ê³¼ ê±°ë¦¬ëŠ” ê°€ê¹Œìš°ë©° , ë‹¤ë¥¸ í™”ìì˜ centroid(ë¹¨ê°„ìƒ‰, ë³´ë¼ìƒ‰ ì‚¼ê°í˜•) ê³¼ëŠ” ê±°ë¦¬ê°€ ë©€ë„ë¡&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94107458-7921a480-fe78-11ea-886e-a847186d8e93.png&quot; alt=&quot;img&quot; style=&quot;zoom:60%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ”  Softmax - Similarity matrix&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$S_{ji,k}$ì— softmax ë¥¼ ì ìš©í•˜ì—¬ j ì™€ k ê°€ ê°™ì€ í™”ì ì¼ ê²½ìš°ëŠ” ì¶œë ¥ ê°’ì„ 1, ë‹¤ë¥¸ í™”ìì¼ ê²½ìš° 0 ì´ ë˜ë„ë¡ í•¨&lt;/li&gt;
  &lt;li&gt;ê° embedding vector ë¥¼ ê·¸ í™”ìì˜ centroid ì™€ëŠ” ê°€ê¹ê²Œ í•˜ê³  , ë‹¤ë¥¸ í™”ìì˜ centroid ë¡œ
ë¶€í„°ëŠ” ë©€ì–´ì§€ê²Œ í•¨&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94107622-d4539700-fe78-11ea-8d58-18bea7a203e9.png&quot; alt=&quot;img&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ”  Contrast - Similarity matrix&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Contrast lossëŠ” positive ìŒê³¼ ê°€ì¥ negative í•œ ìŒìœ¼ë¡œ ì •ì˜&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ëª¨ë“  ë°œí™”ì— ëŒ€í•´ ë‘ ê°€ì§€ êµ¬ì„±ìš”ì†Œê°€ loss ì— ì¶”ê°€&lt;/p&gt;

    &lt;p&gt;(1) embedding vectorì™€ ê·¸ í™”ìì˜ voiceprint ì‚¬ì´ì˜ positive ì¼ì¹˜&lt;br /&gt;
(2) ë‹¤ë¥¸ í™”ìë“¤ì˜ ë°œí™” ì¤‘ ê°€ì¥ ë†’ì€ ìœ ì‚¬ì„±ì„ ê°–ëŠ” voiceprint ì˜ negative ì¼ì¹˜&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94107626-d61d5a80-fe78-11ea-8640-b4e3f492149f.png&quot; alt=&quot;img&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ”  Softmax&amp;amp;Contrast - Similarity matrix&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;TI-SV ì˜ ê²½ìš° softmax loss ê°€ ì•½ê°„ ë” ë‚˜ì€ ì„±ëŠ¥ì„,, TD SV ì˜ ê²½ìš° contrast loss ê°€ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì—¬ ë‘ ê°€ì§€ GE2E loss ì˜ êµ¬í˜„ì´ ëª¨ë‘ ìœ ìš©í•¨ì„ ë°œê²¬&lt;/li&gt;
  &lt;li&gt;$e_ji$ì œê±° : í™”ìì˜ centroid ê³„ì‚°ì‹œ , í›ˆë ¨ì´ ì•ˆì •ë˜ê³  ì‚¬ì†Œí•œ ë¬¸ì œë¥¼ í”¼í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤Œ&lt;/li&gt;
  &lt;li&gt;j ì™€ k ê°€ ê°™ì€ í™”ìì¼ ê²½ìš°ëŠ” (1) ëŒ€ì‹  (8) ì„ ì‚¬ìš©í•˜ì—¬ centroid ê³„ì‚°&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94108065-95721100-fe79-11ea-90cf-b8fe4dd86f1c.png&quot; alt=&quot;img&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ”  Eq. 4, 6, 7, 9ë¥¼ í•©í•˜ì—¬ ë§Œë“  ìµœì¢… GE2E loss&lt;/strong&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94108143-bc304780-fe79-11ea-9c40-d584284261bd.png&quot; alt=&quot;img&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;22-comparison-between-te2e-and-ge2e&quot;&gt;&lt;strong&gt;2.2 Comparison between TE2E and GE2E&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;âœ”  ëª¨ë“  ì…ë ¥ ğ’™ğ‘—ğ‘–ì— ëŒ€í•´ TE2E loss ì—ì„œ ë°œìƒí•˜ëŠ” tuple ì˜ ìˆ˜&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;(1) positive tuples : í™”ì j ì—ì„œ ë¬´ì‘ìœ„ë¡œ P ë°œí™” ì„ íƒ&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94108310-09acb480-fe7a-11ea-9b77-bbc90e239545.png&quot; alt=&quot;img&quot; style=&quot;zoom:70%;&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;(2) negative tuples : í™”ì j ì™€ ë‹¤ë¥¸ í™”ì k ì˜ ë°œí™”ì—ì„œ ë¬´ì‘ìœ„ë¡œ P ë°œí™” ì„ íƒ&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94108326-1204ef80-fe7a-11ea-9252-2f0510d28f49.png&quot; alt=&quot;img&quot; style=&quot;zoom:70%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;strong&gt;â­ ì´ TE2E lossì˜ tuple ìˆ˜&lt;/strong&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94108404-3365db80-fe7a-11ea-82b4-2674279c20ba.png&quot; alt=&quot;img&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;strong&gt;âœ”  GE2E loss ì˜ tuple ìˆ˜&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ê° í™”ìì˜ ëª¨ë“  ë°œí™”ë¥¼ ì„ íƒí•˜ì—¬ centroid ë¡œ ê³„ì‚°í•˜ë¯€ë¡œ P=M&lt;/li&gt;
  &lt;li&gt;ë”°ë¼ì„œ TE2E ì˜ ìµœì†Œ ê°’ [2x(N-1)]ê³¼ ë™ì¼&lt;/li&gt;
  &lt;li&gt;&lt;span style=&quot;background-color:#f2cfa5&quot;&gt;&lt;strong&gt;GE2Eê°€ TE2E ë³´ë‹¤ ì§§ì€ ì‹œê°„ ë‚´ì— ë” ë‚˜ì€ ëª¨ë¸ë¡œ ìˆ˜ë ´&lt;/strong&gt;&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;23-training-with-multireader&quot;&gt;&lt;strong&gt;2.3 Training with MultiReader&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;âœ”  ì‘ì€ ë°ì´í„° ì…‹ D1ê³¼ í° ë°ì´í„° ì…‹ D2 ì¡´ì¬&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;D1 domain model ì— ê´€ì‹¬ì´ ìˆëŠ”ë° , ë™ì¼í•œ domain ì€ ì•„ë‹ˆì§€ë§Œ ë” í° D2 dataset ì´ ìˆì„ ë•Œ , &lt;strong&gt;D2 ì˜ ë„ì›€ì„ ë°›ì•„&lt;/strong&gt; dataset D1 ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ë‹¨ì¼ ëª¨ë¸ì„ êµìœ¡í•˜ê³ ì í•¨&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94109181-9310b680-fe7b-11ea-82df-c7db9d057167.png&quot; alt=&quot;img&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;Regularization ê¸°ë²•ê³¼ ìœ ì‚¬&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94109049-60ff5480-fe7b-11ea-9be6-929668d7d5b1.png&quot; alt=&quot;img&quot; style=&quot;zoom:60%;&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;D1ì— ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ì„ ë•Œ , D2 ì—ì„œë„ ì˜ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ í•¨ìœ¼ë¡œì¨ overfitting ì´ ë°œìƒí•˜ëŠ” ê²ƒì„ ë°©ì§€&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;âœ”  ì¼ë°˜í™”&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;K ê°œì˜ ë‹¤ë¥¸ data source : ğ·1,â€¦,ğ·ğ¾ë¥¼ ê²°í•©í•˜ê¸° ìœ„í•´ ì¼ë°˜í™”&lt;/li&gt;
  &lt;li&gt;ê° data source ì˜ ê°€ì¤‘ì¹˜ ğ›¼ğ‘˜ë¥¼ í• ë‹¹í•˜ì—¬ í•´ë‹¹ data source ì˜ ì¤‘ìš”ì„± í‘œì‹œ&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94109317-c2272800-fe7b-11ea-88d3-dff6b42d6d0a.png&quot; alt=&quot;img&quot; style=&quot;zoom:90%;&quot; /&gt;&lt;/center&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;â…²-experiments&quot;&gt;&lt;strong&gt;â…¢. Experiments&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;âœ”  ì°¸ì¡°ë…¼ë¬¸ [6]ê³¼ process ë™ì¼&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;100ê°œ frameì„ 30 frameì”© (10 frame ì˜†ìœ¼ë¡œ ì´ë™í•´ê°€ë©° ) ì‚¬ìš©&lt;/li&gt;
  &lt;li&gt;40-dimensional log mel filterbank&lt;/li&gt;
  &lt;li&gt;LSTM 3-layer + projection (d-vector size)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ”  Hyper Parameter&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;N : 64 (speakers), M : 10 (utterances)&lt;/li&gt;
  &lt;li&gt;Learning rate : 0.01 (30M ë‹¨ê³„ë§ˆë‹¤ ì ˆë°˜ì”© ê°ì†Œ&lt;/li&gt;
  &lt;li&gt;Optimizer : SGD&lt;/li&gt;
  &lt;li&gt;Loss functionì˜ ì¢‹ì€ ì´ˆê¸° ê°’ : (w, b) = (10, 5)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;31-td-sv&quot;&gt;&lt;strong&gt;3.1 TD-SV&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Keyword detection ê³¼ speaker verification ê°™ì€ íŠ¹ì§• ì‚¬ìš©&lt;/li&gt;
  &lt;li&gt;Keyword detection ì€ keyword ê°€ í¬í•¨ëœ frame ë§Œ SV system ìœ¼ë¡œ ì „ë‹¬&lt;/li&gt;
  &lt;li&gt;ì´ frame ì€ ê³ ì • ê¸¸ì´ segment í˜•ì„±&lt;/li&gt;
  &lt;li&gt;Hidden node : 128, Projection size : 64&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ” Multiple Keyword&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ì‚¬ìš©ìë“¤ì´ ë™ì‹œì— ì—¬ëŸ¬ ê°œì˜ í‚¤ì›Œë“œ ì§€ì›ì„ ë” ì„ í˜¸í•˜ì—¬, â€œOk googleâ€ê³¼ â€œHey googleâ€ ì§€ì›&lt;/li&gt;
  &lt;li&gt;í•˜ë‚˜ì˜ êµ¬ì ˆë¡œ ì œí•œë˜ê±°ë‚˜ ì™„ì íˆ ì œì•½ë˜ì§€ëŠ” ì•Šê¸° ë•Œë¬¸ì— ì—¬ëŸ¬ keywordë¡œ speaker verificationí•˜ëŠ” ê²ƒì€ TD-SVì™€ TI-SV ì‚¬ì´ì— ë†“ì„&lt;/li&gt;
  &lt;li&gt;ì—¬ëŸ¬ data sourceë¥¼ ì§ì ‘ í˜¼í•©í•˜ëŠ” ê²ƒê³¼ ê°™ì€ ë‹¨ìˆœí•œ ì ‘ê·¼ ë°©ì‹ì— ë¹„í•´ MultiReaderëŠ” data sourceì˜ í¬ê¸°ê°€ ë¶ˆê· í˜•í•œ ê²½ìš°ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë“± í° ì´ì ì„ ê°–ìŒ&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;~150M ë°œí™”, ~630K í™”ìë¡œ ì´ë£¨ì–´ì§„ â€œOk googleâ€ setê³¼ ~1.2M ë°œí™”ì™€ ~18Kë¡œ ì´ë£¨ì–´ì§„ â€œHey googleâ€ setì„ ë¹„êµí•˜ë©´ â€œOk googleâ€ì´ 125ë°° ë°œí™” ìˆ˜ê°€ ë” ë§ìœ¼ë©° 35ë°° í™”ì ìˆ˜ê°€ ë” ë§ìŒ&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ” í‰ê°€ ë°©ë²• ë° ê²°ê³¼&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;4ê°€ì§€ ê²½ìš°ì— ëŒ€í•´ EER ì¸¡ì • (2ê°œì˜ keywordë¡œ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ì¡°í•©)&lt;/li&gt;
  &lt;li&gt;í…ŒìŠ¤íŠ¸ dataset : 665 ëª…ì˜ í™”ì / í‰ê·  4.5 íšŒ ë“±ë¡ ë°œí™” , 10 ê°œì˜ í…ŒìŠ¤íŠ¸ ë°œí™”&lt;/li&gt;
  &lt;li&gt;MultiReaderë¥¼ ì ìš©í•œ ê²ƒì´ 4 ê°€ì§€ ê²½ìš° ëª¨ë‘ì—ì„œ ì•½ 30% ì˜ ìƒëŒ€ì  ì„±ëŠ¥ í–¥ìƒì„ ë³´ì„&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94112150-00264b00-fe80-11ea-9b21-fc9d738e90de.png&quot; alt=&quot;img&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ” ì¶”ê°€ ì‹¤í—˜&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;~83K ì„œë¡œ ë‹¤ë¥¸ í™”ìì™€ í™˜ê²½ ì¡°ê±´ì˜ ëŒ€ê·œëª¨ dataset (í‰ê·  7.3íšŒ ë“±ë¡, 5ê°œ í…ŒìŠ¤íŠ¸ ë°œí™” ì‚¬ìš©)&lt;/li&gt;
  &lt;li&gt;GE2E modelì€ TE2Eë³´ë‹¤ ì•½ 60% ë” ì ì€ í›ˆë ¨ ì‹œê°„ì„ ì†Œëª¨&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94112311-349a0700-fe80-11ea-8edf-f098ab62b9a8.png&quot; alt=&quot;img&quot; style=&quot;zoom:75%;&quot; /&gt;&lt;/center&gt;

&lt;blockquote&gt;
  &lt;p&gt;1 í–‰ : 512 ê°œì˜ hidden node ì™€ 128 ì°¨ì›ì˜ embedding vector í¬ê¸°ë¥¼ ê°€ì§„ ë‹¨ì¼ ê³„ì¸µ LSTM&lt;br /&gt;
2í–‰  : 3 layer LSTM (TE2E)&lt;br /&gt;
3 í–‰ : 3 layer LSTM (GE2E)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;32-ti-sv&quot;&gt;&lt;strong&gt;3.2 TI-SV&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Hidden node : 768, Projection size : 256&lt;/li&gt;
  &lt;li&gt;VAD(Voice Activity Detection) í›„ ê³ ì • ê¸¸ì´ segmentë¡œ ë‚˜ëˆ” ; partial utterances&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ” Train&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ê° ë°ì´í„° batch ì— ëŒ€í•´ ğ‘™ğ‘,ğ‘¢ğ‘] = [140, 180] frame ë‚´ì— ì„ì˜ë¡œ ì‹œê°„ ê¸¸ì´ t ì„ íƒ&lt;/li&gt;
  &lt;li&gt;í•´ë‹¹ batch ë‚´ ëª¨ë“  ë°œí™”ì˜ ê¸¸ì´ëŠ” tê°€ ë˜ì–´ ê³ ì • ê¸¸ì´ì˜ segmentë¥¼ ê°–ìŒ&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94110427-97d66a00-fe7d-11ea-94f6-7c3f2413e535.png&quot; alt=&quot;img&quot; style=&quot;zoom:80%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ” Test&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;window sizeë§Œí¼ ê³ ì • segmentë¥¼ ê°€ì ¸ì™€ d-vector ì¶”ì¶œ&lt;/li&gt;
  &lt;li&gt;window sizeë¥¼ 50%ë§Œí¼ ê²¹ì¹˜ê²Œ slidingí•˜ì—¬ ì´ë™&lt;/li&gt;
  &lt;li&gt;windowë§ˆë‹¤ ì¶”ì¶œëœ d-vectorë¥¼ L2 ì •ê·œí™”í•˜ê³  averageí•˜ì—¬ ìµœì¢… d-vectorë¥¼ ìƒì„±&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94110496-b2104800-fe7d-11ea-96b1-41d04121b01d.png&quot; alt=&quot;img&quot; style=&quot;zoom:60%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ” ì‹¤í—˜ ê²°ê³¼&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;í›ˆë ¨ dataset : 36M ë°œí™”ì™€ 18K í™”ìë¥¼ ì‚¬ìš©&lt;/li&gt;
  &lt;li&gt;í…ŒìŠ¤íŠ¸ dataset : 1000 ëª…ì˜ í™”ì , í‰ê·  6.3 ê°œ ë“±ë¡ ë°œí™” , í‰ê·  7.2 ê°œì˜ í…ŒìŠ¤íŠ¸ ë°œí™” ì‚¬ìš©&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/94113342-b8082800-fe81-11ea-9cde-7831829befa1.png&quot; alt=&quot;img&quot; style=&quot;zoom: 50%;&quot; /&gt;&lt;/center&gt;

&lt;blockquote&gt;
  &lt;p&gt;Softmax : í›ˆë ¨ ë°ì´í„°ì˜ ëª¨ë“  í™”ìì— ëŒ€í•œ label ì„ ì˜ˆì¸¡&lt;br /&gt;
TE2E : TE2E ë¡œ í›ˆë ¨ëœ ëª¨ë¸&lt;br /&gt;
GE2E : GE2E ë¡œ í›ˆë ¨ëœ ëª¨ë¸&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;â…³--conclusion&quot;&gt;&lt;strong&gt;â…£.  Conclusion&lt;/strong&gt;&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Speaker Verification ì„ ìœ„í•œ ë³´ë‹¤ íš¨ìœ¨ì ì¸ GE2E loss function ì œì•ˆ&lt;/li&gt;
  &lt;li&gt;ì´ë¡  ë° ì‹¤í—˜ì  ê²°ê³¼ ì—ì„œ ëª¨ë‘ ë³¸ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ ëª¨ë¸ì˜ ì¥ì  ì„ ì…ì¦&lt;/li&gt;
  &lt;li&gt;ë‹¤ì–‘í•œdata source ë¥¼ ê²°í•©í•˜ëŠ” MultiReader ê¸°ë²•ì„ ë„ì…í•˜ì—¬ ì—¬ëŸ¬ í‚¤ì›Œë“œì™€ ì–¸ì–´ë¥¼ ì§€ì› í•  ìˆ˜ ìˆë„ë¡ í•¨&lt;/li&gt;
  &lt;li&gt;&lt;span style=&quot;color:#FF0000&quot;&gt;&lt;strong&gt;ë‘ ê°€ì§€ ê¸°ë²•ì„ ê²°í•©í•˜ì—¬ ë³´ë‹¤ ì •í™•í•œ Speaker Verification Model êµ¬ì¶•&lt;/strong&gt;&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 10 Jul 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2019-07-10-GE2E-loss-for-SV</link>
        <guid isPermaLink="true">http://localhost:4000/2019-07-10-GE2E-loss-for-SV</guid>
        
        
        <category>review</category>
        
      </item>
    
      <item>
        <title>Generative Adversarial Speaker Embedding Networks for Domain Robust End-to-End Speaker Verification</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#-abstract&quot; id=&quot;markdown-toc--abstract&quot;&gt;ğŸ“Œ &lt;strong&gt;Abstract&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#â…°-introduction&quot; id=&quot;markdown-toc-â…°-introduction&quot;&gt;&lt;strong&gt;â… . Introduction&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#â…±-domain-adaption-with-gans&quot; id=&quot;markdown-toc-â…±-domain-adaption-with-gans&quot;&gt;&lt;strong&gt;â…¡. Domain Adaption with GANs&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#â…²-generative-adversarial-speaker-embedding-networks&quot; id=&quot;markdown-toc-â…²-generative-adversarial-speaker-embedding-networks&quot;&gt;&lt;strong&gt;â…¢. Generative Adversarial Speaker Embedding Networks&lt;/strong&gt;&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#31-auxiliary-classifier-gan&quot; id=&quot;markdown-toc-31-auxiliary-classifier-gan&quot;&gt;3.1. Auxiliary Classifier GAN&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#32-gan-variants&quot; id=&quot;markdown-toc-32-gan-variants&quot;&gt;3.2. GAN Variants&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#â…³--experiments-and-results&quot; id=&quot;markdown-toc-â…³--experiments-and-results&quot;&gt;&lt;strong&gt;â…£.  Experiments and Results&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#â…´--conclusion&quot; id=&quot;markdown-toc-â…´--conclusion&quot;&gt;&lt;strong&gt;â…¤.  Conclusion&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span style=&quot;font-size:13pt&quot;&gt;Gautam Bhattacharya, Joao Monteiro, Jahangir Alam, Patrick Kenny&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;-abstract&quot;&gt;ğŸ“Œ &lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;GANsë¥¼ ì´ìš©í•œ domain invariant speaker embeddingì„ ìœ„í•œ ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ì‹ ì œì•ˆ
  - source dataì™€ target dataë¡œ generatorê°€ embeddingì„ ìƒì„±
  - ìƒì„±ëœ embeddingì´ sourceì¸ì§€ targetì¸ì§€ discriminatorê°€ ì‹ë³„&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;ì´ëŸ¬í•œ frameworkë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ê°€ì§€ GAN ë³€í˜•ì„ í›ˆë ¨í•˜ê³  í™”ì ê²€ì¦ì— ì ìš©&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Angular Margin lossë¥¼ ì‚¬ìš©í•˜ì—¬ End-to-End model ìµœì í™”&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/92461324-1e474680-f204-11ea-91bc-e748da169035.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h1 id=&quot;â…°-introduction&quot;&gt;&lt;strong&gt;â… . Introduction&lt;/strong&gt;&lt;/h1&gt;
&lt;p&gt;- í™”ì embedding : ê°œì¸ì˜ identityì™€ ê´€ë ¨ëœ ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ” ì €ì°¨ì› ë²¡í„° í‘œí˜„&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ”  Neural Networkê¸°ë°˜ í™”ì embedding&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ìŒì„± ì¸ì‹, í•©ì„± ë° source ë¶„ë¦¬, í™”ì ê²€ì¦ ì ìš© ë“± ë‹¤ì–‘í•˜ê²Œ ì ìš©&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;âœ”  End-to-End system speaker verification&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ë‘ ê°œì˜ ìŒì„± íŒŒì¼ì—ì„œ embeddingì„ ì¶”ì¶œí•œ ë’¤ embedding ì‚¬ì´ì˜ cosine distance ë“±ì„ ì‚¬ìš©í•˜ì—¬ score ê³„ì‚°&lt;/li&gt;
  &lt;li&gt;ëª¨ë¸ì´ ê²¬ê³ í•˜ê¸° ìœ„í•´ì„œ ì¼ë°˜ì ìœ¼ë¡œ ê±°ë¦¬ ì¸¡ì • ê¸°ì¤€ì„ ì§ì ‘ ìµœì í™”í•´ì•¼ í•¨ (End-to-End)&lt;/li&gt;
  &lt;li&gt;ê·¸ëŸ¬ë‚˜, í™”ì ê²€ì¦ì—ì„œ í›ˆë ¨í•˜ê¸° ì–´ë ¤ìš´ ê²ƒìœ¼ë¡œ íŒë‹¨&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;âœ”  I-vector systemê³¼ ë™ì¼í•˜ê²Œ ì‚¬ìš©&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ì°¨ì› ê°ì†Œì—ëŠ” LDA(Linear Discriminant Analysis) ì‚¬ìš©&lt;/li&gt;
  &lt;li&gt;ê²€ì¦ ì‹œ PLDA(Probabilistic Linear Discriminant Analysis) ì‚¬ìš©&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;âœ”  NIST SRE 2016 dataset ì‚¬ìš©&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;í›ˆë ¨ ë°ì´í„°(ì˜ì–´)ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°(ê´‘ë‘¥ì–´ ë° íƒ€ê°ˆë¡œê·¸ì–´) ì‚¬ì´ì˜ mismatchë¥¼ ë„ì… (Domain or Covariate shift)&lt;/li&gt;
  &lt;li&gt;domain ë³´ìƒì„ ìœ„í•œ ì ì€ ì–‘ì˜ labelì´ ì—†ëŠ” target ë°ì´í„° ì œê³µ&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;âœ”  ë³¸ ë…¼ë¬¸ ì €ìì˜ ìµœê·¼ ì—°êµ¬ì—ì„œ, End-to-Endì˜ cosine scoreë¥¼ ì‚¬ìš©í•˜ëŠ” domain adversarial í›ˆë ¨ì„ ì´ìš©í•œ domain ë¶ˆë³€ í™”ì embedding í›ˆë ¨ ì œì•ˆ (Domain Adversarial Neural Speaker Embeddings, DANSE)&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Gradient reversalì„ ì‚¬ìš©í•˜ì—¬ domain ë¶ˆë³€ì„± ë° adversarial grameì˜ ìµœì†Œí™” ëª©í‘œë¥¼ ë‹¬ì„±&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;background-color:#f4d451&quot;&gt;&lt;strong&gt;âœ”  ë³¸ ë…¼ë¬¸ì—ì„œëŠ” GANsë¥¼ ì‚¬ìš©í•˜ì—¬ unsupervised domain adaptation/invariantë¡œ ì´ì „ ì—°êµ¬ í™•ì¥&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&amp;lt; ì¥ì &amp;gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;gradient reversalë³´ë‹¤ ë¶ˆë³€ì„± mappingì„ í•™ìŠµí•˜ëŠ”ë° ë” ë‚˜ì€ gradients ì œê³µ&lt;/li&gt;
  &lt;li&gt;GAN frameworkëŠ” gradient reversalë³´ë‹¤ ë” ì¼ë°˜ì ì´ê³  í™•ì¥ ê°€ëŠ¥&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ”  ë‹¤ì–‘í•œ GAN ë³€í˜•&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;íŠ¹ì§• ê³µê°„ì˜ ë‹¤ë¥¸ ë³€í˜•ì„ ìƒì„±&lt;/li&gt;
  &lt;li&gt;ì´ëŸ¬í•œ íŠ¹ì§• ê³µê°„ì„ ê²°í•©ì´ ì„±ëŠ¥ í–¥ìƒì„ ê°€ì ¸ì˜´&lt;/li&gt;
  &lt;li&gt;Auxiliary Classifier GAN(AuxGAN)ì˜ ìˆ˜ì •ì„ ì œì•ˆ&lt;/li&gt;
  &lt;li&gt;GAN ëª¨ë¸ì´ DNASE ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ëŠ¥ê°€&lt;/li&gt;
  &lt;li&gt;ë‹¤ì–‘í•œ GAN ëª¨ë¸ì˜ scoreë¥¼ í‰ê· í•¨ìœ¼ë¡œì¨ x-vectorì˜ ì„±ëŠ¥ë³´ë‹¤ í–¥ìƒë¨&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;â…±-domain-adaption-with-gans&quot;&gt;&lt;strong&gt;â…¡. Domain Adaption with GANs&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;âœ”  GAN&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Generator : target dataë¥¼ source dataì˜ domainìœ¼ë¡œ mapping&lt;/li&gt;
  &lt;li&gt;Discriminator : source dataì™€ target dataì˜ domainì„ êµ¬ë³„&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/92464311-ecd07a00-f207-11ea-8527-64991f1f261d.png&quot; alt=&quot;img&quot; style=&quot;zoom: 70%;&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;ì—¬ëŸ¬ GAN ë³€í˜•ì— í•´ë‹¹í•˜ëŠ” ë‹¤ë¥¸ discriminatorì˜ êµ¬ì„±ì´ íŠ¹ì§• ê³µê°„ì˜ ë‹¤ë¥¸ ë³€í™˜ì„ ê°€ì ¸ì˜¨ë‹¤ëŠ” ê²ƒì„ ë°œê²¬&lt;/li&gt;
  &lt;li&gt;vanilla GANì—ì„œ discriminatorëŠ” binary cross-entropy(BCE) lossë¥¼ ìµœì í™”í•˜ì—¬ í›ˆë ¨&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ”  GAN game (ê¸°ì¡´ GAN loss)&lt;/strong&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/92464831-af202100-f208-11ea-9c86-bb4318bebe00.png&quot; alt=&quot;img&quot; style=&quot;zoom: 50%;&quot; /&gt;&lt;/center&gt;

&lt;blockquote&gt;
  &lt;p&gt;E, D : Embedding(generator), Discriminator í•¨ìˆ˜&lt;/p&gt;
&lt;/blockquote&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/92472815-fc09f480-f214-11ea-9b00-1274915072c1.png&quot; alt=&quot;img&quot; style=&quot;zoom: 70%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ”  Gradients reversal model&lt;/strong&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/92472846-01673f00-f215-11ea-8f18-c267f86a118d.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;â…²-generative-adversarial-speaker-embedding-networks&quot;&gt;&lt;strong&gt;â…¢. Generative Adversarial Speaker Embedding Networks&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;âœ”  ë³¸ ë…¼ë¬¸ì˜ ëª©í‘œ&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;í™”ì embedding modelì´ íŠ¹ì§• ì¶”ì¶œê¸°(generator)ì™€ domain ì‹ë³„ì(discriminator) ì‚¬ì´ì˜ GAN gameì„ í†µí•´ domain ë¶ˆë³€ì  íŠ¹ì§•ì„ í•™ìŠµ&lt;/li&gt;
  &lt;li&gt;GANì´ domain ë¶ˆë³€ì„±ì„ ê°–ìœ¼ë©°, embeddingì´ í™”ìë¥¼ êµ¬ë¶„í•  ìˆ˜ ìˆì–´ì•¼ í•¨&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ”  Loss function (AM-softmax/GAN loss)&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;classê°„ cosine similarityë¥¼ ì§ì ‘ ìµœì í™”&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/92466967-d7f5e580-f20b-11ea-9b8b-ae4db11acd0b.png&quot; alt=&quot;img&quot; style=&quot;zoom: 50%;&quot; /&gt;&lt;/center&gt;

&lt;blockquote&gt;
  &lt;p&gt;C, E : Classifier, Embedding(generator)  í•¨ìˆ˜&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/92467164-17243680-f20c-11ea-83c2-adb068c4d9df.png&quot; alt=&quot;img&quot; style=&quot;zoom: 40%;&quot; /&gt;&lt;/center&gt;

&lt;blockquote&gt;
  &lt;p&gt;s, m : scale factor, margin&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;BCE lossë¥¼ ì‚¬ìš©í•˜ì—¬ domain discriminatorë¥¼ í›ˆë ¨&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/92467390-7d10be00-f20c-11ea-9136-515c58c834d9.png&quot; alt=&quot;img&quot; style=&quot;zoom: 50%;&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;ë§ˆì§€ë§‰ìœ¼ë¡œ, ì•„ë˜ì˜ lossë¥¼ ì‚¬ìš©í•˜ì—¬ discriminatorë¥¼ ì†ì´ê¸° ìœ„í•´ generator(embedding) í›ˆë ¨&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/92467437-9580d880-f20c-11ea-9aa0-c336bf2bd007.png&quot; alt=&quot;img&quot; style=&quot;zoom: 50%;&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;embedding í•¨ìˆ˜ëŠ” task lossì™€ í•¨ê»˜ ê·¸ ë‹¤ìŒ adversarial loss ì´ 2ë²ˆ í•™ìŠµ&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;31-auxiliary-classifier-gan&quot;&gt;3.1. Auxiliary Classifier GAN&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;âœ”  AuxGAN(ACGAN)&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ì¡°ê±´(conditional) ì´ë¯¸ì§€ ìƒì„±ì„ ìœ„í•´ ë³´ì¡°(Auxiliary) lossë¥¼ ì‚¬ìš©í•˜ì—¬ GANì„ ë³´ì™„&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;side ì •ë³´(class label ë“±)ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ ëª©í‘œ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;D (discriminator) : 2ê°œì˜ classifier
 - ë°ì´í„°ê°€ ì§„ì§œ(real) ì¸ì§€ ê°€ì§œ(fake) ì¸ì§€ íŒë³„
 - í•´ë‹¹ ë°ì´í„°ì˜ ë²”ì£¼(category)ë¥¼ ë¶„ë¥˜&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;G (generator) : labelì •ë³´ì™€ z(noise)ë¡œ ê°€ì§œ ë°ì´í„° ìƒì„±&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/92468316-ec3ae200-f20d-11ea-882d-0045ffc0cd5c.png&quot; alt=&quot;img&quot; style=&quot;zoom: 50%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;strong&gt;âœ”  ì›ë˜ ACGANì˜ object fuction&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;sourceì˜ log-likelihood $L_s$, classì˜ log-likelihood $L_c$
    &lt;blockquote&gt;
      &lt;p&gt;$L_s$ : ê¸°ì¡´ GANì˜ ëª©ì  í•¨ìˆ˜ì™€ ê°™ìŒ (real/fake íŒë³„)&lt;br /&gt;
$L_c$ : í•´ë‹¹ ë°ì´í„°ì˜ classë¥¼ íŒë‹¨ (conditional-GAN, CGANê³¼ ìœ ì‚¬)&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;D(discriminator)ëŠ” $L_s + L_c$ë¥¼ ìµœëŒ€í™”&lt;/li&gt;
  &lt;li&gt;G(generator)ëŠ” $L_c - L_s$ë¥¼ ìµœëŒ€í™”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/92473528-b699f700-f215-11ea-8256-b66f1ff59f9b.png&quot; alt=&quot;img&quot; style=&quot;zoom: 70%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ”  ë…¼ë¬¸ì—ì„œ ì‚¬ìš©í•œ ACGANì˜ object function&lt;/strong&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/92469327-9a935700-f20f-11ea-8183-b78231d799d4.png&quot; alt=&quot;img&quot; style=&quot;zoom: 50%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;32-gan-variants&quot;&gt;3.2. GAN Variants&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;ğŸ”¹  ë‹¤ì–‘í•œ GANì˜ ë³€í˜• ì‚¬ìš©&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;í‘œì¤€ GAN&lt;/li&gt;
  &lt;li&gt;Least-Squares GAN&lt;/li&gt;
  &lt;li&gt;Relativistic GAN&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;ğŸ”¹  ê° ë³€í˜•ì´ íŠ¹ì§• ê³µê°„ì„ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ë³€í˜•&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ëª¨ë“  ëª¨ë¸ì€ ê±°ì˜ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë³´ì„&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;ğŸ”¹ ëª¨ë“  GAN ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê²°í•©&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;í‰ê·  ì ìˆ˜(cosine distance score)ë¥¼ ê²°í•©í•œ ê²ƒì´ ìµœê³ ì˜ ì„±ëŠ¥ì„ ë³´ì„&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;â…³--experiments-and-results&quot;&gt;&lt;strong&gt;â…£.  Experiments and Results&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;âœ”  Training data(source)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ì œì•ˆí•œ DANSE ëª¨ë¸ê³¼ x vector, i vector ì˜ baseline ì„ í›ˆë ¨í•˜ê¸° ìœ„í•´ NIST SRE 2004 2010 ë° Switchboard Cellular audio ì‚¬ìš©&lt;/li&gt;
  &lt;li&gt;ì¡ìŒ ë° ì”í–¥ìœ¼ë¡œ ë°ì´í„° ì¦ê°• (128Kì˜ noisy dataì¶”ê°€í•˜ì—¬, 220Kê°œ ì‚¬ìš©)&lt;/li&gt;
  &lt;li&gt;Adversarial ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ê¸° ìœ„í•´ , 5 ê°œ ì´í•˜ì˜ ë°œí™”ì¸ í™”ìëŠ” ê±¸ëŸ¬ë‚´ê³  ì•½ 6000 ëª…ì˜ í™”ìë¥¼ ì‚¬ìš©&lt;/li&gt;
  &lt;li&gt;x-vector, i-vector ëŠ” Kaldi toolkit ì‚¬ìš©&lt;/li&gt;
  &lt;li&gt;ëŒ€ë¶€ë¶„ì´ ì˜ì–´ ì‚¬ìš©ì ì´ë©° , ì „í™”ë¥¼ í†µí•´ ë…¹ìŒ&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ”  Model&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Embedding(generator) í•¨ìˆ˜ëŠ” 3X 2 3 input ì˜ Convolutional layer, 4 ê°œì˜ residual block, attentive statistics layer, 2 ê°œì˜ fully connected layer (512, 512) ë¡œ êµ¬ì„±&lt;/li&gt;
  &lt;li&gt;ClassifierëŠ” fully connected layer (64) ì™€ AM softmax output layer ë¡œ êµ¬ì„± (fully connected layer ê°€ ìµœì¢… domain ë¶ˆí¸ í™”ì embedding)&lt;/li&gt;
  &lt;li&gt;DiscriminatorëŠ” 2 ê°œì˜ fully connected layer (256, 256) ì™€ binary cross entropy output layer ë¡œ êµ¬ì„±&lt;/li&gt;
  &lt;li&gt;ELU(Exponential Linear Units)ë¥¼ ëª¨ë“  ê³„ì¸µì— ì‚¬ìš©&lt;/li&gt;
  &lt;li&gt;Batch normalizationì€ attentive statistics layer ë¥¼ ì‚¬ìš©í•œ ê³„ì¸µì— ì‚¬ìš©&lt;/li&gt;
  &lt;li&gt;AMsoftmax loss ì˜ s ì™€ m parameter ëŠ” ê°ê° 30 ê³¼ 0.6 ìœ¼ë¡œ ì„¤ì •&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ”  Optimization&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;cross entropy í›ˆë ¨ì„ ì‚¬ìš©í•˜ì—¬ embedding íŠ¹ì§•ì„ ì‚¬ì „ í›ˆë ¨&lt;/li&gt;
  &lt;li&gt;ì„¸ ê°€ì§€ ë„¤íŠ¸ì›Œí¬ (embedding íŠ¹ì§• , Classifier, ë¥¼ ì„œë¡œ ë‹¤ë¥¸ optimizer ì‚¬ìš©&lt;/li&gt;
  &lt;li&gt;DiscriminatorëŠ” lr = 0.003 ì˜ RMSprop , Classifier ì™€ embedding ì€lr 0.001 ì˜ SGD ì‚¬ìš©&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ”  Data sampling&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;í›ˆë ¨ ì¤‘ í›ˆë ¨ set ì˜ ê° ë…¹ìŒì—ì„œ ë¬´ì‘ìœ„ë¡œ audio chunk sampling&lt;/li&gt;
  &lt;li&gt;ê° ìŒì„±ì„ 10 ë²ˆ sampling (epoch)&lt;/li&gt;
  &lt;li&gt;Source dataì˜ mini batch ì— ëŒ€í•´ GAN í›ˆë ¨ì„ ìœ„í•œ label ì´ ì—†ëŠ” adaption data ë„ ë™ì¼í•˜ê²Œ ë¬´ì‘ìœ„ë¡œ mini batch ë¥¼ sampling&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ”  Speaker Verification&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Testì‹œ embedding ì¶”ì¶œì— í•„ìš”í•˜ì§€ ì•Šì€ domain discriminator ë¥¼ ì—†ì•°&lt;/li&gt;
  &lt;li&gt;64ì°¨ì›ì˜ ë§ˆì§€ë§‰ hidden layer ê°€ ìµœì¢… í™”ì embedding&lt;/li&gt;
  &lt;li&gt;Verificationì‹¤í—˜ì€ cosine distance ë¥¼ ì‚¬ìš©í•˜ì—¬ score ê³„ì‚°&lt;/li&gt;
  &lt;li&gt;ì„±ëŠ¥ì˜ ì§€í‘œëŠ” EER ì‚¬ìš©&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ”  Model block&lt;/strong&gt;&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/92470103-e5fa3500-f210-11ea-8ca4-58b5d1bcf508.png&quot; alt=&quot;img&quot; style=&quot;zoom: 50%;&quot; /&gt;&lt;/center&gt;
&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/92470119-ebf01600-f210-11ea-8d0b-bab531d6d72d.png&quot; alt=&quot;img&quot; style=&quot;zoom: 50%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;âœ”  ì œì•ˆí•œ adversarial í™”ì embeddingê³¼ baseline system ì„±ëŠ¥ ë¹„êµ&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Baselineì‹œìŠ¤í…œ ì¤‘ì—ì„œëŠ” DNN ê¸°ë°˜ì˜ x vector ì‹œìŠ¤í…œì´ LDA ì°¨ì› ê°ì†Œ ì¶”ê°€í•˜ëŠ” ê²ƒ ë§Œìœ¼ë¡œë„ i-vector ì˜ ì„±ëŠ¥ë³´ë‹¤ í–¥ìƒ&lt;/li&gt;
  &lt;li&gt;ëª¨ë“  GAN ê¸°ë°˜ì˜ ëª¨ë¸ì´ DANSE ë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì„&lt;/li&gt;
  &lt;li&gt;AuxGAN(ACGAN), LSGAN, RelGAN embedding ì˜ score ë¥¼ í‰ê· í•œ ê²ƒì´ ê°€ì¥ ì„±ëŠ¥ì„ í¬ê²Œ ê°œì„ í•¨&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/92470359-3d98a080-f211-11ea-8d38-75adaeb55df0.png&quot; alt=&quot;img&quot; style=&quot;zoom: 50%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;â…´--conclusion&quot;&gt;&lt;strong&gt;â…¤.  Conclusion&lt;/strong&gt;&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;GANsë¥¼ ì´ìš©í•œ domain ë¶ˆë³€ í™”ì embedding í•™ìŠµì„ ìœ„í•œ ìƒˆë¡œìš´ framework ì œì•ˆ&lt;/li&gt;
  &lt;li&gt;ì—¬ëŸ¬ ê°€ì§€ GAN ì˜ ë³€í˜•ì„ í•™ìŠµí•˜ì—¬ score ë¥¼ ê²°í•©í•¨ìœ¼ë¡œì¨ í¬ê²Œ í–¥ìƒëœ ì„±ëŠ¥ì„ ì–»ìŒ&lt;/li&gt;
  &lt;li&gt;End-to-End model ì— ìµœì í™”ë˜ì–´ ìˆìœ¼ë©° ê°„ë‹¨í•œ cosine distance ë¥¼ ì‚¬ìš©í•˜ì—¬ score ë¥¼ ê³„ì‚°&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;í–¥í›„ íŠ¹ì§• ê³µê°„ê³¼ ë°ì´í„° ê³µê°„ GAN ì˜ ê²°í•© ë° GAN ê¸°ë°˜ íŠ¹ì§• ê³µê°„ ì¦ê°• ë°©ë²•ê³¼ ê°™ì´ ë‹¤ë¥¸ adversarial ì „ëµì„ ê³ ë ¤í•  ê²ƒ&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 03 Jun 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2019-06-03-Generative-Adversarial-Speaker-Embedding-Networks-for-Domain-Roubust-E2E-SV</link>
        <guid isPermaLink="true">http://localhost:4000/2019-06-03-Generative-Adversarial-Speaker-Embedding-Networks-for-Domain-Roubust-E2E-SV</guid>
        
        
        <category>review</category>
        
      </item>
    
      <item>
        <title>End-to-End DNN based Speaker Recognition Inspired by i-vector and PLDA</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#-abstract&quot; id=&quot;markdown-toc--abstract&quot;&gt;ğŸ“Œ &lt;strong&gt;Abstract&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#â…°-introduction&quot; id=&quot;markdown-toc-â…°-introduction&quot;&gt;&lt;strong&gt;â… . Introduction&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#â…±-database-and-baseline-systems&quot; id=&quot;markdown-toc-â…±-database-and-baseline-systems&quot;&gt;&lt;strong&gt;â…¡. Database and Baseline Systems&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#â…²-proposed-end-to-end-dnn-architecture&quot; id=&quot;markdown-toc-â…²-proposed-end-to-end-dnn-architecture&quot;&gt;&lt;strong&gt;â…¢. Proposed End-to-End DNN Architecture&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#â…³--results-and-discussion&quot; id=&quot;markdown-toc-â…³--results-and-discussion&quot;&gt;&lt;strong&gt;â…£.  Results and Discussion&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#â…´--conclusion&quot; id=&quot;markdown-toc-â…´--conclusion&quot;&gt;&lt;strong&gt;â…¤.  Conclusion&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span style=&quot;font-size:13pt&quot;&gt;Johan Rohdin, Anna Silnova , Mireia Diez, Oldrich Plchot , Pavel Matejka , Lukas Burget&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;-abstract&quot;&gt;ğŸ“Œ &lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;ìµœê·¼ text-dependentë¿ë§Œ ì•„ë‹ˆë¼ ì§§ì€ ë°œí™”ì—ì„œì˜ text-independent taskì—ì„œë„ DNN ê¸°ë°˜ End-to-End ì‹œìŠ¤í…œì˜ ê²½ìŸë ¥ì„ ì…ì¦&lt;/li&gt;
  &lt;li&gt;ê·¸ëŸ¬ë‚˜ &lt;strong&gt;ê¸´ ë°œí™” text-independentì˜ ê²½ìš° ì•„ì§ i-vector + PLDA ê¸°ë°˜ ì‹œìŠ¤í…œì´ ë” ì¢‹ì€ ì„±ëŠ¥&lt;/strong&gt;ì„ ë³´ì„&lt;/li&gt;
  &lt;li&gt;&lt;span style=&quot;background-color:#fff6dd&quot;&gt;i-vector + PLDA baselineì„ ëª¨ë°©í•œ speaker verification systemì„ ì œì•ˆ&lt;/span&gt;
&lt;strong&gt;(End-to-End ë°©ì‹ìœ¼ë¡œ í›ˆë ¨ë˜ì§€ë§Œ baseline systemì— ë©€ë¦¬ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡ ì •ê·œí™”)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ overfittingìœ¼ë¡œ ë°œìƒí•˜ëŠ” ì„±ëŠ¥ ì €í•˜ë¥¼ í•´ê²°í•˜ì˜€ìœ¼ë©°, ê¸´ ë°œí™”ì™€ ì§§ì€ ë°œí™”ì—ì„œ ëª¨ë‘ i-vector + PLDA baseline systemë³´ë‹¤ ì„±ëŠ¥ì´ í–¥ìƒëœ ê²ƒì„ í™•ì¸&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;â…°-introduction&quot;&gt;&lt;strong&gt;â… . Introduction&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;[ ì´ì „ì— ì†Œê°œëœ DNN ê¸°ë°˜ì˜ speaker recognition system íŠ¹ì§• ]&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;i-vector + PLDA systemì˜ êµ¬ì„±ìš”ì†Œ&lt;/strong&gt;(feature extraction, calculation of sufficient statistics, i-vector extraction or PLDA) ì¤‘ &lt;strong&gt;í•˜ë‚˜ë¥¼ NN&lt;/strong&gt;(Neural Network)ë¡œ &lt;strong&gt;ëŒ€ì²´&lt;/strong&gt;í•˜ê±°ë‚˜ &lt;strong&gt;ê°œì„ &lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;MFCC feature ëŒ€ì‹  bottleneck feature ì‚¬ìš©&lt;/li&gt;
      &lt;li&gt;sufficient statistics ê³„ì‚° ì‹œ GMM-UBM ëŒ€ì‹  NN acoustic model ì‚¬ìš©&lt;/li&gt;
      &lt;li&gt;PLDAë¥¼ ë³´ì™„í•˜ê±°ë‚˜ ëŒ€ì²´í•˜ëŠ” NN ì‚¬ìš©&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Speaker IDë¥¼ ë¶„ë¥˜í•˜ì—¬ í›ˆë ¨í•œ NN&lt;/strong&gt;ì„ í†µí•´ &lt;strong&gt;speaker embedding ì¶”ì¶œ&lt;/strong&gt; - ëŒ€í‘œì ì¸ íŠ¹ì§• : d-vector, x-vectorì™€ ê°™ì€ embedding
    &lt;ul&gt;
      &lt;li&gt;acoustic featureë¥¼ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ì„œ speaker labelê³¼ lossë¥¼ ê³„ì‚°í•œ ë’¤, NN ëª¨ë¸ì˜ ì¼ë¶€(hidden layer, TDNN + fully-connected DNN ì¤‘ DNN)ì„ utterance-levelì˜ featureë¡œ ì‚¬ìš©&lt;/li&gt;
      &lt;li&gt;text-dependent, ì§§ì€ ë°œí™” text-independentì—ì„œ íš¨ê³¼ì &lt;/li&gt;
      &lt;li&gt;ë¹„êµì  ê¸´ ë°œí™” text-independentì—ì„œëŠ” i-vector + PLDAë³´ë‹¤ ì„±ëŠ¥ì´ ë‚®ìŒ&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;background-color:#dee03f&quot;&gt;Proposed Method : i-vector + PLDA baselineì„ ëª¨ë°©í•œ End-to-End speaker verification ì‹œìŠ¤í…œ&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. f2s&lt;/strong&gt; (sufficient statistics ì¶”ì¶œì„ ìœ„í•œ NN ëª¨ë“ˆ)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. s2i&lt;/strong&gt; (i-vector ì¶”ì¶œì„ ìœ„í•œ NN ëª¨ë“ˆ)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. DPLDA&lt;/strong&gt; (ì ìˆ˜ ê³„ì‚°ì„ ìœ„í•œ Discriminative PLDA ëª¨ë“ˆ)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ì„¸ ê°œì˜ ëª¨ë“ˆì´ ê°œë³„ì ìœ¼ë¡œ baselineì„ ëª¨ë°©í•˜ê³ , í›ˆë ¨ë˜ë©° ì´í›„ ê²°í•©í•œ ë’¤ ì§§ì€ ë°œí™”ì™€ ê¸´ ë°œí™” ëª¨ë‘ì— ëŒ€í•´ End-to-End ë°©ì‹ìœ¼ë¡œ ì¶”ê°€ í›ˆë ¨ì„ ì§„í–‰í•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ì´ë•Œ, ì¶”ê°€ í›ˆë ¨ ì‹œ ê°œë³„ì ìœ¼ë¡œ í›ˆë ¨í•˜ì—¬ ì–»ì€ íŒŒë¼ë¯¸í„°ê°€ ë„ˆë¬´ ë§ì´ ìˆ˜ì •ë˜ì§€ ì•Šë„ë¡ ì •ê·œí™”ë¥¼ ì‹¤ì‹œ (baselineê³¼ ë„ˆë¬´ ë‹¬ë¼ì§€ëŠ” ê²ƒì„ ë°©ì§€í•˜ê³  overfittingì˜ ìœ„í—˜ì„ ì¤„ì´ëŠ” ì¥ì ì´ ì¡´ì¬)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;NIST SREì—ì„œ íŒŒìƒëœ 3ê°œì˜ ë‹¤ë¥¸ ë°ì´í„° ì…‹ì— ëŒ€í•´ ì‹œìŠ¤í…œì„ í‰ê°€ (ë‹¤ì–‘í•œ ì–¸ì–´ì˜ ìŒì„±ì„ í¬í•¨í•˜ê³ , 2ë¶„ ë¯¸ë§Œì˜ ê¸´ ë°œí™”ì™€ 40ì´ˆ ë¯¸ë§Œì˜ ì§§ì€ ë°œí™” ëª¨ë‘ì— ëŒ€í•´ ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸)&lt;/p&gt;

    &lt;p&gt;&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;â…±-database-and-baseline-systems&quot;&gt;&lt;strong&gt;â…¡. Database and Baseline Systems&lt;/strong&gt;&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ëŠ” PRISM  dataset ê¸°ë°˜, 3ê°€ì§€ í‰ê°€ ì…‹
(1) NIST SRE 2005~2010ë…„ ë°ì´í„° ì›ë³¸(ê¸´) ì „í™” ë°œí™” ì¤‘ ì—¬ì„±
(2) (1) ìŒì›ì„ ì—¬ëŸ¬ ì§§ì€ ë°œí™”ë¡œ ìƒì„±(ë“±ë¡ : 20~50ì´ˆ, í…ŒìŠ¤íŠ¸ 30~40ì´ˆ)
(3) NIST SRE 2016 í‰ê°€ ì„¸íŠ¸ (ë‚¨/ì—¬ ëª¨ë‘, ë‹¨ì¼ ë“±ë¡)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Generative(PLDA) and Discriminative(DPLDA) Baseline
    &lt;ul&gt;
      &lt;li&gt;íŠ¹ì§• : 60dimension-MFCC (20ì°¨ì›, âˆ†, âˆ†âˆ†)&lt;/li&gt;
      &lt;li&gt;í›ˆë ¨ ë°ì´í„° ì¤‘ ì „í™” ë°ì´í„°ë§Œ ì‚¬ìš© (ì§§ì€ ë°œí™” ì‹œê°„ì€ 10~60ì´ˆ ì‚¬ì´ ê· ì¼ ë¶„í¬ë¥¼ ë”°ë¥´ë©° ì´ 85,858ê°œ ì¤‘ ì§§ì€ ë°œí™”ëŠ” 22,766ê°œ)&lt;/li&gt;
      &lt;li&gt;PLDA/DPLDA : 2048ê°œ componentë¥¼ ê°–ëŠ” UBM, 400ì°¨ì› i-vector&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;background-color:#f4d451&quot;&gt;PLDA&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;i-vectorì˜ í‰ê· (ëª¨ë“  í›ˆë ¨ ë°ì´í„°ì˜ i-vector í‰ê· ) ê³¼ ê¸¸ì´ë¥¼ ì •ê·œí™”&lt;/li&gt;
  &lt;li&gt;ì¶”ê°€ì ì¸ domain ì ì‘ì´ë‚˜ score normalizationì€ ìˆ˜í–‰í•˜ì§€ ì•ŠìŒ&lt;/li&gt;
  &lt;li&gt;ê° í™”ìê°€ 6ê°œì˜ ë°œí™”ë¥¼ ê°–ë„ë¡ í›ˆë ¨ ë°ì´í„°ë¥¼ 68,994ê°œë¡œ ì¤„ì—¬ì„œ ì‚¬ìš©&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span style=&quot;background-color:#f4d451&quot;&gt;DPLDA&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;LBFGS optimizerë¡œ binary cross-entropyë¥¼ ìµœì í™” (ëª¨ë¸ í›ˆë ¨ ì‹œ, ì´ˆê¸°í™”ë¡œ PLDAë¥¼ ì‚¬ìš©)&lt;/li&gt;
  &lt;li&gt;i-vectorì˜ í‰ê· ê³¼ ê¸¸ì´ë¥¼ ì •ê·œí™”&lt;/li&gt;
  &lt;li&gt;LDAë¥¼ ìˆ˜í–‰í•˜ì—¬ i-vectorì˜ ì°¨ì›ì„ 250ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;â…²-proposed-end-to-end-dnn-architecture&quot;&gt;&lt;strong&gt;â…¢. Proposed End-to-End DNN Architecture&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;1. Feature to Sufficient statistics : f2s [íŠ¹ì§• ë²¡í„° â†’ ì¶©ë¶„ í†µê³„ëŸ‰]&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ì…ë ¥ ë°œí™”ì˜ ê° frameì— ëŒ€í•´ GMM responsibilities (posteriors, ì‚¬í›„ í™•ë¥ )ì„ ì˜ˆì¸¡
    &lt;ul&gt;
      &lt;li&gt;60ì°¨ì›ì˜ MFCCë¥¼ ì „ì²˜ë¦¬(preprocessing) í•˜ì—¬ inputìœ¼ë¡œ ì‚¬ìš©&lt;/li&gt;
      &lt;li&gt;í˜„ì¬ frameì„ ê¸°ì¤€ìœ¼ë¡œ Â±15 frameì„ ê³ ë ¤ (ì´ 31ê°œ frame) â†’ 6ê°œ ì‚¬ìš©&lt;/li&gt;
      &lt;li&gt;6 * 60 â†’ 360ì°¨ì›&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Hidden layer : 4ê°œ (activation function : sigmoid, node : 1500ê°œ)&lt;/li&gt;
  &lt;li&gt;Output : 2048ê°œ (GMM-UBM baselineì˜ component ìˆ˜) - softmax&lt;/li&gt;
  &lt;li&gt;Optimizer : SGD(stochastic gradient descent)&lt;/li&gt;
  &lt;li&gt;Loss : categorical cross-entropy (label : GMM-UBMì˜ ì‚¬í›„ í™•ë¥ )&lt;/li&gt;
  &lt;li&gt;frameì„ ì¶©ë¶„ í†µê³„ëŸ‰ìœ¼ë¡œ pooling (ì „ì²´ frameì— ê±¸ì³ softmax layerì—ì„œ ë‚˜ì˜¨ ì‚¬í›„ í™•ë¥ , ì „ì²˜ë¦¬í•˜ì§€ ì•Šì€ MFCC)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Sufficient statistics to i-vectors : s2i [ì¶©ë¶„ í†µê³„ëŸ‰ â†’ i-vector]&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;f2sì—ì„œ ë‚˜ì˜¨ ì¶©ë¶„ í†µê³„ëŸ‰ì„ inputìœ¼ë¡œ ì‚¬ìš© (2048x60ì°¨ì›)&lt;/li&gt;
  &lt;li&gt;MAP ì ì‘ëœ supervectorë¡œ ë³€í™˜ (112880 ì°¨ì›) - ì°¨ì› ìˆ˜ë¥¼ ì¤„ì´ê¸° ìœ„í•´ PCAë¥¼ ì‚¬ìš©í•˜ì—¬ 4000ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ&lt;/li&gt;
  &lt;li&gt;Hidden layer : 3ê°œ (activation function : tanh, 1-2 layer node : 600ê°œ, 3 layer node : 250ê°œ) - ë§ˆì§€ë§‰ layerì—ì„œ i-vector ê¸¸ì´ ì •ê·œí™”&lt;/li&gt;
  &lt;li&gt;NNì˜ outputê³¼ LDAë¥¼ ì‚¬ìš©í•˜ì—¬ 250ì°¨ì›ìœ¼ë¡œ ì¤„ì´ê³  ê¸¸ì´ë¥¼ ì •ê·œí™”í•œ reference i-vectorì˜ average cosine distance&lt;/li&gt;
  &lt;li&gt;Optimizer : SGD, L1-regularization&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/89503150-1e1ceb00-d801-11ea-9660-b2255b35a32f.png&quot; alt=&quot;img&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. i-vector to scores (DPLDA)&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ë‘ i-vector(Ï• í‘œê¸°)ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, PLDA ëª¨ë¸ì˜ Log-Likelihood Ratio(LLR)&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/89503158-1fe6ae80-d801-11ea-9705-f8eeb4eefac9.png&quot; alt=&quot;img&quot; style=&quot;zoom:60%;&quot; /&gt;&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;DPLDAëŠ” ìœ„ì˜ ì‹ì—ì„œ íŒŒë¼ë¯¸í„° (Î›, Î“, c, k)ë¥¼ í›ˆë ¨í•˜ì—¬ êµ¬í•˜ëŠ” ê²ƒ&lt;/li&gt;
  &lt;li&gt;ë‘ i-vectorê°€ ê°™ì€ í™”ì ì¸ì§€ íŒë‹¨ (Binary cross-entropy í˜¹ì€ SVM ìµœì í™”ë¥¼ í†µí•´ ì–»ì–´ì§)&lt;/li&gt;
  &lt;li&gt;ëª¨ë“  í›ˆë ¨ ë°ì´í„°ë¥¼ ê¸°ë°˜ ê³„ì‚° (ì „ì²´ batchë¥¼ ì‚¬ìš©) í•˜ë‚˜, End-to-End ì‹œìŠ¤í…œ í›ˆë ¨ ì‹œì—ëŠ” ë„ˆë¬´ ë§ì€ ë©”ëª¨ë¦¬ì™€ ì‹œê°„ì´ í•„ìš”í•˜ì—¬ í›ˆë ¨ ë°ì´í„° ì¤‘ ë¬´ì‘ìœ„ë¡œ subsetì„ ì„ íƒí•œ Minibatch ê¸°ë°˜&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&amp;lt; Minibatch ì„ íƒ rule &amp;gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;ê° í™”ìì— ëŒ€í•´ ëœë¤í•˜ê²Œ ë°œí™”ë¥¼ ìŒìœ¼ë¡œ ë§Œë“ ë‹¤
    &lt;ul&gt;
      &lt;li&gt;ë§Œì•½ ì–´ë–¤ í™”ìì˜ ë°œí™”ê°€ í•˜ë‚˜ë¼ë©´ ë™ì¼í•œ ë°œí™”ë¥¼ ìŒìœ¼ë¡œ ë§Œë“¤ì–´ì„œ ì‚¬ìš©&lt;/li&gt;
      &lt;li&gt;ë§Œì•½ ì–´ë–¤ í™”ìì˜ ë°œí™”ê°€ ê· ë“±í•œ ê°œìˆ˜ê°€ ì•„ë‹ˆë¼ë©´ ë°œí™”ì˜ ìŒ ì¤‘ í•˜ë‚˜ëŠ” ì„¸ ê°œì˜ ë°œí™”ë¥¼ ê°€ì§&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ê° Minibatchì— ëŒ€í•´ ì„ì˜ë¡œ N ê°œì˜ ë°œí™”ë¥¼ ì„ íƒí•˜ì—¬ ì„ íƒëœ ë°œí™”ë¡œ í˜•ì„±ë  ìˆ˜ ìˆëŠ” ëª¨ë“  ì‹¤í—˜ì— ì‚¬ìš©(ë§ˆì§€ë§‰ ìŒì„ ì„ íƒí•œ ê²½ìš° ë‹¤ì‹œ 1ë¡œ ëŒì•„ê°)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4. End-to-End System&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ì•ì„œ ê°œë³„ì ìœ¼ë¡œ í›ˆë ¨í•œ ë’¤ ê²°í•©í•˜ì—¬ End-to-Endë¡œ ì¶”ê°€ í›ˆë ¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ë©”ëª¨ë¦¬ê°€ êµ‰ì¥íˆ ë§ì´ í•„ìš”í•˜ëŠ” ë¬¸ì œì ì´ ì¡´ì¬&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;PCA : f2sì™€ s2ië¥¼ ì—°ê²°í•˜ê¸° ìœ„í•´ networkì˜ ì¼ë¶€ê°€ ë˜ì–´ì•¼ í•˜ëŠ”ë°, 122800x4000ê°œì˜ íŒŒë¼ë¯¸í„°ê°€ í•„ìš”
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;ì „ì²´ End-to-End í›ˆë ¨ ì „ì—, s2i NNê³¼ DPLDA ëª¨ë¸ë§Œ ê³µë™ìœ¼ë¡œ í›ˆë ¨&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;s2iì˜ ê°œë³„ í›ˆë ¨ ì‹œ, f2sê°€ ì—…ë°ì´íŠ¸ë˜ì§€ ì•ŠëŠ” ì´ìƒ ì…ë ¥ì´ ê³ ì •ì´ë¯€ë¡œ PCAë¥¼ ê±°ì¹œ íŠ¹ì§•ì„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;f2s : DPLDA ëª¨ë“ˆì„ í›ˆë ¨í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ ê°€ì§€ ë‹¤ì–‘í•œ ë°œí™”ì˜ ëª¨ë“  frameì„ í•œ ë²ˆì— ì²˜ë¦¬í•´ì•¼ í•¨
    &lt;ul&gt;
      &lt;li&gt;ì¤‘ê°„ ê²°ê³¼ë¥¼ ë©”ëª¨ë¦¬ì— ëœ ìœ ì§€í•˜ë„ë¡ í›ˆë ¨ê³¼ì •ì„ ìˆ˜ì •&lt;/li&gt;
      &lt;li&gt;í•˜ë‚˜ì˜ ë°œí™”ì— ëŒ€í•´ ì¶©ë¶„ í†µê³„ëŸ‰ì„ ê³„ì‚°í•˜ê³  block Aì˜ ëª¨ë“  layerì˜ ì¶œë ¥ì„ ì—†ì•°&lt;/li&gt;
      &lt;li&gt;block Aì˜ íŒŒë¼ë¯¸í„°ëŠ” ì „ì²´ frame(nf) x (1500+1500+1500+1500+2048) ê°œ ë³€ìˆ˜ê°€ ë©”ëª¨ë¦¬ì— ì €ì¥&lt;/li&gt;
      &lt;li&gt;ì¶©ë¶„ í†µê³„ëŸ‰ F, Nìœ¼ë¡œ pooling í•œ ë’¤ íŒŒë¼ë¯¸í„° : 2048x60&lt;/li&gt;
      &lt;li&gt;Optimizer : ADAM&lt;/li&gt;
      &lt;li&gt;Training rateë¥¼ epochì—ì„œ $C^{prm}_{min}$ì´ ê°œì„ ë˜ì§€ ì•Šì„ ë•Œ  ë§ˆë‹¤ ì ˆë°˜ìœ¼ë¡œ ì¤„ì„&lt;/li&gt;
      &lt;li&gt;í›ˆë ¨ ë°ì´í„°ëŠ” DPLDAì™€ ê°™ìŒ&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;â…³--results-and-discussion&quot;&gt;&lt;strong&gt;â…£.  Results and Discussion&lt;/strong&gt;&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Baselineì˜ ì¼ë¶€ë§Œ NNìœ¼ë¡œ ëŒ€ì²´ëœ ì‹œìŠ¤í…œ, End-to-End ê²°ê³¼ í‘œ&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/89503694-df3b6500-d801-11ea-80f2-beb566897aa9.png&quot; alt=&quot;img&quot; /&gt;&lt;/center&gt;

&lt;center&gt;

&amp;gt; 1,2í–‰ : PLDAì™€ DPLDA baselineì˜ ì„±ëŠ¥  
&amp;gt; 3í–‰ : UBMì´ f2s NNìœ¼ë¡œ ëŒ€ì²´ë˜ì—ˆì„ ë•Œ ì„±ëŠ¥  
&amp;gt; 4í–‰ : i-vector ì¶”ì¶œê¸°ê°€ s2i NNìœ¼ë¡œ ëŒ€ì²´ë˜ì—ˆì„ ë•Œ ì„±ëŠ¥  
&amp;gt; 5í–‰ : UBMì˜ ì¶©ë¶„ í†µê³„ëŸ‰ ëŒ€ì‹  f2s ëª¨ë“ˆì˜ ì¶œë ¥ìœ¼ë¡œ s2i í›ˆë ¨ í•œ ì„±ëŠ¥  
&amp;gt; 6í–‰ : PLDA ëŒ€ì‹  DPLDAë¥¼ ì‚¬ìš©í•˜ì˜€ì„ ë•Œ ì„±ëŠ¥  
&amp;gt; 7í–‰ : s2iì™€ DPLDAë§Œ ê³µë™ìœ¼ë¡œ í›ˆë ¨ë  ë•Œì˜ ì„±ëŠ¥  
&amp;gt; 8í–‰ : ëª¨ë“  ëª¨ë“ˆì´ ê³µë™ìœ¼ë¡œ í›ˆë ¨ë  ë•Œì˜ ì„±ëŠ¥  

&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;3ê°œì˜ ëª¨ë“ˆì´ ê³µë™ìœ¼ë¡œ í›ˆë ¨ë  ë•Œì˜ ì„±ëŠ¥(8í–‰)ê³¼ 2ê°œì˜ ëª¨ë“ˆì´ ê³µë™ìœ¼ë¡œ í›ˆë ¨ë˜ì—ˆì„ ë•Œ ì„±ëŠ¥(7í–‰)ì´ í° ì°¨ì´ê°€ ì—†ì—ˆìŒ&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;background-color:#ceddf2&quot;&gt;&amp;lt; 3ê°€ì§€ ê°€ëŠ¥ì„± &amp;gt;&lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Minibatchê°€ ì•ˆì •ì ì¸ í›ˆë ¨ì„ í•˜ê¸°ì— ë„ˆë¬´ ì‘ì„ ìˆ˜ ìˆë‹¤. (3ê°œì˜ ëª¨ë“ˆì„ ê³µë™ìœ¼ë¡œ í›ˆë ¨ ì‹œ, N=75 ìµœëŒ€)&lt;/li&gt;
  &lt;li&gt;ëª¨ë¸ì´ local minimumìœ¼ë¡œ ê³ ì •ë  ìˆ˜ ìˆë‹¤. (f2sì˜ ì¶œë ¥ì— ë”°ë¼ í›„ì† ëª¨ë¸ë“¤ë„ í›ˆë ¨ì´ ë˜ê¸° ë•Œë¬¸)&lt;/li&gt;
  &lt;li&gt;f2sì˜ ì„¤ê³„ê°€ ìƒë‹¹íˆ ì œì•½ì ì´ë‹¤. (ì‚¬í›„ í™•ë¥ ë§Œ ì¶”ì • í•  ë¿ í†µê³„ ê³„ì‚°ì— ì‚¬ìš©ë˜ëŠ” íŠ¹ì§•ì„ ìˆ˜ì •í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;â…´--conclusion&quot;&gt;&lt;strong&gt;â…¤.  Conclusion&lt;/strong&gt;&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;ë‹¤ì–‘í•œ ì–¸ì–´ì™€ ê¸´ ë°œí™”, ì§§ì€ ë°œí™”ë¥¼ ëª¨ë‘ í¬í•¨í•˜ëŠ” ì„¸ ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„°ì…‹ì— ëŒ€í•œ i-vector + PLDA baselineì„ ëŠ¥ê°€í•˜ëŠ” End-to-End í™”ì ê²€ì¦ ì‹œìŠ¤í…œ ê°œë°œ&lt;/li&gt;
  &lt;li&gt;i-vector + PLDA ì‹œìŠ¤í…œê³¼ ë¹„ìŠ·í•˜ê²Œ ë™ì‘í•˜ë„ë¡ ì œí•œí•¨ìœ¼ë¡œì¨ End-to-End ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ì €í•˜ì‹œí‚¤ëŠ” overfittingì„ ì™„í™”&lt;/li&gt;
  &lt;li&gt;ì‹œìŠ¤í…œ 3ê°œì˜ ì„œë¸Œ ëª¨ë“ˆ ì¤‘ 3ê°œì˜ ëª¨ë“ˆì˜ ê³µë™ í›ˆë ¨ì€ ì„±ëŠ¥ì´ ì¢‹ì•˜ì§€ë§Œ, ëª¨ë‘ ê³µë™ í›ˆë ¨í•˜ì˜€ì„ ë•Œ íš¨ê³¼ì ì´ì§€ ì•Šì•˜ìŒ
    &lt;ul&gt;
      &lt;li&gt;ì„¸ê°€ì§€ ëª¨ë“ˆì„ ê³µë™ìœ¼ë¡œ í›ˆë ¨í•˜ì˜€ì„ ë•Œ ë” ë‚˜ì€ ì„±ëŠ¥ì´ ë‚˜ì˜¤ë„ë¡ ê°œë°œí•  ê²ƒ&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ë‹¨ì¼ ë“±ë¡ì„ ì‚¬ìš©í•˜ë„ë¡ ì„¤ê³„, ì—¬ëŸ¬ ë“±ë¡ì„ ì²˜ë¦¬í•˜ë„ë¡ í™•ì¥í•  ê²ƒ&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 22 May 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2019-05-22-E2E-DNN-based-Speaker-Recognition-Inspired-by-i-vector-and-PLDA</link>
        <guid isPermaLink="true">http://localhost:4000/2019-05-22-E2E-DNN-based-Speaker-Recognition-Inspired-by-i-vector-and-PLDA</guid>
        
        
        <category>review</category>
        
      </item>
    
      <item>
        <title>Attentive Statistics Pooling for Deep Speaker Embedding</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#-abstract&quot; id=&quot;markdown-toc--abstract&quot;&gt;ğŸ“Œ &lt;strong&gt;Abstract&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#-introduction&quot; id=&quot;markdown-toc--introduction&quot;&gt;ğŸ“Œ &lt;strong&gt;Introduction&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#-deep-speaker-embedding&quot; id=&quot;markdown-toc--deep-speaker-embedding&quot;&gt;ğŸ“Œ &lt;strong&gt;Deep speaker embedding&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#-high-order-pooling-with-attention&quot; id=&quot;markdown-toc--high-order-pooling-with-attention&quot;&gt;ğŸ“Œ &lt;strong&gt;High-order pooling with attention&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#-experimental-settings&quot; id=&quot;markdown-toc--experimental-settings&quot;&gt;ğŸ“Œ &lt;strong&gt;Experimental settings&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span style=&quot;font-size:13pt&quot;&gt;Koji Okabe, Takafumi Koshinaka, Koichi Shinoda&lt;/span&gt;&lt;/p&gt;

&lt;h1 id=&quot;-abstract&quot;&gt;ğŸ“Œ &lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;span style=&quot;background-color:#FFE49B&quot;&gt;&lt;strong&gt;Text-independent&lt;/strong&gt;(ë¬¸ì¥ ë…ë¦½ : ë°œí™” ë‚´ìš©ì´ ë™ì¼í•˜ì§€ í•˜ì§€ ì•ŠìŒ)í•œ &lt;strong&gt;Speaker Verification&lt;/strong&gt;(í™”ì ê²€ì¦ : ë“±ë¡ëœ í™”ìì¸ì§€ ì•„ë‹Œì§€ íŒë‹¨, SV)ì—ì„œ &lt;strong&gt;Deep speaker embeddingì„ ìœ„í•œ attentive statistics pooling&lt;/strong&gt; ì œì•ˆ&lt;/span&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ê¸°ì¡´ì˜ speaker embeddingì—ì„œëŠ” ë‹¨ì¼ ë°œí™”ì˜ ëª¨ë“  frameì—ì„œ frame-levelì˜ íŠ¹ì§•ì„ ëª¨ë‘ í‰ê·  ë‚´ì–´ utterance-levelì˜ íŠ¹ì§•ì„ í˜•ì„±&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ì œì•ˆí•˜ëŠ” ë°©ë²•ì€ attention mechanismì„ ì‚¬ìš©í•˜ì—¬ ê° frameë§ˆë‹¤ ë‹¤ë¥¸ weight(ê°€ì¤‘ì¹˜)ë¥¼ ë¶€ì—¬í•˜ê³ , weighted mean(ê°€ì¤‘ í‰ê· )ê³¼ weighted standard deviations(ê°€ì¤‘ í‘œì¤€ í¸ì°¨)ë¥¼ ìƒì„±&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;âœ”  &lt;span style=&quot;background-color:#FFE49B&quot;&gt;NISE SRE 2012 ë° VoxCeleb data setì—ì„œ ê¸°ì¡´ ë°©ë²•ì— ë¹„í•´ EERì´ ê°ê° 7.5%, 8.1% ê°ì†Œ&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;-introduction&quot;&gt;ğŸ“Œ &lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;í™”ì ì¸ì‹ì€ ì§€ë‚œ 10ë…„ë™ì•ˆ i-vector paradigmê³¼ ì§„í™”&lt;/strong&gt;í•˜ì˜€ê³ , i-vectorëŠ” ê³ ì •ëœ ì €ì°¨ì›ì˜ íŠ¹ì§• ë²¡í„° í˜•íƒœë¡œ ìŒì„± ë°œí™” í˜¹ì€ í™”ìë¥¼ í‘œí˜„&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ë‹¤ì–‘í•œ ê¸°ê³„í•™ìŠµì„ í†µí•´ Deep learningì´ ì„±ëŠ¥ í–¥ìƒì— í¬ê²Œ ê¸°ì—¬í•˜ë©°, í™”ì ì¸ì‹ì„ ìœ„í•œ íŠ¹ì§• ì¶”ì¶œì— Deep learningì„ ë„ì…ì´ ì¦ê°€&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ì´ˆê¸° ì—°êµ¬ì—ì„œëŠ” ASR(Automatic Speech Recognition)ì˜ ìŒí–¥ ëª¨ë¸ì—ì„œ ë„ì¶œëœ DNNì„ UBMìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ê¸°ì¡´ì˜ GMMê¸°ë°˜ UBMë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ì§€ë§Œ ì–¸ì–´ ì˜ì¡´ì„± ë‹¨ì ê³¼ í›ˆë ¨ì„ ìœ„í•´ ìŒì†Œ transcriptionì´ í•„ìš”&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ìµœê·¼ &lt;strong&gt;DNNì€ ì´ëŸ¬í•œ i-vector frameworkì™€ ë…ë¦½ì &lt;/strong&gt;ìœ¼ë¡œ &lt;strong&gt;í™”ì ë§ˆë‹¤ ê³ ìœ í•œ íŠ¹ì§• ë²¡í„°ë¥¼ ì¶”ì¶œí•˜ëŠ”ë° ìœ ìš©&lt;/strong&gt;í•˜ë‹¤ê³  ë°í˜€ì§ (íŠ¹íˆ, ì§§ì€ ë°œí™” ì¡°ê±´ì—ì„œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì„)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Text-dependent(ë¬¸ì¥ ì¢…ì† : ë°œí™” ë‚´ìš©ì´ ë™ì¼í•¨) SVì—ì„œ LSTM(ë§ˆì§€ë§‰ frameì—ì„œ í•˜ë‚˜ì˜ ì¶œë ¥ì„ ê°–ëŠ” êµ¬ì¡°)ì„ ì‚¬ìš©í•˜ì—¬ utterance-levelì˜ íŠ¹ì§•ì„ ì–»ëŠ” End-to-End Neural Networkê¸°ë°˜ì˜ ë°©ë²•ì´ ì œì•ˆë˜ì—ˆìœ¼ë©°, ê¸°ì¡´ì˜ i-vectorë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Text-independent SVëŠ” ì…ë ¥ìœ¼ë¡œ ë‹¤ì–‘í•œ ê¸¸ì´ì˜ ë°œí™”ë¥¼ ê°–ìœ¼ë¯€ë¡œ average pooling layerê°€ ë„ì…ë˜ì–´ frame-levelì˜ í™”ì íŠ¹ì§• ë²¡í„°ë¥¼ ì¼ì •í•œì°¨ì›ì„ ê°–ëŠ” speaker embedding ë²¡í„°ë¥¼ ì–»ìŒ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ëŒ€ë¶€ë¶„ ìµœê·¼ ì—°êµ¬ì—ì„œ DNNì´ i-vectorë³´ë‹¤ ë” ë‚˜ì€ ì •í™•ë„ë¥¼ ê°–ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ë©° Snyder ì™¸ëŠ” average poolingë¥¼ í™•ì¥í•œ statistics pooling (í‰ê·  ë° í‘œì¤€ í¸ì°¨ ê³„ì‚°)ì„ ì±„íƒ&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ê·¸ëŸ¬ë‚˜ ì•„ì§ ì •í™•ë„ í–¥ìƒì— ëŒ€í•œ í‘œì¤€ í¸ì°¨ poolingì˜ íš¨ìœ¨ì„±ì€ ë³´ê³ í•˜ì§€ ì•ŠìŒ&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ìµœê·¼ ë‹¤ë¥¸ ì—°êµ¬ì—ì„œëŠ” ì´ì „ì— ê¸°ê³„ ë²ˆì—­ì—ì„œ ìƒë‹¹í•œ ì„±ëŠ¥ í–¥ìƒì„ ê°€ì ¸ì˜¨ &lt;strong&gt;attention mechanismê³¼ í†µí•©&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;í™”ì ì¸ì‹ì—ì„œë„ ì¤‘ìš”ë„ ê³„ì‚° ì‹œ, speaker embedding ì¶”ì¶œí•˜ëŠ” networkì˜ ì¼ë¶€ë¡œ ì‘ë™í•˜ëŠ” ì‘ì€ attention network ì‚¬ìš©&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ê³„ì‚°ëœ ì¤‘ìš”ë„ëŠ” frame-levelì˜ íŠ¹ì§• ë²¡í„°ì˜ weighted mean ê³„ì‚°í•  ë•Œ ì‚¬ìš©í•˜ì—¬ speaker embeddingì´ ì¤‘ìš”í•œ frameì— ì´ˆì ì„ ë§ì¶¤&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ê·¸ëŸ¬ë‚˜ ì´ì „ ì—°êµ¬ì—ì„œëŠ” ê³ ì • ê¸¸ì´ì˜ text-independent í˜¹ì€ text-dependent í™”ì ì¸ì‹ê³¼ ê°™ì€ ì œí•œëœ ì‘ì—…ì—ì„œë§Œ ìˆ˜í–‰&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;- ë³¸ ë…¼ë¬¸ì—ì„œ attention mechanismìœ¼ë¡œ ê³„ì‚°ëœ ì¤‘ìš”ë„ë¡œ importance-weighted standard deviationê³¼ weighted meanì‚¬ìš©í•œ ìƒˆë¡œìš´ poolingë°©ë²•ì¸ attentive statistics poolingë¥¼ ì œì•ˆ&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ê°€ë³€ ê¸¸ì´ì˜ text-independentí•œ í™˜ê²½ì—ì„œ attentive statisitics poolingì„ ì‚¬ìš©í•˜ëŠ” ì²« ë²ˆì§¸ ì‹œë„ ì´ë©°, ë‹¤ì–‘í•œ pooling layer ë¹„êµë¥¼ í†µí•´ í‘œì¤€ í¸ì°¨ê°€ í™”ì íŠ¹ì„±ì— ë¯¸ì¹˜ëŠ” íš¨ê³¼ë¥¼ ì‹¤í—˜ì ìœ¼ë¡œ ë³´ì—¬ì¤Œ&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;-deep-speaker-embedding&quot;&gt;ğŸ“Œ &lt;strong&gt;Deep speaker embedding&lt;/strong&gt;&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;ê¸°ì¡´ì˜ DNNì„ ì‚¬ìš©í•œ speaker embedding ì¶”ì¶œ ë°©ë²•&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;input : acoustic feature (MFCC, filter-bank ë“±)&lt;br /&gt;
frame-levelì˜ íŠ¹ì§• ì¶”ì¶œì„ ìœ„í•´ TDNN, CNN, LSTM ë“±ì˜ Neural Network&lt;br /&gt;
ê°€ë³€ ê¸¸ì´ì˜ frame-level íŠ¹ì§•ì„ ê³ ì • ì°¨ì›ì˜ ë²¡í„°ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•œ pooling layer&lt;br /&gt;
utterance-levelì˜ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ê¸° ìœ„í•œ fully-connected layer(hidden layer ì¤‘ í•˜ë‚˜ì˜ node ìˆ˜ë¥¼ ì‘ê²Œ í•˜ì—¬ bottleneck featureë¡œ ì‚¬ìš©)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/89165519-a443f200-d5b3-11ea-8009-d34a68859aa4.png&quot; alt=&quot;img&quot; style=&quot;zoom:60%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;-high-order-pooling-with-attention&quot;&gt;ğŸ“Œ &lt;strong&gt;High-order pooling with attention&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;&amp;lt; Statistics pooling - ê¸°ì¡´ì— ì‚¬ìš©í•˜ë˜ pooling ë°©ë²• &amp;gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;frame-level íŠ¹ì§•ì— ëŒ€í•´ í‰ê· (mean)ê³¼ í‘œì¤€ í¸ì°¨(standard deviation) ê³„ì‚° (âŠ™ : Hadamard ê³±)í•˜ì—¬ concatenation&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/89165568-b160e100-d5b3-11ea-9a93-2a31b6530b2b.png&quot; alt=&quot;img&quot; style=&quot;zoom: 45%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&amp;lt; Attention mechanism &amp;gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ê¸°ê³„ ë²ˆì—­ì—ì„œ ê¸´ ë¬¸ì¥ì˜ ì„±ëŠ¥ ì €í•˜ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ëª¨ë¸ì´ ì¶œë ¥ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•  ë•Œ &lt;strong&gt;íŠ¹ì • ë‹¨ì–´ë¥¼ ì§‘ì¤‘&lt;/strong&gt;í•´ì„œ ë³´ëŠ” ë°©ë²•ì„ ë„ì…&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/89165571-b1f97780-d5b3-11ea-91e3-8fa3f49000fc.png&quot; alt=&quot;img&quot; style=&quot;zoom: 80%;&quot; /&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/89165573-b1f97780-d5b3-11ea-9545-3a591f97f98d.png&quot; alt=&quot;img&quot; style=&quot;zoom: 50%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/89165553-aefe8700-d5b3-11ea-9e0a-c4c8d5fc14a0.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;decoderì˜ &lt;span style=&quot;color:#a5cbf0&quot;&gt;&lt;strong&gt;ì‹œê°„ i(í˜„ì¬)ì—ì„œ hidden state ë²¡í„°&lt;/strong&gt;&lt;/span&gt;ëŠ” &lt;span style=&quot;color:#a5cbf0&quot;&gt;&lt;strong&gt;ì‹œê°„ i-1(ì´ì „)ì˜ hidden state ë²¡í„°&lt;/strong&gt;&lt;/span&gt;ì™€ &lt;span style=&quot;color:#ffaddf&quot;&gt;&lt;strong&gt;ì‹œê°„ i-1(ì´ì „)ì—ì„œ decoderì˜ output&lt;/strong&gt;&lt;/span&gt;, ê·¸ë¦¬ê³  &lt;span style=&quot;color:#7cbfb6&quot;&gt;&lt;strong&gt;ì‹œê°„ i(í˜„ì¬)ì—ì„œì˜ context ë²¡í„°&lt;/strong&gt;&lt;/span&gt;ë¥¼ ì…ë ¥ìœ¼ë¡œ ê³„ì‚°&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/89165558-af971d80-d5b3-11ea-84c7-8f0478e8e680.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;span style=&quot;color:#7cbfb6&quot;&gt;&lt;strong&gt;context ë²¡í„°&lt;/strong&gt;&lt;/span&gt;ëŠ” &lt;strong&gt;ì‹œê°„ iì—ì„œ ì…ë ¥ xì— ëŒ€í•œ ê¸¸ì´ T&lt;/strong&gt; ì „ì²´ì— ëŒ€í•œ &lt;strong&gt;&lt;span style=&quot;color:#f9d877&quot;&gt;encoder hidden state ë²¡í„°&lt;/span&gt;&lt;/strong&gt;ì˜ &lt;strong&gt;ê°€ì¤‘í•©&lt;/strong&gt;ìœ¼ë¡œ ê³„ì‚°&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/89165559-b02fb400-d5b3-11ea-9ad9-a8383a6810d6.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;span style=&quot;color:#33558c&quot;&gt;&lt;strong&gt;ì‹œê°„ iì—ì„œ jë²ˆì§¸ ë‹¨ì–´ì˜ energy&lt;/strong&gt;&lt;/span&gt;ëŠ” &lt;strong&gt;&lt;span style=&quot;color:#a5cbf0&quot;&gt;ì‹œê°„ i-1(ì´ì „)ì—ì„œ decoder hidden state&lt;/span&gt;&lt;/strong&gt;ì™€&lt;strong&gt;&lt;span style=&quot;color:#f9d877&quot;&gt;Â jë²ˆì§¸ encoder hidden state&lt;/span&gt;&lt;/strong&gt;ê°€ ì…ë ¥ì¸ &lt;strong&gt;aligment model(a)&lt;/strong&gt; ê²°ê³¼ê°’ (alignment modelì€ tanh, ReLU ë“± activation function)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/89165560-b02fb400-d5b3-11ea-8753-68026664a442.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&amp;lt; Attentive statistics pooling &amp;gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/89165563-b0c84a80-d5b3-11ea-9590-62c129a447e4.png&quot; alt=&quot;img&quot; style=&quot;zoom: 50%;&quot; /&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/89165564-b0c84a80-d5b3-11ea-8a2f-c887055c76d8.png&quot; alt=&quot;img&quot; style=&quot;zoom: 50%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;attention mechanismì„ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°í•œ &lt;strong&gt;ê°€ì¤‘ì¹˜ë¥¼ í†µí•´ meanê³¼ standard deviationì„ ê°±ì‹ &lt;/strong&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46676700/89165566-b160e100-d5b3-11ea-9625-41ccb0db4353.png&quot; alt=&quot;img&quot; style=&quot;zoom: 67%;&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;-experimental-settings&quot;&gt;ğŸ“Œ &lt;strong&gt;Experimental settings&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;i-vector&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;input : 60ì°¨ì› MFCC&lt;br /&gt;
UBM : 2048 mixture&lt;br /&gt;
TV matrix, i-vector : 400ì°¨ì›&lt;br /&gt;
Similarity score : PLDA&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Deep speaker embedding&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;input : 20ì°¨ì›(SRE 12), 40ì°¨ì›(VoxCeleb) MFCC&lt;br /&gt;
hidden layer : 5-layer TDNN(activation function : ReLU, node : 512)&lt;br /&gt;
pooling dimension : 1500ì°¨ì›&lt;br /&gt;
acoustic feature vector(MFCC) 15ê°œ frameìœ¼ë¡œ frame-level íŠ¹ì§• ìƒì„±&lt;br /&gt;
2 fully-connected layer (1st : bottleneck feature - 512, activation function : ReLU, batch   normalization)&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Wed, 01 May 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2019-05-01-Attentive-Statistics-Pooling-for-Deep-Speaker-Embedding</link>
        <guid isPermaLink="true">http://localhost:4000/2019-05-01-Attentive-Statistics-Pooling-for-Deep-Speaker-Embedding</guid>
        
        
        <category>review</category>
        
      </item>
    
  </channel>
</rss>

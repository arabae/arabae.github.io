---
layout: post
title: "Conformer: Convolution-augmented Transformer for Speech Recognition"
date: 2021-08-03
category: review
thumbnail: /style/image/conformer.png
use_math: true
icon: book
---

* contents
{:toc}

<span style="font-size:13pt">Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, Ruoming Pang</span>

# **_Abstract_**

ìµœê·¼ Transformer ë° Convolution neural network(CNN) ê¸°ë°˜ ëª¨ë¸ì€ Automatic Speech Recognition(ASR)ì—ì„œ Recurrent neural networks (RNNs)ë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ì•„ ê¸°ëŒ€ë˜ëŠ” ê²°ê³¼ë¥¼ ë³´ì„

Transformer ëª¨ë¸ì€ content-based global interactionì„ ì˜ í¬ì°©í•˜ëŠ” ë°˜ë©´ CNNì€ local featureë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•¨
- parameter-efficient ë°©ì‹ìœ¼ë¡œ audio sequenceì˜ local ë° global dependencyë¥¼ ëª¨ë‘ ëª¨ë¸ë§í•˜ê¸° ìœ„í•´ CNNê³¼ Transformerë¥¼ ê²°í•©í•˜ëŠ” ë°©ë²•ì„ ì—°êµ¬í•˜ì—¬ ë‘ ì„¸ê³„ì˜ ì¥ì ì„ ëª¨ë‘ ë‹¬ì„±

**â‡’ Conformerë¼ëŠ” ìŒì„± ì¸ì‹ì„ ìœ„í•œ Convolution-Augmented Transformerë¥¼ ì œì•ˆ**

ConformerëŠ” SOTA ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ëŠ” ì´ì „ Transformer ë° CNN ê¸°ë°˜ ëª¨ë¸ë³´ë‹¤ í›¨ì”¬ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ê°€ì ¸ì˜´

LibriSpeech ë²¤ì¹˜ë§ˆí¬ ì‚¬ìš©
- WER 2.1% / 4.3% (language model X) - test/testother
- WER 1.9% / 3.9% (language model O)
- WER 2.7% / 6.3% (small model, only 10M parameter)

# **_1. Introduction_**

NNê¸°ë°˜ì˜ End-to-End ASR systemì€ ìµœê·¼ ëª‡ ë…„ ë™ì•ˆ í¬ê²Œ ê°œì„ ë¨

RNNì€ audio sequenceì˜ temproal dependencyë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ëª¨ë¸ë§í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ASRì— ëŒ€í•´ ì‚¬ì‹¤ìƒ ì¼ë°˜ì ì¸ ì„ íƒ

ìµœê·¼ self-attentionì— ê¸°ë°˜ì˜ **transformer êµ¬ì¡°**ëŠ” **long distance interactionì„ capture**í•˜ëŠ” ëŠ¥ë ¥ê³¼ **high training efficiency**ë¡œ sequence ëª¨ë¸ë§ì— ì£¼ë¡œ ì‚¬ìš©ë¨

ë”ë¶ˆì–´, CNNë„ **local receptive field layer**ë¥¼ í†µí•´ **ì ì§„ì ìœ¼ë¡œ local contextë¥¼ capture**í•˜ì—¬ ASRì—ì„œë„ ì„±ê³µì 

ê·¸ëŸ¬ë‚˜ self-attention ë˜ëŠ” CNN ëª¨ë¸ì€ ê°ê° í•œê³„ì ì´ ì¡´ì¬

> ***Transformers***

- long-range global context patternì— íš¨ê³¼ì 
- ì„¸ë¶„í™”ëœ local feature patternì„ ì¶”ì¶œí•˜ëŠ” ëŠ¥ë ¥ì€ ë–¨ì–´ì§

> ***CNN***

- local ì •ë³´ë¥¼ í™œìš©í•˜ê³ , visionì—ì„œ ì‚¬ì‹¤ìƒ computational blockìœ¼ë¡œ ì‚¬ìš©ë¨
- [translation equivariance](#further-reading)ë¥¼ ìœ ì§€í•˜ê³  edgeì™€ shapeê³¼ ê°™ì€ featureë¥¼ captureí•  ìˆ˜ ìˆëŠ” local windowë¥¼ í†µí•´ shared position-based kernelì„ í•™ìŠµ
- local connectivityë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ global informationì„ captureí•˜ê¸° ìœ„í•´ì„  ë” ë§ì€ layerì™€ parameterê°€ í•„ìš”í•˜ë‹¤ëŠ” ì œí•œì´ ì¡´ì¬

ì´ëŸ¬í•œ ë¬¸ì œì ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ë™ì‹œì— ì—°êµ¬ëœ **contextnet**ì€ ë” ê¸´ contextë¥¼ capture í•˜ê¸° ìœ„í•´ **ê° residual blockì— squeeze-and-excitation moduleì„ ë‘š**
- ê·¸ëŸ¬ë‚˜ ì „ì²´ sequenceì— ëŒ€í•´ **global averageë§Œ ì ìš©**í•˜ê¸° ë•Œë¬¸ì— **dynamicí•œ global context**ë¥¼ captureí•˜ê¸°ì—” ì—¬ì „íˆ **ì œí•œì **ì„

ìµœê·¼ ì—°êµ¬ì— ë”°ë¥´ë©´ CNNê³¼ self-attentionì„ ê²°í•©í•˜ë©´ ê°œë³„ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤ í–¥ìƒë˜ì—ˆìŒ

- position-wise local featureë¥¼ ëª¨ë‘ í•™ìŠµí•˜ê³  content-based global interactionì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ
- ë™ì‹œì— [15, 16]ê³¼ ê°™ì€ ë…¼ë¬¸ì€ equivarianceì„ ìœ ì§€í•˜ëŠ” ìƒëŒ€ì  ìœ„ì¹˜ ê¸°ë°˜ ì •ë³´ë¡œ self-attentionì„ ê°•í™”í•¨
- Wu et al. [17]ì€ ì…ë ¥ì„ self-attentionê³¼ convolutionì˜ ë‘ ê°€ì§€ branchë¡œ ë¶„í• í•˜ê³  ì¶œë ¥ì„ ì—°ê²°í•˜ëŠ” multi-branch architectureë¥¼ ì œì•ˆ
    - ì´ taskëŠ” mobile applicationì„ ëŒ€ìƒìœ¼ë¡œ í–ˆìœ¼ë©°, machine translation taskì˜ ê°œì„ ì„ ë³´ì—¬ì¤Œ

<center><img src="https://user-images.githubusercontent.com/46676700/128826541-f87104f7-5b5e-41c9-9081-29db15b294bf.png" alt="img" style="zoom:40%;"/></center>

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ASRì—ì„œ CNNê³¼ self-attentionì„ ìœ ê¸°ì (organically)ìœ¼ë¡œ ê²°í•©í•˜ëŠ” ë°©ë²•ì„ ì—°êµ¬
globalê³¼ local interactionì´ parameter íš¨ìœ¨ì„±ì„ ìœ„í•´ ì¤‘ìš”í•˜ë‹¤ê³  ê°€ì •
â†’ ì´ë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ self-attentionê³¼ convolutionì˜ ìƒˆë¡œìš´ ì¡°í•©ì´ ë‘ê°œì˜ ì¥ì ì„ ëª¨ë‘ ë‹¬ì„±í•  ê²ƒì´ë¼ê³  ì œì•ˆ

self-attentionì€ global interationì„ í•™ìŠµí•˜ëŠ” ë°˜ë©´ convolutionì€ relative-offset-based local correlationë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ captureí•¨
- Wu et al. [17, 18],ì—ì„œ ì˜ê°ì„ ë°›ì•˜ê³ , ê·¸ë¦¼ 1ê³¼ ê°™ì´ í•œ ìŒì˜ feedforward module ì‚¬ì´ì— ë¼ì›Œì§„ self-attentionê³¼ convolutionì˜ ìƒˆë¡œìš´ ì¡°í•©ì„ ì†Œê°œ!

> ***Conformer***

ì´ì „ SOTA Transformer Transducer[7]ì™€ ë¹„êµ
- LibriSpeech dataset ì‚¬ìš© (ì™¸ë¶€ language modelì´ ìˆëŠ” testother ë°ì´í„° ì…‹ì—ì„œ ìƒëŒ€ì ìœ¼ë¡œ 15% í–¥ìƒ)

10M, 30M, 118M parameter í¬ê¸°ë¥¼ ê°–ëŠ” ëª¨ë¸ ë¹„êµ
- 10M: test/testotherì—ì„œ 2.7%/6.3%ë¡œ ìœ ì‚¬í•œ í¬ê¸°ì˜ ë‹¤ë¥¸ ëª¨ë¸[10]ê³¼ ë¹„êµí–ˆì„ ë•Œ ê°œì„ ë¨
- 30M: 139M parameterë¥¼ ì‚¬ìš©í•˜ëŠ” transformer transducer[7]ë³´ë‹¤ ê°œì„ ë¨
- 118M: ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  2.1%/4.3%, ì‚¬ìš©í•˜ë©´ 1.9%/3.9% ì„±ëŠ¥ì„ ë³´ì„

â• attention head ìˆ˜, convolution kernel size, activation fuction, feedforward layer ë°°ì¹˜, convolution moduleì„ transformerê¸°ë°˜ networkì— ì¶”ê°€í•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²•ì˜ íš¨ê³¼ì— ëŒ€í•´ ê¹Šì´ ì—°êµ¬í•˜ê³ , ê°ê°ì´ ì–´ë–»ê²Œ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚¤ëŠ”ì§€ ì´ˆì ì„ ë‘š

<img src="https://user-images.githubusercontent.com/46676700/128826558-cf9ff480-0a20-4313-804d-569ac4c39e3e.png" alt="img" style="zoom:60%;"/>

# *2. ConformerEncoder*

audio encoderëŠ” ë¨¼ì € convolution subsampling layerì„ ì‚¬ìš©í•´ ì…ë ¥ì„ ì²˜ë¦¬í•˜ê³ , ë‹¤ìŒì— fig1ê³¼ ê°™ì´ ì—¬ëŸ¬ conformer blockì„ ê±°ì¹¨

ë³¸ ë…¼ë¬¸ modelì˜ êµ¬ë³„ë˜ëŠ” íŠ¹ì§•ì€ [7, 19]ì—ì„œ transformer block ë¶€ë¶„ì´ conformer blockìœ¼ë¡œ ì‚¬ìš©ë¨

conformer blockì€ 4ê°œì˜ module(feed-forward module, self-attention module, convolution module, second feed-forward module)ì´ í•¨ê»˜ ìŒ“ì—¬ êµ¬ì„±ë¨

section 2.1, 2 and 2.3ì—ì„œëŠ” ê°ê° self-attention, convolution, feed-forward moduleì„ ì†Œê°œí•˜ê³ , ë§ˆì§€ë§‰ìœ¼ë¡œ 2.4ì—ì„œëŠ” ì´ëŸ¬í•œ í•˜ìœ„ blockì´ ì–´ë–»ê²Œ ê²°í•©ë˜ëŠ”ì§€ ì„¤ëª…

### 2.1. Multi-Headed Self-Attention Module

relative sinusoidal(sin ê³¡ì„ ) positional encoding ë°©ì‹ì¸ Transformer-XLì˜ ì¤‘ìš”í•œ ê¸°ìˆ ì„ í†µí•©í•˜ë©´ì„œ multi-head self-attention (MHSA)ë¥¼ ì‚¬ìš©

**ğŸ’¡ relative positional encoding**  
- self-attention moduleì´ ë‹¤ë¥¸ ì…ë ¥ ê¸¸ì´ì— ëŒ€í•´ ë”ìš± ì˜ ì¼ë°˜í™”í•  ìˆ˜ ìˆë„ë¡ í•¨
- resulting encoderëŠ” ë°œí™” ê¸¸ì´ì˜ ë³€í™”ì— ëŒ€í•´ ë” ê°•ì¸í•¨

ë” ê¹Šì€ ëª¨ë¸ì„ í›ˆë ¨í•˜ê³ , ì •ê·œí™”í•˜ëŠ”ë° ë„ì›€ì´ ë˜ëŠ” dropoutê³¼ í•¨ê»˜ pre-norm residual unitì„ ì‚¬ìš©í•¨

ì•„ë˜ì˜ ê·¸ë¦¼ 3ì€ multi-head self-attention module blockì„ ë‚˜íƒ€ëƒ„

<center><img src="https://user-images.githubusercontent.com/46676700/128826564-520cebdc-c97e-45b1-8349-2842c44f6ca0.png" alt="img" style="zoom:40%;"/></center>

### 2.2. Convolution Module

[17]ì—ì„œ ì˜ê°ì„ ë°›ì•„ convolution moduleì€ pointwise convolutionê³¼ gated linear unit(glu)ì¸ gating mechanismìœ¼ë¡œ ì‹œì‘

ê·¸ ë‹¤ìŒ 1D depthwise convolution layerê°€ ì´ì–´ì§€ê³ , Batchnormì€ deep ëª¨ë¸ í›ˆë ¨ì„ ë•ê¸° ìœ„í•´ convolution ì§í›„ì— ìœ„ì¹˜í•¨

ê·¸ë¦¼ 2ëŠ” convolution blockì„ ë‚˜íƒ€ëƒ„

<img src="https://user-images.githubusercontent.com/46676700/128827668-4697e2e9-3d33-49e7-9968-28a8af2a70e8.png" alt="img" style="zoom:60%;"/>

### 2.3. FeedForward Module

[6]ì—ì„œ ì œì•ˆëœ Transformer êµ¬ì¡°ëŠ” MHSA layer ì´í›„ feed-forward moduleì´ ì´ì–´ì§€ê³ , two linear transformation ì‚¬ì´ì— nonlinear activationì´ ì¡´ì¬í•¨

residual connectiondms feed-forward layer ìœ„ì— ì¶”ê°€ë˜ê³  layer normalizationì´ ì´ì–´ì§

ì´ êµ¬ì¡°ëŠ” Transformer ASR model [7, 24]ì—ë„ ì ìš©ë¨

<img src="https://user-images.githubusercontent.com/46676700/128826571-ee6a4944-a20c-4625-98ba-99df6b0fc53c.png" alt="img" style="zoom:60%;"/>

pre-norm residual unit[21, 22]ì„ ë”°ë¥´ê³ , residual unitì•ˆì— ì²« ë²ˆì§¸ linear layer ì´ì „ ì…ë ¥ì—ì„œ layer normalizationì„ ì ìš©í•¨

ë˜í•œ, Swish activation ë° dropoutì„ ì ìš©í•˜ì—¬ networkë¥¼ ì •ê·œí™”í•˜ëŠ”ë° ë„ì›€ì„ ì¤Œ

ê·¸ë¦¼ 4ëŠ” Feed-Forward Network(FFN) moduleì„ ë‚˜íƒ€ëƒ„

### 2.4. Conformer Block

ì œì•ˆí•œ conformer blockì—ëŠ” ê·¸ë¦¼ 1ê³¼ ê°™ì´ **multi-head self-attention moduleê³¼ convolution module ì‚¬ì´ì— 2ê°œì˜ feed-forward module**ì´ í¬í•¨ë¨  
- ì´ ìƒŒë“œìœ„ì¹˜ êµ¬ì¡°ëŠ” transformer blockì˜ ì›ë˜ feed-forward layerë¥¼ 2ê°œì˜ half-step feed-forwar layer(attention layer ì „ í›„ë¡œ ë°°ì¹˜)ë¡œ ëŒ€ì²´í•œ Macaron-Net[18]ì—ì„œ ì˜ê°ì„ ì–»ì—ˆìŒ
- Macron-Netì—ì„œì™€ ê°™ì´ ë³¸ ë…¼ë¬¸ì˜ feed-forward layerì—ì„œ half-step residual weightë¥¼ ì‚¬ìš©í•¨

ë‘ë²ˆì§¸ feed-forward module ë‹¤ìŒì— ìµœì¢… layernorm layerê°€ ì˜´

ìˆ˜í•™ì ìœ¼ë¡œ conformer block iì— ëŒ€í•œ ì…ë ¥ $x_i$ì— ëŒ€í•´ blockì˜ ì¶œë ¥ $y_i$ê°€ ë‹¤ìŒê³¼ ê°™ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•¨

$\tilde{x_i} = x_i + \frac{1}{2}FFN(x_i)$
$x'_i = \tilde{x_i} + MHSA(\tilde{x_i})$

$x''_i = x'_i + Conv(x'_i)$

$y_i = Layernorm(x''_i + \frac{1}{2}FFN(x''_i))$

section 3.4.3ì—ì„œ ì´ì „ ì‘ì—…ì—ì„œ ì‚¬ìš©ëœ **vanilla FFNê³¼ Macron-styleì˜ half-step FFNì„ ë¹„êµ**í•¨

- 2ê°œì˜ macaron-net style feed-forward layer ì‚¬ì´ì— attention moduleê³¼ convolution moduleì„ ë¼ì›Œë„£ëŠ” half-step residual connectionì´ ìˆëŠ”ê²Œ conformer architectureì—ì„œ ë‹¨ì¼ feed-forward moduleì„ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤ **ìƒë‹¹íˆ ê°œì„ **ëœë‹¤ëŠ” ê²ƒì„ ë°œê²¬í•¨

convolutionê³¼ self-attentionì˜ ì¡°í•©ì€ ì´ì „ì— ì—°êµ¬ë˜ì—ˆìœ¼ë©° ì´ë¥¼ ë‹¬ì„±í•˜ëŠ” ë§ì€ ë°©ë²•ì„ ìƒìƒí•  ìˆ˜ ìˆì—ˆìŒ
self-attentionìœ¼ë¡œ convolutionì„ ì¦ê°€ì‹œí‚¤ëŠ” ë‹¤ì–‘í•œ ì˜µì…˜ì€ section 3.4.2ì— ì‘ì„±

â‡’ **self-attention module ë’¤ì— ìŒ“ì¸ convolution module**ì´ ìŒì„± ì¸ì‹ì— ê°€ì¥ ì˜ ì‘ë™í•˜ëŠ” ê²ƒì„ ë°œê²¬

# *3. Experiments*

### 3.1 Data

970ì‹œê°„ labeled speechì™€ language model êµ¬ì¶•ì„ ìœ„í•œ ì¶”ê°€ 800M word token textì „ìš© corpusë¡œ êµ¬ì„±ëœ LibriSpeech datasetì—ì„œ ì œì•ˆëœ ëª¨ë¸ì„ í‰ê°€
- 25ms window, 10ms stride
- 80-channel filterbank feature

SpecAugment [27, 28] with mask parameter (F=27)ì™€ ìµœëŒ€ time-mask ratio(ps=0.05)ë¥¼ ê°€ì§„ 10ê°œ time mask ì‚¬ìš©
- time msakì˜ ìµœëŒ€ sizeëŠ” ë°œí™” ê¸¸ì´ * psë¡œ ì„¤ì •

### 3.2 Conformer Tranducer

network ê¹Šì´, model dimension, attention head ìˆ˜ì˜ ë‹¤ì–‘í•œ ì¡°í•©ì„ ìŠ¤ìœ„í•‘í•˜ê³ , model parameter size ì œì•½ ë‚´ì—ì„œ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸ì„ ì„ íƒí•´ 10M, 30M, 118M  parameterë¥¼ ì‚¬ìš©í•˜ì—¬ ì†Œ, ì¤‘, ëŒ€ ì„¸ê°€ì§€ ëª¨ë¸ì„ ì‹ë³„
- ëª¨ë“  ëª¨ë¸ì—ì„œ single-LSTM layer decoderë¥¼ ì‚¬ìš©

í‘œ 1ì€ architecture hyperparameterë¥¼ ë³´ì—¬ì¤Œ

<img src="https://user-images.githubusercontent.com/46676700/128827078-593e8915-0585-42e0-b603-f3974ec64f4d.png" alt="img" style="zoom:60%;"/>

- **dropout**: module ì…ë ¥ì— ì¶”ê°€ë˜ê¸° ì „ì— conformerì˜ ê° residual unit, ì¦‰ ê° moduleì˜ ì¶œë ¥ì— ì ìš© (ë¹„ìœ¨ $P_{drop}$ = 0.1)
- **Variational noise**[5, 30]
- **L2 regularization**: 1e-6 weight (ëª¨ë“  í•™ìŠµ ê°€ëŠ¥í•œ wightì— ì¶”ê°€)
- **Adam** optimizer(Î²1 = 0.9, Î²2 = 0.98, Îµ = 10âˆ’9)
- **transformer** **learning rate schedule** (10k warm-up step, ìµœëŒ€ learning rate $\frac{0.05}{\sqrt{d}}$ (d: model dimension)
- **3-layer LSTM LM** (width 4096)
  - LibriSpeech 960hì—ì„œ êµ¬ì¶•ëœ 1k Words Per Minute(WPM)ìœ¼ë¡œ tokenized LibriSpeech960h transcriptê°€ ì¶”ê°€ëœ LibriSpeech language model corpusì—ì„œ í›ˆë ¨
  - LMì€ dev-set transcriptì˜ word-level perplexity(í˜¼ë€ë„)ê°€ 63.9
  - shallow fusionì— ëŒ€í•œ LM weigth Î»ëŠ” grid searchë¥¼ í†µí•´ dev-setì—ì„œ ì¡°ì •

â‡’ ëª¨ë“  ëª¨ë¸ì€ **Lingvo toolkit**ìœ¼ë¡œ êµ¬í˜„

### 3.3 Results on LibriSpeech

<img src="https://user-images.githubusercontent.com/46676700/128827091-238c5479-203d-4918-b555-655df0c6614a.png" alt="img" style="zoom:60%;"/>

í‘œ 2ëŠ” LibriSpeech test-clean/test-otherì— ëŒ€í•œ ëª¨ë¸ì˜ WER ê²°ê³¼ë¥¼ ContextNet, Transformer transducer ë° QuartzNetì„ í¬í•¨í•œ ëª‡ ê°€ì§€ ìµœì‹  ëª¨ë¸ê³¼ ë¹„êµ
- ëª¨ë“  í‰ê°€ ê²°ê³¼ëŠ” ì†Œìˆ˜ì  ì´í•˜ 1ìë¦¬ë¡œ ë°˜ì˜¬ë¦¼

**ì–¸ì–´ ëª¨ë¸ X**  
- ì¤‘ê°„ ëª¨ë¸ì˜ ì„±ëŠ¥ì€ test/testotherì—ì„œ ì´ë¯¸ ê°€ì¥ ì˜ ì•Œë ¤ì§„ Transformer, LSTM ê¸°ë°˜ ëª¨ë¸ ë˜ëŠ” ìœ ì‚¬í•œ í¬ê¸°ì˜ convolution ëª¨ë¸ì„ ëŠ¥ê°€í•˜ëŠ” 2.3/5.0ë¡œ ê²½ìŸë ¥ ìˆëŠ” ê²°ê³¼ë¥¼ ë‹¬ì„±

**ì–¸ì–´ ëª¨ë¸ O**  
- ëª¨ë“  ê¸°ì¡´ ëª¨ë¸ ì¤‘ ê°€ì¥ ë‚®ì€ WER
- single NNì—ì„œ Transformerì™€ convolutionì„ ê²°í•©í•˜ëŠ” ê²ƒì˜ íš¨ìœ¨ì„±ì„ ë¶„ëª…íˆ ë³´ì—¬ì¤Œ

### 3.4 Ablation Studies

> ***3.4.1. Conformer Block vs Transformer Block***

Conformer blockì€ ì—¬ëŸ¬ ë°©ë©´ì—ì„œ Transformer blockê³¼ ë‹¤ë¦„

íŠ¹íˆ, macaron-styleì˜ convolution blockê³¼ ì´ë¥¼ ë‘˜ëŸ¬ì‹¼ FFN pairê°€ ì¡´ì¬
â‡’ ì´ parameter ìˆ˜ë¥¼ ë³€ê²½í•˜ì§€ ì•Šê³ , conformer blockì„ transformer blockìœ¼ë¡œ ë³€ê²½í•˜ì—¬ ì°¨ì´ë¥¼ í™•ì¸

í‘œ 3ëŠ” conformer blockì— ëŒ€í•œ ê° ë³€í˜•ì˜ ì˜í–¥ì„ ë‚˜íƒ€ëƒ„

<img src="https://user-images.githubusercontent.com/46676700/128827098-dde8d71e-599e-405c-b83e-ad70b5fe9e0e.png" alt="img" style="zoom:60%;"/>

ëª¨ë“  ì°¨ì´ì  ì¤‘ì—ì„œ **convolution sub-block**ì´ ê°€ì¥ ì¤‘ìš”í•œ featureì´ì§€ë§Œ macaron-styleì˜ FFN pairë¥¼ ê°–ëŠ” ê²ƒì´ ë™ì¼í•œ ìˆ˜ì˜ parameterë¥¼ ê°–ëŠ” single FFNë³´ë‹¤ ë” íš¨ê³¼ì 

swish activationì„ ì‚¬ìš©í•˜ë©´ Conformer ëª¨ë¸ì—ì„œ ë” ë¹ ë¥¸ ìˆ˜ë ´ì´ ì´ë£¨ì–´ì§

> ***3.4.2 Combinations of Convolution and Transformer Modules***

MHSA moduleê³¼ convolution moduleì„ ê²°í•©í•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²•ì˜ íš¨ê³¼ë¥¼ ì—°êµ¬

1. convolution moduleì˜ depthwise convolutionì„ lightweight convolution[35]ìœ¼ë¡œ êµì²´ ì‹œë„
- íŠ¹íˆ, dev-other datasetì—ì„œ ì„±ëŠ¥ì´ í¬ê²Œ ë–¨ì–´ì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŒ
1. Conformer ëª¨ë¸ì—ì„œ MHSA module ì•ì— convolution moduleì„ ë°°ì¹˜
- dev-otherì—ì„œ 0.1ë§Œí¼ ê²°ê³¼ê°€ ì €í•˜ì‹œí‚¤ëŠ” ê²ƒì„ ë°œê²¬
1. [17]ì—ì„œ ì œì•ˆí•œ ëŒ€ë¡œ outputì´ ì—°ê²°ëœ multi-head self-attention moduleê³¼ convolution moduleì˜ parallel branchë¡œ inputì„ ë¶„í• 
- ì œì•ˆí•œ architectureì™€ ë¹„êµí•  ë•Œ ì„±ëŠ¥ì„ ì•…í™”ì‹œí‚¨ë‹¤ëŠ” ê²ƒì„ ë°œê²¬

â‡’ í‘œ 4ëŠ” Conformer blockì—ì„œ self-attention module ë’¤ì— convolution moduleì„ ë°°ì¹˜í•˜ëŠ” ì´ì ì„ ì‹œì‚¬í•¨

<img src="https://user-images.githubusercontent.com/46676700/128826577-dfee6a64-4c88-426e-8652-9ea83a8a39de.png" alt="img" style="zoom:60%;"/>

> ***3.4.3. Macaron Feed Forward Modules***

Transformer ëª¨ë¸ì—ì„œì™€ ê°™ì´ attention block ì´í›„ single FFN ëŒ€ì‹  Conformer blockì—ëŠ” self-attention ë° convolution module ì‚¬ì´ì— macaronê³¼ ê°™ì€ í•œ ìŒì˜ feed-forward moduleì´ ìˆìŒ

ë˜í•œ, Conformer feed-forward moduleì€ half-step residuleê³¼ í•¨ê»˜ ì‚¬ìš©ë¨

í‘œ 5ëŠ” single FFN ë˜ëŠ” ì „ì²´ full-step residualì„ ì‚¬ìš©í•´ Conformer blockì„ ë³€ê²½í•  ë•Œ ê²°ê³¼ë¥¼ ë‚˜íƒ€ëƒ„  
- ì°¨ì´ê°€ ë§ì´ ì—†ì§€ë§Œ, macaron style feed-forward moduleì´ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„

<img src="https://user-images.githubusercontent.com/46676700/128827103-fde4055b-51d1-48d7-a372-6e8e0624c306.png" alt="img" style="zoom:60%;"/>

> ***3.4.4. Number of Attention Heads***

self-attentionì—ì„œ ê° attention headëŠ” ì…ë ¥ì˜ ë‹¤ë¥¸ ë¶€ë¶„ì— ì´ˆì ì„ ë§ì¶”ì–´ í•™ìŠµí•˜ì—¬ ë‹¨ìˆœí•œ weighted average ì´ìƒìœ¼ë¡œ predictë¥¼ ê°œì„ í•  ìˆ˜ ìˆìŒ

large ëª¨ë¸ì—ì„œ ëª¨ë“  layerì—ì„œ 4~32ê¹Œì§€ ë™ì¼í•œ ìˆ˜ì˜ attention headë¥¼ ë³€ê²½í•˜ë©´ì„œ ì‚¬ìš©í•´ íš¨ê³¼ë¥¼ ì—°êµ¬í•˜ê¸° ìœ„í•´ ì‹¤í—˜ì„ ìˆ˜í–‰

í‘œ 6ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ íŠ¹íˆ dev-other datasetì— ëŒ€í•´ attention headë¥¼ ìµœëŒ€ 16ê¹Œì§€ ì¦ê°€ì‹œí‚¤ë©´ ì •í™•ë„ê°€ í–¥ìƒëœë‹¤ëŠ” ê²ƒì„ ë°œê²¬

<img src="https://user-images.githubusercontent.com/46676700/128826587-7f85fcb1-4ec0-4660-8df9-5d54144b5562.png" alt="img" style="zoom:60%;"/>

> ***3.4.5. Ablation study on depthwise convolution kernel sizes***

depthwise convolutionì—ì„œ kernel sizeì˜ ì˜í–¥ì„ ì—°êµ¬í•˜ê¸° ìœ„í•´ ëª¨ë“  layerì— ëŒ€í•´ ë™ì¼í•œ kernel sizeë¥¼ ì‚¬ìš©í•´ large ëª¨ë¸ì—ì„œ kernel sizeë¥¼ {3, 7, 17, 32, 65}ë¡œ ìŠ¤ìœ•í•˜ì—¬ ì‹¤í—˜

kernel size 17ê³¼ 32ê¹Œì§€ sizeê°€ í´ìˆ˜ë¡ ì„±ëŠ¥ì´ í–¥ìƒë˜ì§€ë§Œ, í‘œ 7ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ size 65ì˜ ê²½ìš°ì—ëŠ” ì„±ëŠ¥ì´ ì•…í™”ëœë‹¤ëŠ” ê²ƒì„ ë°œê²¬

dev WERì—ì„œ ì†Œìˆ˜ ë‘˜ì§¸ìë¦¬ë¥¼ ë¹„êµí•˜ë©´ ë¹„êµí•˜ë©´ ë‚˜ë¨¸ì§€ë³´ë‹¤ size 32ê°€ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì„

<img src="https://user-images.githubusercontent.com/46676700/128826596-fede2ba9-ac3d-4a18-b5ac-0c2a6196592e.png" alt="img" style="zoom:60%;"/>

# *4. Conclusion*

ë³¸ ëª¬ë¬¸ì—ì„œëŠ” End-to-End speech recognitionì„ ìœ„í•´ **CNN ë° Transformerì˜ êµ¬ì„± ìš”ì†Œë¥¼ í†µí•©**í•˜ëŠ” architectureì¸ **Conformerë¥¼ ë„ì…**

ê° êµ¬ì„± ìš”ì†Œì˜ ì¤‘ìš”ì„±ì„ ì—°êµ¬í•´ Convolution moduleì„ í¬í•¨í•˜ëŠ” ê²ƒì´ Conformer ì„±ëŠ¥ì— ì¤‘ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤Œ

LibriSpeech datasetì— ëŒ€í•œ ì´ì „ modelë³´ë‹¤ ë” ì ì€ parameterë¡œ í–¥ìƒëœ ì •í™•ë„ë¥¼ ë³´ì„  
- **test/test-otherì— ëŒ€í•´ 1.9%/3.9%ë¡œ SOTA ë‹¬ì„±**

---

### **Further reading**
**ğŸ’¡ translation equivariance**  
[What is translation equivariance, and why do we use convolutions to get it?](https://chriswolfvision.medium.com/what-is-translation-equivariance-and-why-do-we-use-convolutions-to-get-it-6f18139d4c59)

**ğŸ’¡ Transformerì™€ êµ¬ì¡°ì ìœ¼ë¡œ ë¹„êµ**  
[kakaobrain/nl-paper-reading](https://github.com/kakaobrain/nlp-paper-reading/blob/master/notes/conformer.md)

---
layout: post
title: "Attention-based Models For Text-dependent Speaker Verification : REVIEW"
subtitle: "F A Rezaur Rahman Chowdhury, Quan Wang, Ignacio Lopez Moreno, Li Wan"
tags: [Attention, pooling, LSTM,, SpeakerVerification, SpeakerRecognition, research_review]
author: Ara Bae
comments: True
---

### âœ Abstract ğŸ”
- Attention ê¸°ë°˜ ëª¨ë¸ : ì…ë ¥ sequenceì˜ ì „ì²´ ê¸¸ì´ë¥¼ ìš”ì•½í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥
- ìŒì„± ì¸ì‹, ê¸°ê³„ ë²ˆì—­, ì´ë¯¸ì§€ ìº¡ì…˜ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ê³³ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì„
- End-to-End Text-dependent í™”ì ì¸ì‹ ì‹œìŠ¤í…œì—ì„œ attention mechanism ì‚¬ìš©ì„ ë¶„ì„
- ë‹¤ì–‘í•œ attention layerì˜ ë³€í˜•ì„ ì—°êµ¬í•˜ê³  attention weightì— ëŒ€í•œ ë‹¤ì–‘í•œ poolingë°©ë²•ì„ ë¹„êµ
- Attention mechanismì„ ì‚¬ìš©í•˜ì§€ ì•Šì€ LSTMê³¼ ì„±ëŠ¥ ë¹„êµ

<br/>

---

<br/>
### â… . Introduction ğŸ”

**âœ” Global Password Text-dependent Speaker Verification(SV) ì‹œìŠ¤í…œ**

- ë“±ë¡ ë° í…ŒìŠ¤íŠ¸ ë°œí™”ê°€ íŠ¹ì • ë‹¨ì–´ë¡œ ì œí•œ (Text-dependent)
- â€œOk-Googleâ€ê³¼ â€œHey Googleâ€ ì‚¬ìš© ( Global password)


<br/>

**âœ” í˜„ì¬ ê°€ì¥ ë§ì´ ì ‘ê·¼í•˜ê³  ìˆëŠ” í›ˆë ¨ ë°©ë²•**

- ë“±ë¡ ë° í…ŒìŠ¤íŠ¸í•˜ëŠ” ë‹¨ê³„ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” End-to-End êµ¬ì¡°
- [6]ë…¼ë¬¸ â€œi-vector+PLDA ì‹œìŠ¤í…œì„ ê·¸ëŒ€ë¡œ ëª¨ë°©í•œ êµ¬ì¡°â€ì˜ ê²½ìš°, ë” ë‚˜ì€ ì„±ëŠ¥ì„ ìœ„í•´ ëª¨ë¸ì„ ê·œì œí•˜ì˜€ìœ¼ë‚˜ ì´ˆê¸°í™”ë¥¼ ìœ„í•´ ê¸°ì¡´ì˜ i-vectorì™€ PLDA ëª¨ë¸ì´ í•„ìš”
- [7] ë…¼ë¬¸, TD-SV taskì—ì„œ LSTM ë„¤íŠ¸ì›Œí¬ê°€ ê¸°ì¡´ End-to-End DNNë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œ

<br/>

**âœ”ì´ì „ ë…¼ë¬¸ì—ì„œì˜ ë¬¸ì œì **

- ë¬µìŒê³¼ ë°°ê²½ ì¡ìŒì´ ë§ì´ ì—†ìŒ
- ë³¸ ë…¼ë¬¸ì—ì„œëŠ” keyword ê²€ì¶œì— ì˜í•´ ë¶„í• ëœ 800msì˜ ì§§ì€ frameì´ì§€ë§Œ, ë¬µìŒê³¼ ì¡ìŒì´ ìˆìŒ

<br/>

**âœ”ì´ìƒì ì¸ Embedding ìƒì„±**

- ìŒì†Œì— í•´ë‹¹í•˜ëŠ” frameì„ ì‚¬ìš©í•˜ì—¬ ì œì‘
- ì…ë ¥ sequence ì¤‘ ê´€ë ¨ì„±ì´ ë†’ì€ ìš”ì†Œë¥¼ ê°•ì¡°í•˜ê¸° ìœ„í•´ attention layer ì‚¬ìš©

<br/>

---


<br/>

### â…¡. Baseline ArchitectureğŸ”

#### <span style="background-color:#aee4ff">**TE2E model**</span>

**âœ”  baseline end-to-end training architecture**

<center><img src="https://user-images.githubusercontent.com/46676700/94424981-1573e000-01c6-11eb-8bf5-4890542a60db.png" alt="img" style="zoom: 80%;" /></center>

- í›ˆë ¨ ë‹¨ê³„ì—ì„œ, í•˜ë‚˜ì˜ í‰ê°€ìš© ë°œí™” ğ’™ğ‘—~ì™€ Nê°œì˜ ë“±ë¡ ë°œí™” ğ’™ğ‘˜ğ‘› (ğ‘“ğ‘œğ‘Ÿ ğ‘›=1, â€¦, ğ‘) tupleì´ LSTM networkì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©

> {ğ’™ğ‘—~, (ğ’™ğ‘˜1, â€¦, ğ’™ğ‘˜ğ‘)} ; input
> 
> ğ’™ : ê³ ì • ê¸¸ì´ì˜ log-mel fiterbank feature
> 
> ğ‘—, ğ‘˜ : ë°œí™”í•œ í™”ì (jì™€ këŠ” ê°™ì„ ìˆ˜ ìˆìŒ)
> 
> ë§Œì•½ ğ’™ğ‘—~ì™€ ğ‘€ ê°œì˜ ë“±ë¡ ë°œí™”ê°€ ê°™ì€ í™”ìë¼ë©´ tuple positive (ğ‘—=ğ‘˜), ë‹¤ë¥´ë©´ negative

- â„ğ‘¡ : të²ˆì§¸ frameì—ì„œ LSTMì˜ ë§ˆì§€ë§‰ layerì˜ ì¶œë ¥ ( ê³ ì • ì°¨ì›ì˜ vector )
- ë§ˆì§€ë§‰ frameì˜ outputì„ d-vector ğ (â„ğ‘‡) ë¡œ ì •ì˜

> {ğğ‘—~, (ğğ‘˜1, â€¦, ğğ‘˜ğ‘)} ; output
> 
> Tuple (ğğ‘˜1, â€¦, ğğ‘˜ğ‘)ì„ í‰ê· ë‚´ì–´ centroid ê³„ì‚°

<br/>

<center><img src="https://user-images.githubusercontent.com/46676700/94425430-e447df80-01c6-11eb-9148-c79bd11b149b.png" alt="img" style="zoom:80%;" /></center>

<br/>

**âœ”  Cosine Similarity Function ì •ì˜**

<center><img src="https://user-images.githubusercontent.com/46676700/94425434-e611a300-01c6-11eb-990c-2a6bc83ad06b.png" alt="img" style="zoom:80%;" /></center>

<br/>

**âœ”  Loss Function ì •ì˜**

<center><img src="https://user-images.githubusercontent.com/46676700/94425445-e7db6680-01c6-11eb-9729-e41c138555a5.png" alt="img" style="zoom: 80%;" /></center>

<br/>


---


<br/>

### â…¢. Attention-based Model
#### <span style="background-color:#aee4ff">**3.1 Basic attention layer**</span>

**âœ”  Baseline systemê³¼ ì°¨ì´ì **

- ë§ˆì§€ë§‰ frameì˜ ì¶œë ¥ì„ d-vector(ğ)ë¡œ ì§ì ‘ ì‚¬ìš©
- Attention layerëŠ” ê° t frame ì—ì„œì˜ LSTM ì¶œë ¥ â„ğ‘¡ì— ëŒ€í•œ ìŠ¤ì¹¼ë¼ ì ìˆ˜ ğ‘’ğ‘¡ ë¥¼ í›ˆë ¨í•˜ì—¬ weighted sumí•œ ê²°ê³¼ë¡œ d-vector(ğ) ì •ì˜

<center><img src="https://user-images.githubusercontent.com/46676700/94430186-76071b00-01ce-11eb-8ae9-0fdf5abcf182.png" alt="img" style="zoom: 80%;" /></center>

- Normalized weight ğ›¼ğ‘¡ì™€ weighted sumí•œ ê²°ê³¼ d-vectorëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ 

<center><img src="https://user-images.githubusercontent.com/46676700/94430336-ac449a80-01ce-11eb-8094-4fcf8644fec6.png" alt="img" style="zoom: 80%;" /></center>

<center><img src="https://user-images.githubusercontent.com/46676700/94430342-ae0e5e00-01ce-11eb-9395-90efff7c8674.png" alt="img" style="zoom: 80%;" /></center>

<br/>

- **aritectureë¡œ ë³´ëŠ” ì°¨ì´ì **
<center><img src="https://user-images.githubusercontent.com/46676700/94430460-eca41880-01ce-11eb-9807-6a7dea6d97fa.png" alt="img" style="zoom: 80%;" /></center>


<br/>

#### <span style="background-color:#aee4ff">**3.2 Scoring functions**</span>

- Bias-only attention
ì—¬ê¸°ì„œ bğ‘¡ëŠ” scalar. LSTM ì¶œë ¥ hğ‘¡ì— ì˜ì¡´í•˜ì§€ ì•ŠìŒ.

<center><img src="https://user-images.githubusercontent.com/46676700/94430647-34c33b00-01cf-11eb-87f5-e43a51edc41a.png" alt="img" style="zoom: 80%;" /></center>

- Linear attention
ì—¬ê¸°ì„œ wğ‘¡ëŠ” mì°¨ì› vector, bğ‘¡ëŠ” scalar. frameë§ˆë‹¤ ë‹¤ë¥¸ parameterê°€ ì‚¬ìš©

<center><img src="https://user-images.githubusercontent.com/46676700/94430651-368cfe80-01cf-11eb-85a2-d759801a1634.png" alt="img" style="zoom: 80%;" /></center>

- Shared-parameter linear attention
ëª¨ë“  frameì— ëŒ€í•´ mì°¨ì› vector  wì™€ scalar bê°€ ë™ì¼í•˜ê²Œ ì‚¬ìš©

<center><img src="https://user-images.githubusercontent.com/46676700/94430653-37be2b80-01cf-11eb-95d0-af0d4afd142b.png" alt="img" style="zoom: 80%;" /></center>

- Non-linear attention
ì—¬ê¸°ì„œ ğ‘¾ğ’•ëŠ” mâ€™ X m matrix, ğ›ğ‘¡ì™€ ğ¯ğ‘¡ëŠ” mâ€™ì°¨ì›ì˜ vector(ì°¨ì› mâ€™ì€ í›ˆë ¨ ë°ì´í„° ì…‹ì—ì„œ ì¡°ì •)

<center><img src="https://user-images.githubusercontent.com/46676700/94430710-50c6dc80-01cf-11eb-8673-5af3e52f4b04.png" alt="img" style="zoom: 80%;" /></center>

- Shared-parameter non-linear attention
ëª¨ë“  í”„ë ˆì„ì— ëŒ€í•´ ë™ì¼í•œ parameter ğ–, ğ›, ğ¯ ë¥¼ ê³µìœ 

<center><img src="https://user-images.githubusercontent.com/46676700/94430715-51f80980-01cf-11eb-9b90-9a302bca378a.png" alt="img" style="zoom: 80%;" /></center>

<br/>

#### <span style="background-color:#aee4ff">**3.3 Attention layer variants**</span>

- ê¸°ë³¸ì ì¸ attention layerì™€ ë‹¬ë¦¬ ë‘ê°€ì§€ì˜ ë³€í˜•ëœ ê¸°ë²• Cross-layer attentionì™€ Divided-layer attention ì†Œê°œ

**âœ” Cross-layer attention**

- ê¸°ì¡´ì˜ ë°©ë²• : ë§ˆì§€ë§‰ LSTMì˜ layerì˜ ì¶œë ¥ hğ‘¡ (1â‰¤ğ‘¡â‰¤ğ‘‡)ë¥¼ ì‚¬ìš©í•˜ì—¬ score eğ‘¡ì™€ weight Î±ğ‘¡ë¥¼ ê³„ì‚°
- ë³€í˜•ëœ ë°©ë²• : ì¤‘ê°„ LSTM layerì˜ ì¶œë ¥ h'ğ‘¡(1â‰¤ğ‘¡â‰¤ğ‘‡)ìœ¼ë¡œ ê³„ì‚° (ê·¸ë¦¼ 3.(a) outputì—ì„œ ë§ˆì§€ë§‰ 2ë²ˆì§¸ layerë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°)
- d-vector ğëŠ” ì—¬ì „íˆ ë§ˆì§€ë§‰ layer ì¶œë ¥ hğ‘¡ì™€ weighted sumìœ¼ë¡œ ê³„ì‚°

<center><img src="https://user-images.githubusercontent.com/46676700/94431728-9df77e00-01d0-11eb-83a4-7694a369266d.png" alt="img" style="zoom: 80%;" /></center>

<br/>

**âœ” Divided-layer attention**

- ë§ˆì§€ë§‰ LSTM layerì˜ ì¶œë ¥ hğ‘¡ì˜ ì°¨ì›ì„ 2ë°°ë¡œ ëŠ˜ë¦¬ê³  ê·¸ ì°¨ì›ì„ part aì™€ part b ë‘ ë¶€ë¶„ìœ¼ë¡œ ê· ë“±í•˜ê²Œ ë‚˜ëˆ”
- part bë¥¼ ì‚¬ìš©í•˜ì—¬ weightë¥¼ ê³„ì‚°í•˜ê³ , ë‚˜ë¨¸ì§€ part aì™€ weighted sumí•˜ì—¬ d-vector ìƒì„±

<center><img src="https://user-images.githubusercontent.com/46676700/94431901-e1ea8300-01d0-11eb-80d9-464a2cafaf02.png" alt="img" style="zoom: 80%;" /></center>

<br/>

#### <span style="background-color:#aee4ff">**3.4 Weights pooling**</span>

**âœ” Basic attention layerì˜ ë˜ ë‹¤ë¥¸ ë³€í™”**

- LSTMì˜ output â„ë¥¼ averageí•˜ê¸° ìœ„í•´ normalized weight ğ›¼ğ‘¡ ë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì§€ ì•Šê³ , maxpoolingìœ¼ë¡œ ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©

**âœ” ë‘ ê°€ì§€ maxpooling ë°©ë²• ì‚¬ìš©**

- Sliding Window maxpooling : Sliding windowì•ˆì˜ weight ì¤‘ í° ê°’ë§Œ ë‘ê³ , ë‚˜ë¨¸ì§€ëŠ” 0ìœ¼ë¡œ ë§Œë“¦ 
- Global top-K maxpooling : ê°€ì¥ í° Kê°œì˜ ê°’ë§Œ ë‘ê³ , ë‚˜ë¨¸ì§€ëŠ” 0ìœ¼ë¡œ ë§Œë“¦

<center><img src="https://user-images.githubusercontent.com/46676700/94432216-63421580-01d1-11eb-8235-ee4f90a727af.png" alt="img" style="zoom: 80%;" /></center>

> të²ˆì§¸ pixel : ê°€ì¤‘ì¹˜ ğ›¼ğ‘¡
> 
> ë°ì„ ìˆ˜ë¡ ê°€ì¤‘ì¹˜ê°€ í° ê°’ì„ ì˜ë¯¸


---

<br/>

### â…£. Experiments  ğŸ”

#### <span style="background-color:#aee4ff">**4.1 Datasets and basic setup**</span>

**âœ”  ì‚¬ìš©í•œ Dataset**

- â€œOk Googleâ€ê³¼ â€œHey Googleâ€ì´ í˜¼í•©ëœ ë°œí™” ë°ì´í„°
- ì•½ 630K í™”ìê°€ 150M ë°œí™” (í…ŒìŠ¤íŠ¸ ë°ì´í„° : 665ëª… í™”ì)
- í‰ê· ì ìœ¼ë¡œ enrollmentëŠ” 4.5ê°œ, evaluationì€ 10ê°œì˜ ë°œí™”ë¡œ êµ¬ì„±

<br/>

**âœ”  Basic setup**

- ê¸°ë³¸ baselineì€ 3ê°œì˜ layerë¡œ ì´ë£¨ì–´ì§„ LSTM
- ê° layerëŠ” 128ì°¨ì›ì´ë©°, 64ì°¨ì›ìœ¼ë¡œ projectioní•˜ëŠ” linear layerë¥¼ ê°€ì§€ê³  ìˆìŒ
- Global passwordë§Œ í¬í•¨í•˜ëŠ” ê¸¸ì´ T=80 frame(800ms)ì˜ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë¶„ë¦¬í•˜ëŠ” keyword detection í›„ 40ì°¨ì›ì˜ log-mel-filterbank feature ìƒì„±
- MultiReaderê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ë‘ ê°œì˜ keywordë¥¼ í˜¼í•©í•˜ì—¬ ì‚¬ìš©

<br/>

#### <span style="background-color:#aee4ff">**4.2 Basic attention layer**</span>

- ë‹¤ì–‘í•œ ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ Basic attention layerê³¼ ë¹„êµ

<center><img src="https://user-images.githubusercontent.com/46676700/94432416-b4eaa000-01d1-11eb-8141-99d48b30f3da.png" alt="img" style="zoom: 80%;" /></center>

- Bias-onlyì™€ linear attentionì€ EERì´ ê±°ì˜ ê°œì„ ë˜ì§€ ì•ŠìŒ
- Non-linear ì¤‘ íŠ¹íˆ, shared-parameterì˜ ê²½ìš° ì„±ëŠ¥ í–¥ìƒì´ ìˆìŒ

#### <span style="background-color:#aee4ff">**4.3 Variants**</span>

- Basic attention layerì™€ ë‘ ê°€ì§€ ë³€í˜•(cross-layer, divided-layer) ë¹„êµ
- ì´ì „ ì‹¤í—˜ì—ì„œ ìµœê³ ì˜ ì„±ëŠ¥ì„ ë‚¸ shared-parameter non-linear scoring functionì„ ì‚¬ìš©

<center><img src="https://user-images.githubusercontent.com/46676700/94432517-d8ade600-01d1-11eb-8325-50d593324e2b.png" alt="img" style="zoom: 80%;" /></center>

- cross-layerëŠ” ë§ˆì§€ë§‰ì—ì„œ 2ë²ˆì§¸ layerì—ì„œ scoreë¥¼ í›ˆë ¨ 
- divided-layer attentionì´ ë§ˆì§€ë§‰ LSTM layerì˜ ì°¨ì›ì´ 2ë°°ì´ì§€ë§Œ, Basic attentionê³¼ cross-layer attentionë³´ë‹¤ ì•½ê°„ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì„


#### <span style="background-color:#aee4ff">**4.4 Weights pooling**</span>

- Attention weightë¥¼ ë‹¤ì–‘í•œ poolingë°©ë²•ìœ¼ë¡œ ì‚¬ìš©í•œ ê²ƒê³¼ ë¹„êµ
- Shared-parameter non-linear scoring functionê³¼ divided-layer attention ì‚¬ìš©
- Sliding window maxpooling : 10 frame window sizeì™€ 5 frame step size
- Global top-K maxpooling : K = 5

<center><img src="https://user-images.githubusercontent.com/46676700/94432565-ea8f8900-01d1-11eb-856a-11c004b078e2.png" alt="img" style="zoom: 80%;" /></center>

- Sliding window maxpoolingì´ EERì´ ì•½ê°„ ë” ë‚®ì€ ê²ƒì„ í™•ì¸

<br/>

**âœ” ê° ë°©ë²•ì—ì„œ attention weightë¥¼ visualization**

<center><img src="https://user-images.githubusercontent.com/46676700/94433266-03e50500-01d3-11eb-8044-2e31658644e1.png" alt="img"/></center>


- Poolingì´ ì—†ì„ ë•Œ, 4ìŒì†Œ(O-kay-Goo-gle) ë˜ëŠ” 3ìŒì†Œ(Hey-Goo-gle) íŒ¨í„´ì„ í™•ì¸
- Poolingì„ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ ì‹œì‘ë¶€ë¶„ ë³´ë‹¤ëŠ” ëë¶€ë¶„ì˜ ë°œí™”ê°€ ë” í° attention weightë¥¼ ê°€ì§
- LSTMì€ ì´ì „ ìƒíƒœ ê°’ì„ ëˆ„ì í•˜ì—¬ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì— ë§ˆì§€ë§‰ìœ¼ë¡œ ê°ˆìˆ˜ë¡ ë” ë§ì€ ì •ë³´ë¥¼ ê°€ì§ìœ¼ë¡œì¨ ë‚˜ì˜¤ê²Œ ë˜ëŠ” í˜„ìƒìœ¼ë¡œ íŒë‹¨


<br/>

---

<br/>

### â…¤.  Conclusion ğŸ”

- ë³¸ ë…¼ë¬¸ì—ì„œëŠ” keyword ê¸°ë°˜ì˜ Text-dependent í™”ì ê²€ì¦ ì‹œìŠ¤í…œì„ ìœ„í•œ ë‹¤ì–‘í•œ Attention mechanismì„ ì‹¤í—˜

- ê°€ì¥ ì¢‹ì€ ë°©ë²•
1. shared-parameter non-linear scoring function ì‚¬ìš©
2. LSTMì˜ ë§ˆì§€ë§‰ layerì— divided-layer attention ì‚¬ìš©
3. Sliding window maxpoolingì„ attention weightì— ì ìš©

- ìœ„ì˜ 3ê°€ì§€ë¥¼ ê²°í•©í•˜ì˜€ì„ ë•Œ ê¸°ë³¸ LSTMëª¨ë¸ EER 1.72%ì—ì„œ 14%ì˜ ìƒëŒ€ì  ì„±ëŠ¥ í–¥ìƒì„ ê°€ì ¸ì˜´

- <span style="color:#FF0000">**ë™ì¼í•œ attention mechanism(íŠ¹íˆ, shared-parameter scoring function)ì€ Text-independentí•œ í™”ì ê²€ì¦ ë° í™”ì ì‹ë³„ì„ ê°œì„ í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë  ìˆ˜ ìˆìŒ**</span>


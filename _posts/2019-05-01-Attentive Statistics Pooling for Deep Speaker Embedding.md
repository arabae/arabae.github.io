---
layout: post
title: "Attentive Statistics Pooling for Deep Speaker Embedding"
date: 2019-05-01
category: review
thumbnail: /style/image/attention.png
icon: book
---

* contents
{:toc}

<span style="font-size:13pt">Koji Okabe, Takafumi Koshinaka, Koichi Shinoda</span>

# ğŸ“Œ **Abstract**
- <span style="background-color:#FFE49B">**Text-independent**(ë¬¸ì¥ ë…ë¦½ : ë°œí™” ë‚´ìš©ì´ ë™ì¼í•˜ì§€ í•˜ì§€ ì•ŠìŒ)í•œ **Speaker Verification**(í™”ì ê²€ì¦ : ë“±ë¡ëœ í™”ìì¸ì§€ ì•„ë‹Œì§€ íŒë‹¨, SV)ì—ì„œ **Deep speaker embeddingì„ ìœ„í•œ attentive statistics pooling** ì œì•ˆ</span>

- ê¸°ì¡´ì˜ speaker embeddingì—ì„œëŠ” ë‹¨ì¼ ë°œí™”ì˜ ëª¨ë“  frameì—ì„œ frame-levelì˜ íŠ¹ì§•ì„ ëª¨ë‘ í‰ê·  ë‚´ì–´ utterance-levelì˜ íŠ¹ì§•ì„ í˜•ì„±

- ì œì•ˆí•˜ëŠ” ë°©ë²•ì€ attention mechanismì„ ì‚¬ìš©í•˜ì—¬ ê° frameë§ˆë‹¤ ë‹¤ë¥¸ weight(ê°€ì¤‘ì¹˜)ë¥¼ ë¶€ì—¬í•˜ê³ , weighted mean(ê°€ì¤‘ í‰ê· )ê³¼ weighted standard deviations(ê°€ì¤‘ í‘œì¤€ í¸ì°¨)ë¥¼ ìƒì„±


âœ”  <span style="background-color:#FFE49B">NISE SRE 2012 ë° VoxCeleb data setì—ì„œ ê¸°ì¡´ ë°©ë²•ì— ë¹„í•´ EERì´ ê°ê° 7.5%, 8.1% ê°ì†Œ</span>

<br/>

---

<br/>

# ğŸ“Œ **Introduction**

- **í™”ì ì¸ì‹ì€ ì§€ë‚œ 10ë…„ë™ì•ˆ i-vector paradigmê³¼ ì§„í™”**í•˜ì˜€ê³ , i-vectorëŠ” ê³ ì •ëœ ì €ì°¨ì›ì˜ íŠ¹ì§• ë²¡í„° í˜•íƒœë¡œ ìŒì„± ë°œí™” í˜¹ì€ í™”ìë¥¼ í‘œí˜„

- ë‹¤ì–‘í•œ ê¸°ê³„í•™ìŠµì„ í†µí•´ Deep learningì´ ì„±ëŠ¥ í–¥ìƒì— í¬ê²Œ ê¸°ì—¬í•˜ë©°, í™”ì ì¸ì‹ì„ ìœ„í•œ íŠ¹ì§• ì¶”ì¶œì— Deep learningì„ ë„ì…ì´ ì¦ê°€

- ì´ˆê¸° ì—°êµ¬ì—ì„œëŠ” ASR(Automatic Speech Recognition)ì˜ ìŒí–¥ ëª¨ë¸ì—ì„œ ë„ì¶œëœ DNNì„ UBMìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ê¸°ì¡´ì˜ GMMê¸°ë°˜ UBMë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ì§€ë§Œ ì–¸ì–´ ì˜ì¡´ì„± ë‹¨ì ê³¼ í›ˆë ¨ì„ ìœ„í•´ ìŒì†Œ transcriptionì´ í•„ìš”

- ìµœê·¼ **DNNì€ ì´ëŸ¬í•œ i-vector frameworkì™€ ë…ë¦½ì **ìœ¼ë¡œ **í™”ì ë§ˆë‹¤ ê³ ìœ í•œ íŠ¹ì§• ë²¡í„°ë¥¼ ì¶”ì¶œí•˜ëŠ”ë° ìœ ìš©**í•˜ë‹¤ê³  ë°í˜€ì§ (íŠ¹íˆ, ì§§ì€ ë°œí™” ì¡°ê±´ì—ì„œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì„)

- Text-dependent(ë¬¸ì¥ ì¢…ì† : ë°œí™” ë‚´ìš©ì´ ë™ì¼í•¨) SVì—ì„œ LSTM(ë§ˆì§€ë§‰ frameì—ì„œ í•˜ë‚˜ì˜ ì¶œë ¥ì„ ê°–ëŠ” êµ¬ì¡°)ì„ ì‚¬ìš©í•˜ì—¬ utterance-levelì˜ íŠ¹ì§•ì„ ì–»ëŠ” End-to-End Neural Networkê¸°ë°˜ì˜ ë°©ë²•ì´ ì œì•ˆë˜ì—ˆìœ¼ë©°, ê¸°ì¡´ì˜ i-vectorë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„

- Text-independent SVëŠ” ì…ë ¥ìœ¼ë¡œ ë‹¤ì–‘í•œ ê¸¸ì´ì˜ ë°œí™”ë¥¼ ê°–ìœ¼ë¯€ë¡œ average pooling layerê°€ ë„ì…ë˜ì–´ frame-levelì˜ í™”ì íŠ¹ì§• ë²¡í„°ë¥¼ ì¼ì •í•œì°¨ì›ì„ ê°–ëŠ” speaker embedding ë²¡í„°ë¥¼ ì–»ìŒ

- ëŒ€ë¶€ë¶„ ìµœê·¼ ì—°êµ¬ì—ì„œ DNNì´ i-vectorë³´ë‹¤ ë” ë‚˜ì€ ì •í™•ë„ë¥¼ ê°–ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ë©° Snyder ì™¸ëŠ” average poolingë¥¼ í™•ì¥í•œ statistics pooling (í‰ê·  ë° í‘œì¤€ í¸ì°¨ ê³„ì‚°)ì„ ì±„íƒ

- ê·¸ëŸ¬ë‚˜ ì•„ì§ ì •í™•ë„ í–¥ìƒì— ëŒ€í•œ í‘œì¤€ í¸ì°¨ poolingì˜ íš¨ìœ¨ì„±ì€ ë³´ê³ í•˜ì§€ ì•ŠìŒ

<br/>

- ìµœê·¼ ë‹¤ë¥¸ ì—°êµ¬ì—ì„œëŠ” ì´ì „ì— ê¸°ê³„ ë²ˆì—­ì—ì„œ ìƒë‹¹í•œ ì„±ëŠ¥ í–¥ìƒì„ ê°€ì ¸ì˜¨ **attention mechanismê³¼ í†µí•©**

- í™”ì ì¸ì‹ì—ì„œë„ ì¤‘ìš”ë„ ê³„ì‚° ì‹œ, speaker embedding ì¶”ì¶œí•˜ëŠ” networkì˜ ì¼ë¶€ë¡œ ì‘ë™í•˜ëŠ” ì‘ì€ attention network ì‚¬ìš©

- ê³„ì‚°ëœ ì¤‘ìš”ë„ëŠ” frame-levelì˜ íŠ¹ì§• ë²¡í„°ì˜ weighted mean ê³„ì‚°í•  ë•Œ ì‚¬ìš©í•˜ì—¬ speaker embeddingì´ ì¤‘ìš”í•œ frameì— ì´ˆì ì„ ë§ì¶¤

- ê·¸ëŸ¬ë‚˜ ì´ì „ ì—°êµ¬ì—ì„œëŠ” ê³ ì • ê¸¸ì´ì˜ text-independent í˜¹ì€ text-dependent í™”ì ì¸ì‹ê³¼ ê°™ì€ ì œí•œëœ ì‘ì—…ì—ì„œë§Œ ìˆ˜í–‰

**- ë³¸ ë…¼ë¬¸ì—ì„œ attention mechanismìœ¼ë¡œ ê³„ì‚°ëœ ì¤‘ìš”ë„ë¡œ importance-weighted standard deviationê³¼ weighted meanì‚¬ìš©í•œ ìƒˆë¡œìš´ poolingë°©ë²•ì¸ attentive statistics poolingë¥¼ ì œì•ˆ**

- ê°€ë³€ ê¸¸ì´ì˜ text-independentí•œ í™˜ê²½ì—ì„œ attentive statisitics poolingì„ ì‚¬ìš©í•˜ëŠ” ì²« ë²ˆì§¸ ì‹œë„ ì´ë©°, ë‹¤ì–‘í•œ pooling layer ë¹„êµë¥¼ í†µí•´ í‘œì¤€ í¸ì°¨ê°€ í™”ì íŠ¹ì„±ì— ë¯¸ì¹˜ëŠ” íš¨ê³¼ë¥¼ ì‹¤í—˜ì ìœ¼ë¡œ ë³´ì—¬ì¤Œ

<br/>

---

<br/>

# ğŸ“Œ **Deep speaker embedding**

- ê¸°ì¡´ì˜ DNNì„ ì‚¬ìš©í•œ speaker embedding ì¶”ì¶œ ë°©ë²•

> input : acoustic feature (MFCC, filter-bank ë“±)  
> frame-levelì˜ íŠ¹ì§• ì¶”ì¶œì„ ìœ„í•´ TDNN, CNN, LSTM ë“±ì˜ Neural Network  
> ê°€ë³€ ê¸¸ì´ì˜ frame-level íŠ¹ì§•ì„ ê³ ì • ì°¨ì›ì˜ ë²¡í„°ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•œ pooling layer  
> utterance-levelì˜ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ê¸° ìœ„í•œ fully-connected layer(hidden layer ì¤‘ í•˜ë‚˜ì˜ node ìˆ˜ë¥¼ ì‘ê²Œ í•˜ì—¬ bottleneck featureë¡œ ì‚¬ìš©)  

<br/>

<center><img src="https://user-images.githubusercontent.com/46676700/89165519-a443f200-d5b3-11ea-8009-d34a68859aa4.png" alt="img" style="zoom:60%;" /></center>

<br/>

---

<br/>

# ğŸ“Œ **High-order pooling with attention**

< Statistics pooling - ê¸°ì¡´ì— ì‚¬ìš©í•˜ë˜ pooling ë°©ë²• >

- frame-level íŠ¹ì§•ì— ëŒ€í•´ í‰ê· (mean)ê³¼ í‘œì¤€ í¸ì°¨(standard deviation) ê³„ì‚° (âŠ™ : Hadamard ê³±)í•˜ì—¬ concatenation

<center><img src="https://user-images.githubusercontent.com/46676700/89165568-b160e100-d5b3-11ea-9a93-2a31b6530b2b.png" alt="img" style="zoom: 45%;" /></center>

< Attention mechanism >

- ê¸°ê³„ ë²ˆì—­ì—ì„œ ê¸´ ë¬¸ì¥ì˜ ì„±ëŠ¥ ì €í•˜ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ëª¨ë¸ì´ ì¶œë ¥ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•  ë•Œ **íŠ¹ì • ë‹¨ì–´ë¥¼ ì§‘ì¤‘**í•´ì„œ ë³´ëŠ” ë°©ë²•ì„ ë„ì…

<center><img src="https://user-images.githubusercontent.com/46676700/89165571-b1f97780-d5b3-11ea-91e3-8fa3f49000fc.png" alt="img" style="zoom: 80%;" /><img src="https://user-images.githubusercontent.com/46676700/89165573-b1f97780-d5b3-11ea-9545-3a591f97f98d.png" alt="img" style="zoom: 50%;" /></center>

<br/>

<img src="https://user-images.githubusercontent.com/46676700/89165553-aefe8700-d5b3-11ea-9e0a-c4c8d5fc14a0.png" alt="img"/>

- decoderì˜ <span style="color:#a5cbf0">**ì‹œê°„ i(í˜„ì¬)ì—ì„œ hidden state ë²¡í„°**</span>ëŠ” <span style="color:#a5cbf0">**ì‹œê°„ i-1(ì´ì „)ì˜ hidden state ë²¡í„°**</span>ì™€ <span style="color:#ffaddf">**ì‹œê°„ i-1(ì´ì „)ì—ì„œ decoderì˜ output**</span>, ê·¸ë¦¬ê³  <span style="color:#7cbfb6">**ì‹œê°„ i(í˜„ì¬)ì—ì„œì˜ context ë²¡í„°**</span>ë¥¼ ì…ë ¥ìœ¼ë¡œ ê³„ì‚°

<img src="https://user-images.githubusercontent.com/46676700/89165558-af971d80-d5b3-11ea-84c7-8f0478e8e680.png" alt="img"/>

- <span style="color:#7cbfb6">**context ë²¡í„°**</span>ëŠ” **ì‹œê°„ iì—ì„œ ì…ë ¥ xì— ëŒ€í•œ ê¸¸ì´ T** ì „ì²´ì— ëŒ€í•œ **<span style="color:#f9d877">encoder hidden state ë²¡í„°</span>**ì˜ **ê°€ì¤‘í•©**ìœ¼ë¡œ ê³„ì‚°

<img src="https://user-images.githubusercontent.com/46676700/89165559-b02fb400-d5b3-11ea-9ad9-a8383a6810d6.png" alt="img"/>


- <span style="color:#33558c">**ì‹œê°„ iì—ì„œ jë²ˆì§¸ ë‹¨ì–´ì˜ energy**</span>ëŠ” **<span style="color:#a5cbf0">ì‹œê°„ i-1(ì´ì „)ì—ì„œ decoder hidden state</span>**ì™€**<span style="color:#f9d877">Â jë²ˆì§¸ encoder hidden state</span>**ê°€ ì…ë ¥ì¸ **aligment model(a)** ê²°ê³¼ê°’ (alignment modelì€ tanh, ReLU ë“± activation function)

<img src="https://user-images.githubusercontent.com/46676700/89165560-b02fb400-d5b3-11ea-8753-68026664a442.png" alt="img"/>

<br/>

< Attentive statistics pooling >

<center><img src="https://user-images.githubusercontent.com/46676700/89165563-b0c84a80-d5b3-11ea-9590-62c129a447e4.png" alt="img" style="zoom: 50%;" /><img src="https://user-images.githubusercontent.com/46676700/89165564-b0c84a80-d5b3-11ea-8a2f-c887055c76d8.png"  alt="img" style="zoom: 50%;" /></center>

attention mechanismì„ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°í•œ **ê°€ì¤‘ì¹˜ë¥¼ í†µí•´ meanê³¼ standard deviationì„ ê°±ì‹ **

<center><img src="https://user-images.githubusercontent.com/46676700/89165566-b160e100-d5b3-11ea-9625-41ccb0db4353.png"  alt="img" style="zoom: 67%;"/></center>

<br/>

---

<br/>

# ğŸ“Œ **Experimental settings**

**i-vector**

> input : 60ì°¨ì› MFCC  
> UBM : 2048 mixture  
> TV matrix, i-vector : 400ì°¨ì›  
> Similarity score : PLDA  

<br/>

**Deep speaker embedding**

> input : 20ì°¨ì›(SRE 12), 40ì°¨ì›(VoxCeleb) MFCC  
> hidden layer : 5-layer TDNN(activation function : ReLU, node : 512)  
> pooling dimension : 1500ì°¨ì›  
> acoustic feature vector(MFCC) 15ê°œ frameìœ¼ë¡œ frame-level íŠ¹ì§• ìƒì„±  
> 2 fully-connected layer (1st : bottleneck feature - 512, activation function : ReLU, batch   normalization)  

---
layout: post
title: "ContextNet: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context"
date: 2021-08-04
category: review
thumbnail: /style/image/contextnet.png
use_math: true
icon: book
---

* contents
{:toc}

<span style="font-size:13pt">Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin, Anmol Gulati, Ruoming Pang, Yonghui Wu</span>

# **<span style="color:#E84560">_Abstract_</span>**
End-to-End speech recognitionì—ì„œ Convolution neural network (CNN)ëŠ” ê¸°ëŒ€ë˜ëŠ” ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì§€ë§Œ, ì—¬ì „íˆ RNN/Transformer ê¸°ë°˜ ëª¨ë¸ë³´ë‹¤ëŠ” ë’¤ì³ì§

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì§€ê¸ˆê¹Œì§€ ì—†ì—ˆë˜ ContextNetì´ë¼ ë¶€ë¥´ëŠ” CNN-RNN-transducer architectureë¡œ ì´ëŸ¬í•œ ì°¨ì´ë¥¼ ë©”ìš°ê³ , ë” ë‚˜ì•„ê°ˆ ë°©ë²•ì„ ì—°êµ¬

ContextNetëŠ” **squeeze-and-excitation moduleì„ ì¶”ê°€**í•˜ì—¬ global context informationì„ í†µí•©í•˜ëŠ” **fully convolutional encoder**ê°€ íŠ¹ì§•

ì¶”ê°€ì ìœ¼ë¡œ computationê³¼ accuracy ì‚¬ì´ì˜ ì¢‹ì€ trade-offë¥¼ ë‹¬ì„±í•˜ëŠ” ContextNetì˜ width scaleì„ ê°„ë‹¨í•˜ê²Œ scaling ë°©ë²•ì„ ì œì•ˆ

ì „ë°˜ì ìœ¼ë¡œ Librispeech benchmarkë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤í—˜

**_ContextNet_**

- LM X: WER 2.1%/4.6% (clean/nosiy test set)
- LM O: WER 1.9%/4.1%
- 10M parameter: 2.9%/7.0%

**_ë¹„êµ ëª¨ë¸_**

- LM O: WER 2.0%/4.6%
- 20M parameter: 3.9%/11.3%

â‡’ ì œì•ˆëœ ContextNet ëª¨ë¸ì˜ ìš°ìˆ˜ì„±ì€ í›¨ì”¬ ë” í° ë‚´ë¶€ ë°ì´í„° ì„¸íŠ¸ì—ì„œë„ í™•ì¸ë¨


# **<span style="color:#E84560">_1. Introduction_</span>**

E2E speech recognitionë¥¼ ìœ„í•œ CNN ê¸°ë°˜ ëª¨ë¸ì€ ê´€ì‹¬ì´ ì ì  ì¦ê°€í•˜ëŠ” ì¶”ì„¸

ê·¸ ì¤‘ì—ì„œë„ **Jasper model**ì€ LibriSpeech test-cleanì—ì„œ WER 2.95%ë¡œ SOTAë¥¼ ë‹¬ì„±

> **_Jasper model_**

- 1D convolution layerë¥¼ ìŒ“ì€ deep convolutionê¸°ë°˜ encoder
- skip conntection

> **_Depthwise separable convolutions (Xception)_**

- CNN ëª¨ë¸ì˜ ì†ë„ì™€ ì •í™•ë„ë¥¼ ë”ìš± ë†’ì´ê¸° ìœ„í•´ ì‚¬ìš©

CNNê¸°ë°˜ ëª¨ë¸ì˜ í•µì‹¬ ì´ì ì€ parameterë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©

â†’ ê·¸ëŸ¬ë‚˜ CNN best ëª¨ë¸(QuartzNet)ì€ ì—¬ì „íˆ RNN/transformerê¸°ë°˜ ëª¨ë¸ë³´ë‹¤ ë’¤ì³ì§

*RNN/transformer ê¸°ë°˜ ëª¨ë¸ê³¼ CNN ëª¨ë¸ì˜ **ì£¼ìš” ì°¨ì´ì **ì€* **contextì˜ ê¸¸ì´ê°€ ë‹¤ë¦„**

- Bidirectional RNN modelì—ì„œ ì´ë¡ ìƒ cellì€ ì „ì²´ ì‹œí€€ìŠ¤ì˜ ì •ë³´ì— ì ‘ê·¼ ê°€ëŠ¥
- Transformer ëª¨ë¸ì—ì„œ attention mechanismì€ ë¨¼ time stampì— ìˆëŠ” ë‘ ê°œì˜ ë…¸ë“œê°€ ì„œë¡œ ì˜í–¥ì„ ì£¼ë„ë¡ ëª…ì‹œì ìœ¼ë¡œ í—ˆìš©
- ì œí•œëœ kernel sizeë¥¼ ê°–ëŠ” naiveí•œ convolutionì€ time domainì—ì„œ ì‘ì€ windowë§Œ coverí•  ìˆ˜ ìˆìŒ

â†’ contextê°€ ì‘ê³ , global informationì„ í†µí•©í•  ìˆ˜ ì—†ìŒ

ë³¸ ë…¼ë¬¸ì—ì„œ CNNê¸°ë°˜ ASR ëª¨ë¸ê³¼ RNN/transformer ê¸°ë°˜ ëª¨ë¸ì˜ WER ì°¨ì´ì˜ í•µì‹¬ì ì¸ ì´ìœ ì¸ **global context**ì˜ ë¶€ì¡±ì„ ì—°êµ¬

â†’ CNN ëª¨ë¸ì˜ global contextë¥¼ ê°•í™”í•˜ê¸° ìœ„í•´ [12]ì—ì„œ ì†Œê°œëœ **sequeeze-and-excitation (SE) layer**ì—ì„œ ì˜ê°ì„ ì–»ì–´, **ContextNet**ì´ë¼ê³  ë¶€ë¥´ëŠ” ASRì„ ìœ„í•œ ì´ì „ì— ì—†ë˜ CNN ëª¨ë¸ì„ ì œì•ˆ

> **_Sequeeze-and-Excitation (SE) layer_**

- local feature vector sequenceë¥¼ single global contextë¡œ ì••ì¶•í•˜ê³ , ì´ contextë¥¼ ë‹¤ì‹œ ê° local feature vectorë¡œ broadcast í›„ ê³±ì…ˆì„ í†µí•´ ë‘˜ì„ ë³‘í•©
    - average ë“±ì„ í†µí•´ í•˜ë‚˜ì˜ global featureë¥¼ ìƒì„±í•˜ê³ , ë‹¤ì‹œ ì°¨ì›ì„ local featureë¡œ broadcastí•œ ë’¤ ë‘˜ì„ ê³±í•¨ìœ¼ë¡œì¨ í•˜ë‚˜ì˜ ê²°ê³¼ë¥¼ ì–»ìŒ

naiveí•œ convolution layer ë’¤ì— SE layerë¥¼ ë°°ì¹˜í•  ë•Œ convolution outputì— global ì •ë³´ì— ëŒ€í•œ ì ‘ê·¼ì„ ì¤Œ

ê²½í—˜ì ìœ¼ë¡œ **ContextNetì— SE layerë¥¼ ì¶”ê°€**í•˜ë©´ LibriSpeech test-otherì—ì„œ **WERì´ ê°€ì¥ ë§ì´ ê°ì†Œ**í•œë‹¤ëŠ” ê²ƒì„ ë°œê²¬

# **<span style="color:#E84560">_2. Model_</span>**

### **<span style="color:#724598">2.1 End-to-End Network: CNN-RNN-Transducer</span>**

â€” ***ContextNet high-level design***

- RNN-Transducer framework ê¸°ë°˜
- networkì˜ 3ê°€ì§€ êµ¬ì„± ìš”ì†Œ
  1. input utteranceë¥¼ ë°›ëŠ” audio encoder
  2. input labelì„ ë°›ëŠ” label encoder
  3. ì´ ë‘˜ì„ ê²°í•©í•˜ê³  decodingí•˜ëŠ” joint network

â‡’ LSTM ê¸°ë°˜ label encoderì™€ joint networkë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ **ìƒˆë¡œìš´ CNN ê¸°ë°˜ audio encoderë¥¼ ì œì•ˆ**

### **<span style="color:#724598">2.2 Encoder Design</span>**

â€” ***Convolutional encoder (model accuracy maintaining, temporal length reduce)***

$x = (x_1,...,x_T)$ ; input sequence  
$h = AudioEncoder(x) = C_K(C_{K-1}(...C_1(x)))$  

- encoderëŠ” ì›ë˜ signal xë¥¼ high level representationìœ¼ë¡œ ë³€í™˜
- ê° $C_k(Â·)$ëŠ” Convolution blockì„ ì •ì˜
    - ëª‡ ê°œì˜ convolution layerê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©°, ê° layerì—ëŠ” batch normalization, activation functionì´ ìˆìŒ
    - ë˜í•œ, Squeeze-and-excitation êµ¬ì„± ìš”ì†Œì™€ skip connectionì„ í¬í•¨

C(Â·)ì˜ ì¤‘ìš”í•œ ëª¨ë“ˆ
![image](https://user-images.githubusercontent.com/46676700/133924656-7617477a-2f82-4f32-a1c9-754587dcfc33.png)


> **2.2.1. seqeeze-and-excitation**

$SE(Â·)$ ; Squeeze-and-Excitation function

- input xì— ëŒ€í•´ global average pooling â†’ global channelwise weight $\theta(x)$ â†’ element-wise multiplies each frame

â‡’ 1D caseì— ì´ ideaë¥¼ ë„ì…

$\bar{x} = \frac{1}{T} \Sigma_t{x_t}$

$\theta{(x)} = Sigmoid(W_2(Act(W_1\bar{x}+b_1))+b_2)$

$SE(x) = \theta{(x)} \; â—¦ \; x$

- â—¦ ; element-wise multiplication
- $W_1, W_2$ ; weigth matrix
- $b_1, b_2$ ; bias vector

> **2.2.2. Depthwise separable convolution**

$conv(Â·)$ ; encoderì—ì„œ ì‚¬ìš©ë˜ëŠ” convolution function

- depthë³„ ë¶„ë¦¬ ê°€ëŠ¥í•œ convolutionì‚¬ìš©
- ì´ëŸ¬í•œ ì„¤ê³„ê°€ **ì •í™•ë„ì— ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šê³ **, **ë” ë‚˜ì€ parameter íš¨ìœ¨ì„±ì„ ë‹¬ì„±**í•˜ê¸° ìœ„í•´ ë‹¤ì–‘í•œ ì—°êµ¬[6, 4, 28]ì—ì„œ ë³´ì—¬ì¤Œ
- **ë‹¨ìˆœí™”**ë¥¼ ìœ„í•´ networkì˜ ëª¨ë“  ê¹Šì´ë³„ convolution layerì—ì„œ ë™ì¼í•œ kernel sizeë¥¼ ì‚¬ìš©

> **2.2.3. Swish activation function**

$Act(Â·)$ ; encoderì˜ activation function

- ReLUì™€ swish functionì„ ëª¨ë‘ ì‹¤í—˜

**Swish function**
$Act(x) = x \cdot \sigma{(\beta x)} = \frac{x}{1+\exp(-\beta x)}$

- $\beta$ ; ëª¨ë“  ì‹¤í—˜ì— ëŒ€í•´ 1
- **swish function**ì´ ReLUë³´ë‹¤ ì¼ê´€ë˜ê²Œ ì‘ë™í•˜ëŠ” ê²ƒì„ ê´€ì°°

> **2.2.4. Convolution block**

figure 3 ; $C(\cdot)$ì˜ high-level architecture

- block $C(\cdot)$ì—ëŠ” ì—¬ëŸ¬ $conv(\cdot)$ì´ í¬í•¨ë  ìˆ˜ ìˆìŒ (m ;  $conv(\cdot)$ì˜ ìˆ˜)
- $BN(Â·)$ ; batch normalization

â†’ $f(x) = Act(BN(Conv(x))$

â‡’ $C(x) = Act(SE(f^m(x)) + P(x))$

- $f^m$ ; inputì— $f(\cdot)$ functionì´ mê°œ ì¸µì´ ìŒ“ì„
- $P(\cdot)$ ; residualì— ëŒ€í•œ pointwise projection function

![image](https://user-images.githubusercontent.com/46676700/133924665-8d0cc2af-3731-4796-a012-567ad1339027.png)

ì²« ë²ˆì§¸, ë§ˆì§€ë§‰ layerëŠ” ë‚˜ë¨¸ì§€(m-2) layerì™€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ

- blockì— input channel ìˆ˜($D_{in}$)ê³¼ output channel ìˆ˜($D_{out}$)ì´ ìˆëŠ” ê²½ìš°, ì²« ë²ˆì§¸ layer ($f'$)ëŠ” $D_{in}$ channelì„ $D_{out}$ channelë¡œ ë³€í™˜í•˜ê³  ë‚˜ë¨¸ì§€(m-1) layerëŠ” channel ìˆ˜ë¥¼ $D_{out}$ìœ¼ë¡œ ìœ ì§€
- blockì´ input sequenceë¥¼ two timeì— ê±¸ì³ downsamplingí•˜ëŠ” ê²½ìš°, ë§ˆì§€ë§‰ layerëŠ” strideê°€ 2ì´ë©° ë‚˜ë¨¸ì§€(m-1) layerëŠ” strideê°€ 1 (ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ëª¨ë‘ strideê°€ 1)

ê¸°ì¡´ ì—°êµ¬ë“¤ì— ë”°ë¼ projection function $P$ëŠ” ì²« ë²ˆì§¸ layerì™€ ê°™ì€ strideë¥¼ ê°–ìŒ

> **2.2.5. Progressive downsampling**

temporal downsamplingì„ ìœ„í•´ strided convolution ì‚¬ìš©

- downsampling layerê°€ ë§ì„ ìˆ˜ë¡ ê³„ì‚° ë¹„ìš©ì´ ê°ì†Œ
- encoderì„œ ê³¼ë„í•œ downsamplingì€ decoderì— ë¶€ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŒ

â†’ ê²½í—˜ì ìœ¼ë¡œ progressiveí•˜ê²Œ 8ë°°ë¥¼ downsamplingí•˜ëŠ”ê²Œ ì†ë„ì™€ ì„±ëŠ¥ë©´ì—ì„œ ì¢‹ì€ trade-offë¥¼ ê°–ëŠ” ê²ƒì„ ë°œê²¬ (ì´ëŸ¬í•œ ì ˆì¶©ì€ ì„¹ì…˜ 3.3ì—ì„œ ë…¼ì˜)

> **2.2.6. Configuration details of ContextNet**

ContextNetì—ëŠ” 23ê°œì˜ convolution block $C_0, . . . , C_{22}$ ì´ ì¡´ì¬

- ëª¨ë“  convolution blockì—ëŠ” ê°ê° í•˜ë‚˜ì˜ convolution layerë§Œ ìˆëŠ” $C_0$ ë° $C_{22}$ë¥¼ ì œì™¸í•˜ê³ , 5ê°œì˜ convolution layer ì¡´ì¬

Table 1 ; architecture detailì´ ìš”ì•½ë¨

- global parameter $\alpha$ëŠ” model scalingì„ ì¡°ì •
    - $Î± > 1$: $Î±$ë¥¼ ì¦ê°€ì‹œí‚¤ë©´ convolutionì˜ channel ìˆ˜ê°€ ì¦ê°€ â†’ model sizeê°€ í´ìˆ˜ë¡ modelì€ ë” ë§ì€ í‘œí˜„ë ¥ì„ ê°–ìŒ

![image](https://user-images.githubusercontent.com/46676700/133924661-dffde0a7-9213-46d6-8ad9-d3f76bfab21a.png)

# **<span style="color:#E84560">_3. Experiments_</span>**

- Librispeech dataset
    - 970ì‹œê°„ labeled speech + text only corpus (for building language model)
- feature
    - 10ms stride, 25ms window, 80 dimensional fiterbank
- Adam optimizer
- Transformer learning rate schedule
    - 15k warm-up step, 0.0025 peak learning rate
- L2 regularization with $10^{-6}$ weight
- single layer LSTM as decoder
    - 640 input dimension, variational noise
- SpecAugment
    - mask parameter (F=27), 10ê°œ maximum time-mask ratio (ps=0.05), time-maskì˜ ìµœëŒ€ í¬ê¸°ëŠ” utteranceê¸¸ì´ * ps
    - time warping ì‚¬ìš© X

LibriSpeech 960hì—ì„œ êµ¬ì¶•ëœ 1k WPMìœ¼ë¡œ í† í°í™”ëœ LibriSpeech960h transcriptê°€ ì¶”ê°€ëœ LibriSpeechì—ì„œ í›ˆë ¨ëœ width 4096ì˜ 3-layer LSTM LM ì‚¬ìš©

- LMì€ dev-set transcriptì—ì„œ word-level perplexityê°€ 63.9
- shallow fusionì— ëŒ€í•œ LM weight $Î»$ëŠ” grid searchë¥¼ í†µí•´ dev-setì—ì„œ ì¡°ì •
- ëª¨ë“  ëª¨ë¸ì€ Lingvo toolkitìœ¼ë¡œ êµ¬í˜„

### **<span style="color:#724598">3.1. Results on LibriSpeech</span>**

LibriSpeechì—ì„œ ContextNetì˜ ì„¸ ê°€ì§€ ë‹¤ë¥¸ êµ¬ì„±ì„ í‰ê°€

- ëª¨ë‘ table 1ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ì§€ë§Œ network width $Î±$ê°€ ë‹¤ë¦„ (ì¦‰, model sizeê°€ ë‹¤ë¦„)

small, medium, large ContextNetì— ëŒ€í•´ {0.5, 1, 2}ë¡œ Î±ë¥¼ ì„ íƒ

- ë˜í•œ ì°¸ì¡°ìš©ìœ¼ë¡œ ìì²´ LSTM baselineì„ êµ¬ì¶•

Table 2ëŠ” ì´ì „ì— ë°œí‘œëœ ëª‡ ê°€ì§€ ì‹œìŠ¤í…œê³¼ì˜ evaluation result ë¹„êµë¥¼ ìš”ì•½

- ì´ì „ì— ë°œí‘œëœ ì‹œìŠ¤í…œì— ë¹„í•´ ContextNetì´ ê°œì„ ë˜ì—ˆìŒì„ ì‹œì‚¬í•¨
- ContextNet(S): ì–¸ì–´ ëª¨ë¸ì´ ìˆê±°ë‚˜ ì—†ëŠ” ìœ ì‚¬í•œ í¬ê¸°[4]ì˜ ì´ì „ ì‹œìŠ¤í…œì— ëŒ€í•´ ê°œì„ ë¨
- ContextNet(M): 31M parameterë§Œ ê°€ì§€ê³  ìˆìœ¼ë©°, í›¨ì”¬ ë” í° ì‹œìŠ¤í…œê³¼ ë¹„êµí•˜ì—¬ ìœ ì‚¬í•œ WERì„ ë‹¬ì„±
- ContextNet(L): ì´ì „ SOTAë³´ë‹¤ test-cleanì—ì„œ ìƒëŒ€ì ìœ¼ë¡œ 13%, test-otherì—ì„œ ìƒëŒ€ì ìœ¼ë¡œ 18% ë” ìš°ìˆ˜í•¨

![image](https://user-images.githubusercontent.com/46676700/133924670-44ad7506-3262-45e1-8601-859c6e230dca.png)

### **<span style="color:#724598">3.2. Effect of Context Size</span>**

> **ablation ì—°êµ¬ë¥¼ ìˆ˜í–‰**

- ASRìš© CNN ëª¨ë¸ì— global contextë¥¼ ì¶”ê°€í•˜ëŠ” íš¨ê³¼ë¥¼ ê²€ì¦
- Squeeze-and-Excitation moduleì´ LibriSpeech test-clean/test-otherì˜ WERì— ì–´ë–»ê²Œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€

Table 1ì˜ ContextNetì€ ëª¨ë“  squeeze-and-excitation ëª¨ë“ˆì´ ì œê±°ë˜ê³ , $Î±$ = 1.25ê°€ zero contextì˜ baselineìœ¼ë¡œ ì‚¬ìš©

- vanilla seuqeeze-and-excitation moduleì€ ì „ì²´ utteranceë¥¼ contextë¡œ ì‚¬ìš©
- ë‹¤ì–‘í•œ context sizeì˜ ì˜í–¥ì„ ì¡°ì‚¬í•˜ê¸° ìœ„í•´ squeeze-and-excitation moduleì˜ global average pooling ì—°ì‚°ìë¥¼ pooling windowì˜ í¬ê¸°ë¡œ contextë¥¼ ì œì–´í•  ìˆ˜ ìˆëŠ” stride-one pooling ì—°ì‚°ìë¡œ ëŒ€ì²´
- ëª¨ë“  convolution blockì—ì„œ 256, 512 ë° 1024ì˜ window sizeë¥¼ ë¹„êµ

Table 3; SE moduleì€ baselineì— ë¹„í•´ ê°œì„ ë¨

- ë˜í•œ, context windowì˜ ê¸¸ì´ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ ë”ìš± í–¥ìƒë¨
- ì´ëŠ” image classification modelì— ëŒ€í•œ SEì˜ ìœ ì‚¬í•œ ì—°êµ¬ì—ì„œ ê´€ì°°ëœ ê²ƒê³¼ ì¼ì¹˜ [34].

![image](https://user-images.githubusercontent.com/46676700/133924678-2e019104-c9f1-4296-84a1-c4c53eb6bb8f.png)

### **<span style="color:#724598">3.3. Depth, Width, Kernel Size and Downsampling</span>**

_Depth_: convolutional blockì˜ ìˆ˜ì— ëŒ€í•´ sweepingì„ ìˆ˜í–‰í•˜ê³ , ìµœìƒì˜ configì€ Table 1ì— ìˆìŒ

- ì´ configë¥¼ ì‚¬ìš©í•´ ì•ˆì •ì ì¸ ìˆ˜ë ´ìœ¼ë¡œ í•˜ë£¨ë§Œì— ëª¨ë¸ì„ í›ˆë ¨í•  ìˆ˜ ìˆìŒì„ ë°œê²¬

_Width_: ëª¨ë“  encoder layerì—ì„œ network width(ì¦‰, channel ìˆ˜)ë¥¼ ì „ì²´ì ìœ¼ë¡œ í™•ì¥í•˜ê³  ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì—°êµ¬

- êµ¬ì²´ì ìœ¼ë¡œ, Tabel 1ì—ì„œ ContextNet ëª¨ë¸ì„ ì·¨í•˜ê³ , $Î±$ë¥¼ sweepí•˜ì—¬ LibriSpeechì—ì„œ model sizeì™€ WER í™•ì¸ (Tabel 5 ; ê²°ê³¼ ìš”ì•½)
- ContextNetì˜ WERê³¼ ëª¨ë¸ í¬ê¸° ì‚¬ì´ì˜ ì ì ˆí•œ ê· í˜•ì„ ë³´ì„

_Downsampling and kernel size_: Tabel 4 ; downsampling ë° filter sizeì˜ ë‹¤ì–‘í•œ ì„ íƒê³¼ í•¨ê»˜ LibriSpeechì˜ FLOPS ë° WERì„ ìš”ì•½

- baseline: $C_3$ì— í•˜ë‚˜ì˜ downsampling layer ì¶”ê°€

    â†’ ë”°ë¼ì„œ baselineì€ 2ë°° temporal reduction

- {3, 5, 11, 21}ì—ì„œ kernel sizeë¥¼ sweepí•˜ê³ , ê° kernel sizeëŠ” ëª¨ë“  depthë³„ convolution layerì— ì ìš©

    â†’ progressiveí•œ downsamplingì´ FLOPS ìˆ˜ë¥¼ ìƒë‹¹íˆ ì ˆì•½í•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬

- ë˜í•œ, ì‹¤ì œë¡œ ëª¨ë¸ì˜ ì •í™•ë„ì— ì•½ê°„ì˜ ì´ì ì´ ìˆì—ˆìœ¼ë©°, progressive downsamplingìœ¼ë¡œ kernelë¥¼ ëŠ˜ë¦¬ë©´ ëª¨ë¸ì˜ WERì´ ê°ì†Œí•¨!

![image](https://user-images.githubusercontent.com/46676700/133924687-15f0c85f-a9b2-4629-ac63-a0ef7b849ade.png)
![image](https://user-images.githubusercontent.com/46676700/133924693-c9268573-4396-411b-8f2b-6728af6196ca.png)

**ğŸ’¡FLOPS** (FLoating point Operations Per Second)
ì»´í“¨í„° ì„±ëŠ¥ì„ ìˆ˜ì¹˜ë¡œ ë‚˜íƒ€ë‚  ë•Œ ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë‹¨ìœ„
ì´ˆë‹¹ ë¶€ë™ì†Œìˆ˜ì  ì—°ì‚°ì´ë¼ëŠ” ì˜ë¯¸ë¡œ ì»´í“¨í„°ê°€ 1ì´ˆë™ì•ˆ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ë¶€ë™ì†Œìˆ˜ì  ì—°ì‚°ì˜ íšŸìˆ˜

**ğŸ’¡GLOPS**
1ì´ˆì˜ audio ì²˜ë¦¬ì— ëŒ€í•œ average encoder FLOPS

### **<span style="color:#724598">3.4. Large Scale Experiments</span>**

ì œì•ˆëœ architectureê°€ ëŒ€ê·œëª¨ datasetì—ì„œë„ íš¨ê³¼ì 

[35]ì™€ ìœ ì‚¬í•œ ì‹¤í—˜ ì„¤ì •ì„ ì‚¬ìš©

- training set: [36]ì˜ ì ‘ê·¼ ë°©ì‹ì— ì˜í•´ ìƒì„±ëœ semi-supervised transcriptê°€ í¬í•¨ëœ public Youtube video
- ì´ 24.12ì‹œê°„ ë™ì•ˆ 117ê°œì˜ videoë¥¼ í‰ê°€ (testsetì€ ë‹¤ì–‘í•˜ê³  challengingí•œ ìŒí–¥ í™˜ê²½)

Table 6 ; ê²°ê³¼ ìš”ì•½

â†’ ContextNetì´ ë” ì ì€ parameterì™€ FLOPSë¡œ convolutionê³¼ bidirectional LSTMì˜ ì¡°í•©ì¸ TDNN(ì´ì „ ìµœê³ ì˜ architecture)ë¥¼ ìƒëŒ€ì ìœ¼ë¡œ 12% ëŠ¥ê°€

![image](https://user-images.githubusercontent.com/46676700/133924701-f95e23a4-a2a2-4d5f-8081-160ceb12fd18.png)

# **<span style="color:#E84560">_4. Conclusion_</span>**

- End-to-End speech recognitionì„ ìœ„í•œ CNN ê¸°ë°˜ architectureë¥¼ ì œì•ˆí•˜ê³  í‰ê°€
- ì´ì „ì— ë°œí‘œëœ CNN ëª¨ë¸ì— ë¹„í•´ í›¨ì”¬ ì ì€ ìˆ˜ì˜ parameterë¡œ LibriSpeech benchmarkì—ì„œ ë” ë‚˜ì€ ì •í™•ë„ë¥¼ ë‹¬ì„±
- ì œì•ˆëœ architectureëŠ” networkì˜ widthë¥¼ ì œí•œí•˜ì—¬ ì‘ì€ ASR ëª¨ë¸ì„ ê²€ìƒ‰í•˜ëŠ”ë° ì‰½ê²Œ ì‚¬ìš© ê°€ëŠ¥í•¨
- í›¨ì”¬ ë” í¬ê³ , challengeí•œ datasetì— ëŒ€í•œ ì´ˆê¸° ì—°êµ¬ì—ì„œë„ ë³¸ ë…¼ë¬¸ ì €ìë“¤ì˜ ë°œê²¬ì´ í™•ì¸ë¨
